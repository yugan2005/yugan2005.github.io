<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Echo-ohcE]]></title>
  <link href="www.echo-ohce.com/atom.xml" rel="self"/>
  <link href="www.echo-ohce.com/"/>
  <updated>2017-01-03T00:03:43-08:00</updated>
  <id>www.echo-ohce.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.coderforart.com/">CoderForArt</generator>

  
  <entry>
    <title type="html"><![CDATA[giraph]]></title>
    <link href="www.echo-ohce.com/14742423250614.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250614.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Kill the background job</a>
</li>
<li>
<a href="#toc_1">giraph Unit testing</a>
</li>
<li>
<a href="#toc_2">giraph edge value not Mutable in place</a>
</li>
<li>
<a href="#toc_3">giraph shell runner / command line</a>
</li>
<li>
<a href="#toc_4">Unsolved problem with the input string split in InputFormat class</a>
</li>
<li>
<a href="#toc_5">giraph log file memory info free/total/max</a>
</li>
<li>
<a href="#toc_6">Giraph Edge and Message memory optimization</a>
<ul>
<li>
<a href="#toc_7">Edge memory management</a>
</li>
<li>
<a href="#toc_8">Message memory management</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">Use MessageStore</a>
</li>
<li>
<a href="#toc_10">The memory cost for java classes</a>
</li>
<li>
<a href="#toc_11">Mutate Graph</a>
<ul>
<li>
<a href="#toc_12">When there is a conflict for the mutate request</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">The timeout parameter for waiting resources</a>
</li>
<li>
<a href="#toc_14">duplicated job tracker setting is a must</a>
</li>
<li>
<a href="#toc_15">Use both Edge Input and Vertex Value Input</a>
</li>
<li>
<a href="#toc_16">Output during computation</a>
</li>
<li>
<a href="#toc_17">A lot of supersteps cause counters.LimitExceededException</a>
</li>
<li>
<a href="#toc_18">giraph jar Hadoop building dependency</a>
</li>
<li>
<a href="#toc_19">CPU bounded computation</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Kill the background job</h1>

<p>logging on sc2-hive1, run the following command to check the running job on background. (It works for both oozie and normal hadoop jobs)  </p>

<pre><code class="language-bash">ps -u yu
hadoop job -list | grep yu
</code></pre>

<p>When using the <code>ps</code> command and what to see the detailed command argument, get the <code>pid#</code> and run:  </p>

<pre><code class="language-bash">ps --pid pid#
</code></pre>

<p>Hadoop YARN command equivalent to <code>hadoop job -list</code>:  </p>

<pre><code class="language-bash">yarn application -appStates RUNNING -list
</code></pre>

<h1 id="toc_1">giraph Unit testing</h1>

<ul>
<li>When use <code>InternalVertexRunner</code>, comment out all giraph/hadoop counter logs</li>
<li>Most of the case, the unit test fails silently. Adding some <code>sout</code> maybe the only way to debug</li>
<li>the testing string input delimiter should always be <code>space</code> even in reality we can use all kinds of delimiter like <code>\t</code> etc.</li>
</ul>

<p><strong>wrong</strong></p>

<pre><code class="language-java">// tab &#39;\t&#39; delimited
String[] edges = new String[] { &quot;0  1&quot;, &quot;0  3&quot;, &quot;1  0&quot;, &quot;1  3&quot;, &quot;1  2&quot;};
</code></pre>

<p><strong>correct</strong></p>

<pre><code class="language-java">// space delimited
String[] edges = new String[] { &quot;0 1&quot;, &quot;0 3&quot;, &quot;1 0&quot;, &quot;1 3&quot;, &quot;1 2&quot;};
</code></pre>

<h1 id="toc_2">giraph edge value not Mutable in place</h1>

<p>edges are just a reference, it can not be changed in place.<br/>
After mutate the edge value, use <code>vertex.setEdgeValue(targetVertexId, edgeValue)</code> to change it.<br/>
<a href="https://giraph.apache.org/apidocs/org/apache/giraph/graph/Vertex.html#setEdgeValue-I-E-">Official Document</a></p>

<h1 id="toc_3">giraph shell runner / command line</h1>

<p>This is one file for running giraph with MasterCompute, Aggregator, etc.  </p>

<pre><code class="language-bash">WORKER=177
MEM=5120
HEAP=4352
#zoo keeper
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

hadoop jar dpp-giraph-0.0.1-with-giraph-core.jar \
        com.adsymp.dpp.giraph.lp.LPRunner \
        -Dmapred.job.queue.name=datascience \
        -Dmapreduce.map.memory.mb=$MEM \
        -Dmapreduce.task.timeout=1800000 \
        -Dmapred.task.timeout=1800000 \
        -Dmapreduce.map.java.opts=-Xmx${HEAP}m \
        -Dgiraph.zkList=$ZKLIST \
        com.adsymp.dpp.giraph.lp.WeightedLPComputation \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        -mc com.adsymp.dpp.giraph.lp.ClusterHistogramMC \
        -aw org.apache.giraph.aggregators.TextAggregatorWriter \
        -ca giraph.textAggregatorWriter.frequency=1 \
        -vif com.adsymp.dpp.giraph.lp.LongScoredValueFloatAdjacencyListVertexInputFormat \
        -vip $ADJ \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op $OUT \
        -w $WORKER -ca giraph.SplitMasterWorker=true \
        -ca giraph.zkSessionMsecTimeout=360000 \
        -ca giraph.zkOpsMaxAttempts=20 \
        -ca giraph.zkOpsRetryWaitMsecs=10000 \
        -ca mapred.job.tracker=sc2-rm1:8088 \
        -ca mapreduce.job.tracker=sc2-rm1:8088 \
        -ca com.adsymp.giraph.max.cd=8 \
        -ca giraph.waitingRequestMsecs=300000 \
        -ca com.adsymp.giraph.max.cc=100 \
        -ca com.adsymp.giraph.max.dd=8 \
        -ca com.adsymp.giraph.max.degree=80 \
        -ca com.adsymp.giraph.ignore.dev.types=false \
        -ca com.adsymp.giraph.weight.threshold=0.4 \
        -ca giraph.jobRetryCheckerClass=com.adsymp.dpp.giraph.RetryThriceChecker \
        -ca mapred.job.queue.name=datascience \
        -ca mapred.output.compress=true \
        -ca mapred.task.timeout=1800004 \
        -ca mapreduce.map.memory.mb=$MEM \
        -ca mapreduce.map.java.opts=-Xmx${HEAP}m
</code></pre>

<p>Couple important points:<br/>
1. the line breaker <code>\</code>: must have one space before it and <strong>no</strong> space after it, and every broken line must have it.<br/>
2. Pay special attention to the sequence of the arguments, some of them matters.<br/>
3. The zookeeper list after the line of the Computation class is a must!  </p>

<p>This one file is for running giraph with jython:  </p>

<pre><code class="language-bash">hdfs dfs -rm -r giraph-jython/output
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

yarn jar giraph-jython-1.0-SNAPSHOT-jar-with-dependencies.jar \
        com.adsymp.dpp.giraph.jython.GiraphJythonRunner \
        -Dmapred.job.queue.name=datascience \
        -Dgiraph.zkList=$ZKLIST \
        graph-distance.py \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        --jythonClass GraphDistance \
        --typesHolder com.adsymp.dpp.giraph.jython.JythonTypes \
        -eif org.apache.giraph.io.formats.IntNullTextEdgeInputFormat \
        -eip giraph-jython/input/tiny_graph \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op giraph-jython/output \
        -w 1 \
        -ca giraph.SplitMasterWorker=false
</code></pre>

<h1 id="toc_4">Unsolved problem with the input string split in InputFormat class</h1>

<p>My input file is delimited by <code>^A</code> (i.e.<code>\u0001</code>). Tried to change the <code>LongNullTextEdgeInputFormat.LongNullTextEdgeReader#preprocessLine</code> for splitting by <code>\u0001</code>. Failed.</p>

<p>Temporary workaround is wrote another pig script to convert the input file schema.</p>

<h1 id="toc_5">giraph log file memory info free/total/max</h1>

<p>In the log file of each individual worker, we see the following lines about the real time memory usage: </p>

<blockquote>
<p>2016-10-21 21:35:44,615 INFO [compute-0] org.apache.giraph.graph.ComputeCallable: call: Completed 18 partitions, 159 remaining Memory (free/total/max) = 6154.37M / 16748.50M / 27079.00M.  </p>
</blockquote>

<p>This log is generated by the class: <code>org.apache.giraph.utils.MemoryUtils</code> <a href="https://github.com/apache/giraph/blob/release-1.1/giraph-core/src/main/java/org/apache/giraph/utils/MemoryUtils.java">source code here</a></p>

<p>this class calls <code>java.lang.Runtime</code> class and use the method <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#freeMemory()"><code>freeMemory()</code></a>, <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#maxMemory()"><code>maxMemory()</code></a>, <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#totalMemory()"><code>totalMemory()</code></a> to get the real time memory usage stats</p>

<ul>
<li>free<br/>
the amount of free memory in the Java Virtual Machine. Calling the <code>gc</code> method may result in increasing the value returned by freeMemory. <strong>an approximation to the total amount of memory currently available for future allocated objects</strong></li>
<li>max<br/>
the maximum amount of memory that the Java virtual machine will attempt to use. If there is no inherent limit then the value Long.MAX_VALUE will be returned. <strong>the maximum amount of memory that the virtual machine will attempt to use</strong> </li>
<li>total<br/>
the total amount of memory in the Java virtual machine. The value returned by this method may vary over time, depending on the host environment. (Note that the amount of memory required to hold an object of any given type may be implementation-dependent.) <strong>the total amount of memory currently available for current and future objects</strong></li>
</ul>

<h1 id="toc_6">Giraph Edge and Message memory optimization</h1>

<p>From the book chapter 11:</p>

<blockquote>
<p>Giraph does some memory management on its own. For instance, it stores some of the data, like edges, messages and vertex values, serialized inside of byte arrays that it allocates at the beginning of the computation. Still, it offers a pure Java object-oriented API. To achieve this abstraction, Giraph keeps a number of objects around, internally called representative objects, which it reinitializes with data coming from these binary arrays before passing them to the user, and which it serializes back to the arrays after the user is done with them. This mechanism is used, for example, for the Iterable containing the messages passed to the compute function, or the default implementation of the OutEdges interface where edges are stored for each vertex.</p>
</blockquote>

<p>With this said, it is different between edges and messages.</p>

<h2 id="toc_7">Edge memory management</h2>

<p>Edge in its full name should be <code>Edge&lt;I,E&gt;</code> in which <code>I</code> is the vertexID type (which is the vertexIDs that the edge is pointing to, i.e. <code>OutEdges</code>) and <code>E</code> is the edgeValue type. So we need have a data structure that builds the 1-1 corresponds between the pointing-to ID and the corresponding edgeValue.</p>

<p>The naive way of implementing it is just a <code>List&lt;I, E&gt;</code> or <code>Map&lt;I, E&gt;</code>,  the first one only provides iterator but is cheaper, and the second one provides random access efficiently but is more expensive.</p>

<p>Actually, the two different real implementations are: </p>

<ol>
<li>Provides only Iterable - <code>ByteArrayEdges</code>

<ol>
<li>Use a reusable <code>edge&lt;I,E&gt;</code> object for iterating all edges </li>
<li>The underlying data structure is byte array. Every time, it is deserialized (or serialized) to the byte array</li>
<li>It is already the most memory efficient implementation. The cons are: Not random access, CPU intensive<br/></li>
</ol></li>
<li>Provide random access - <code>OpenHashMapOutedges</code>

<ol>
<li>Can not use byte array anymore, because the edgeValue changes, not fix length of the byte array anymore.</li>
<li>For each vertex, all the edges are save in one Map, using pointing-to ID as the key, and the edgeValue as the Map value.</li>
<li>Use <code>fastutil</code> primitive map type.</li>
<li>Still using a reusable <code>edge&lt;I,E&gt;</code> object for iterating all edges.</li>
<li>Efficient <code>getEdgeValue</code> and <code>setEdgeValue</code> methods.</li>
</ol></li>
</ol>

<p><strong><font color='red'>Special Notes:</font></strong>  </p>

<ul>
<li>Can I use <code>ByteArrayEdges</code> but instructing the Map inside <code>compute</code> method for fast random access? It has pros and cons. 

<ul>
<li>Pros: the edges are saved in memory efficient byte arrays for all vertexes. Otherwise, for each vertex, it needs a map. Because in a giraph worker thread, the graph is processed partition after partition, and in a partition, the graph is processed vertex after vertex, so at a single time, for one worker thread, there will be only one map. </li>
<li>Cons: Stressed on CPU for creating and destroying a lot of maps. Also serialize and deserializes.<br/></li>
</ul></li>
<li>Hashmap actually is quite expensive w.r.t the memory usage, because of the load factor and avoid collision</li>
<li>When using the default <code>ByteArrayEdges</code>, and called <code>getEdgeValue</code> and <code>setEdgeValue</code> method, under the hood, for the <code>getEdgeValue</code>, it just scan through the immutable edges iterator, and for the  <code>setEdgeValue</code> it scan through the <code>MutableEdge</code> iterator.<br/></li>
</ul>

<blockquote>
<p>If the VertexEdges implementation has a specialized random-access method, we use that; otherwise, we scan the edges.</p>
</blockquote>

<h2 id="toc_8">Message memory management</h2>

<p>Different than the Edges, Message are always stored in a <strong>serialized format inside of byte arrays</strong>. We are using the same concept of re-usable iterator to pass the messages to the compute method.</p>

<p>Each giraph <strong>worker</strong> has one <strong>MessageStore</strong>, which is the message &#39;inbox&#39; for <strong>all</strong> the vertexes of this <strong>worker</strong>.</p>

<p>So, the message store, is organized by <strong>Partition index</strong> then further organized by <strong>Vertex index</strong>, essentially a nested map, with partitions as keys of the outer map, and vertex IDs as keys of the inner maps.</p>

<p>The default implementation of giraph uses <strong>original java HashMap</strong> and is not very efficient. There is some space of saving by using the <code>fastutil</code>. (comparing <code>long</code>: 8 bytes, and <code>Long</code>:24 bytes)</p>

<p>This optimization should be done always, because it only sacrifices generality of the partition ID and vertex ID type.</p>

<p><strong><font color='red'><a href="https://github.com/apache/giraph/blob/release-1.1/giraph-core/src/main/java/org/apache/giraph/comm/messages/primitives/long_id/LongByteArrayMessageStore.java">ALREADY HAVE A IMPLEMENTATION</a>: Just use <code>LongByteArrayMessageStore</code> </font></strong></p>

<h1 id="toc_9">Use MessageStore</h1>

<p><strong><font color=red>The book&#39;s content is outdated as of giraph v1.1</font></strong></p>

<p>According to the book, I must first write a class that implements <code>MessageStoreFactory</code> interface, and this factory generates the <code>MessageStore</code> implementation that I need.</p>

<p>Then specify the parameter of <code>giraph.messageStoreFactoryClass</code> to let giraph know.</p>

<p>But actually, the default giraph already implemented the <code>LongByteArrayMessageStore</code></p>

<ol>
<li><a href="http://giraph.apache.org/options.html">the default configuration</a> lists: 

<ol>
<li><code>messageEncodeAndStoreType</code> = <code>BYTEARRAY_PER_PARTITION</code></li>
<li><code>messageStoreFactoryClass</code> = <code>InMemoryMessageStoreFactory</code></li>
</ol></li>
<li>The <a href="https://github.com/apache/giraph/blob/release-1.1/giraph-core/src/main/java/org/apache/giraph/comm/messages/InMemoryMessageStoreFactory.java#L131">source code</a> of <code>InMemoryMessageStoreFactory</code> shows for vertexID <code>LongWritable</code>, <code>messageEncodeAndStoreType</code> = <code>BYTEARRAY_PER_PARTITION</code>, <code>messageStore = new LongByteArrayMessageStore()</code></li>
</ol>

<h1 id="toc_10">The memory cost for java classes</h1>

<p>Remember the load factors for Hash*** stuff, which make it expensive.</p>

<p>A good series posts here:<br/>
<a href="http://java-performance.info/overview-of-memory-saving-techniques-java/">An overview of memory saving techniques in Java</a><br/>
<a href="http://java-performance.info/memory-consumption-of-java-data-types-1/">Memory consumption of popular Java data types – part 1</a><br/>
<a href="http://java-performance.info/memory-consumption-of-java-data-types-2/">Memory consumption of popular Java data types – part 2</a></p>

<h1 id="toc_11">Mutate Graph</h1>

<p>Three different mechanism of mutating a graph</p>

<p>It is in the book of chapter 8</p>

<h2 id="toc_12">When there is a conflict for the mutate request</h2>

<p>It is resolved by <code>VertexResolver</code></p>

<blockquote>
<p>The default vertex resolver performs the following operations: </p>

<ol>
<li>If there were any edge removal requests, first apply these removals. </li>
<li>If there was a request to remove the vertex, then remove it. This is achieved by setting the return Vertex object to null . </li>
<li>If there was a request to add the vertex, and it does not exist, then create the vertex. </li>
<li>If the vertex has messages sent to it, and it does not exist, then create the vertex. </li>
<li>If there was a request to add edges to the vertex, if the vertex does not exist, first create and then add the edges; otherwise, simply add the edges. </li>
</ol>

<p>The order of this list is important because it defines exactly the way that Giraph resolves any conflicts by default. This means that if, for instance, there is request to remove a vertex and at the same time a request to add it, then because the default resolver checks the vertex creation after it does the deletion, it ends up creating the vertex.</p>
</blockquote>

<p><code>-ca giraph.vertexResolverClass=MyVertexResolver</code></p>

<h1 id="toc_13">The timeout parameter for waiting resources</h1>

<pre><code class="language-bash">-Dgiraph.maxMasterSuperstepWaitMsecs=18700000 \
</code></pre>

<h1 id="toc_14">duplicated job tracker setting is a must</h1>

<p>In <code>oozie</code> or just run in command line, for certain version of giraph, we must specify both <code>mapred.job.tracker</code> and <code>mapreduce.job.tracker</code> parameters</p>

<p>Otherwise, there is be error message looks like below:</p>

<blockquote>
<p>java.lang.IllegalArgumentException: checkLocalJobRunnerConfiguration: When using LocalJobRunner, must have only one worker since only 1 task at a time!</p>
</blockquote>

<p>This bug actually is reported <a href="https://issues.apache.org/jira/browse/GIRAPH-1001">here</a></p>

<h1 id="toc_15">Use both Edge Input and Vertex Value Input</h1>

<p>EdgeInput to build edges, and vertex value input to attach vertex information to the vertexes.</p>

<p>From <a href="http://giraph.apache.org/io.html">Giraph official document</a> </p>

<blockquote>
<p>To summarize, VertexInputFormat is usually used by itself, whereas EdgeInputFormat may be used in combination with VertexValueInputFormat.</p>
</blockquote>

<p>The way of specifying <code>VertexValueInputFormat</code> in the command line parameter is exactly the same as using <code>VertexInputFormat</code></p>

<p>The schema of EdgeInput is: <code>VId1, VId2, EdgeValue</code> \(\rightarrow\) type info: <code>&lt;I, E&gt;</code><br/>
The schema of VertexValueInput is : <code>VId, VValue</code> \(\rightarrow\) type info: <code>&lt;I, V&gt;</code></p>

<p>See my latest push of <code>algo-109-v1</code> for example <code>VIF=&#39;com.adsymp.dpp.giraph.jython.inputformats.LongTextTextVertexValueInputFormat&#39;</code></p>

<h1 id="toc_16">Output during computation</h1>

<p>The main class is <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/GiraphTextOutputFormat.java">GiraphTextOutputFormat</a></p>

<ol>
<li>Set <code>giraph.doOutputDuringComputation</code> <a href="http://giraph.apache.org/options.html">doc</a></li>
<li>Change behavior of <code>ImmutableClassesGiraphConfiguration</code>

<ol>
<li>generate <code>SynchronizedSuperstepOutput</code> as <code>SuperstepOutput</code>, the <code>ImmutableClassesGiraphConfiguration</code> is passed to <code>SynchronizedSuperstepOutput</code> constructor <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/conf/ImmutableClassesGiraphConfiguration.java#L462">doc</a></li>
<li>Inside <code>SynchronizedSuperstepOutput</code>, the <code>ImmutableClassesGiraphConfiguration</code> also create <code>WrappedVertexOutputFormat</code>. <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/superstep_output/SynchronizedSuperstepOutput.java#L64">doc</a></li>
<li>The <code>WrappedVertexOutputFormat</code> wraps <code>VertexOutputFormat</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/conf/ImmutableClassesGiraphConfiguration.java#L386">doc</a></li>
<li>The <code>VertexOutputFormat</code> is created by <font color='salmon'><strong>User specified Vertex Output Format Class</strong></font> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/conf/ImmutableClassesGiraphConfiguration.java#L357">doc</a></li>
<li>The <code>WrappedVertexOutputFormat</code> uses the wrapped <code>VertexOutputFormat</code> class&#39;s method <code>createVertexWriter</code> <font color='salmon'><strong>This is the place we can hook up the logic of creating SS aware outputpaht</strong></font>to create the base <code>VertexWriter</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/internal/WrappedVertexOutputFormat.java#L68">doc</a>, and then wrap on this base <code>VertexWriter</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/internal/WrappedVertexOutputFormat.java#L71">doc</a></li>
</ol></li>
<li><code>VertexWriter</code> has following abstract method: 

<ol>
<li>itself has <code>initialize</code> and <code>close</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/VertexWriter.java">doc</a>. </li>
<li>It also inherit from interface <code>SimpleVertexWriter</code> with method <code>writeVertex</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/SimpleVertexWriter.java#L43">doc</a></li>
<li>It also inherit from concrete class <code>DefaultImmutableClassesGiraphConfigurable</code> with method <code>setConf</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/conf/DefaultImmutableClassesGiraphConfigurable.java#L39">doc</a></li>
</ol></li>
<li>Assume that we are using <code>org.apache.giraph.io.formats.IdWithValueTextOutputFormat</code> as the Vertex Output Format class. 

<ol>
<li>Its <code>createVertexWriter</code> methods gives <code>IdWithValueVertexWriter</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/IdWithValueTextOutputFormat.java#L54">doc</a></li>
<li>This Vertex Output Format class extends <code>TextVertexOutputFormat</code> and its writer extends <code>TextVertexWriterToEachLine</code>. </li>
</ol></li>
<li>The <code>TextVertexOutputFormat</code> extends <code>VertexOutputFormat</code> and its <code>TextVertexWriterToEachLine</code> extends <code>TextVertexWriter</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/TextVertexOutputFormat.java#L148">doc</a>, which is also defined in the same class <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/TextVertexOutputFormat.java#L87">doc</a></li>
<li>The <code>TextVertexOutputFormat</code> delegate <code>GiraphTextOutputFormat</code> to get the <code>OutputCommitter</code>, which defines the output path in Hadoop <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/TextVertexOutputFormat.java#L49">doc</a></li>
<li>The real work of generate the output path inside <code>GiraphTextOutputFormat</code> is <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/GiraphTextOutputFormat.java#L60">here</a></li>
</ol>

<p>DAMN! I wasted a lot of time. Actually, the output is just appended in the output folder. That&#39;s it.</p>

<h1 id="toc_17">A lot of supersteps cause counters.LimitExceededException</h1>

<p>Because by default, giraph use Hadoop counters to record giraph stats, like how many messages, how many vertexes, edges etc. in each superstep, if the superstep is very large, we will have error like this:</p>

<blockquote>
<p>.MasterThread: masterThread: Master algorithm failed with LimitExceededException<br/>
org.apache.hadoop.mapreduce.counters.LimitExceededException: Too many counters: 1025 max=1024</p>
</blockquote>

<p>The way of fixing it is to set <code>-ca giraph.useSuperstepCounters=false</code> in the runner to turn off the recording mechanism for every superstep.</p>

<h1 id="toc_18">giraph jar Hadoop building dependency</h1>

<p>When build the giraph jar, correct Hadoop version dependency is a must. For my case, the main cluster run at <code>Hadoop 2.5.0-cdh5.3.0</code> </p>

<p>When I use <code>pom.xml</code> like follow, it fails: (This pom was supposed to be used for a different client&#39;s Hadoop environment)  </p>

<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
  &lt;version&gt;2.6.4&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt;
  &lt;version&gt;2.6.4&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>And the error message is very confusing: </p>

<blockquote>
<p>2017-03-23 00:24:03,717 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://sc2prod:8020]<br/>
2017-03-23 00:24:03,718 INFO [main] org.apache.hadoop.service.AbstractService: Service JobHistoryEventHandler failed in state INITED; cause: java.lang.IllegalArgumentException: Wrong FS: hdfs://sc2prod:8020/user/yu/.staging/job_1489616796019_14222, expected: hdfs://sc2prod<br/>
java.lang.IllegalArgumentException: Wrong FS: hdfs://sc2prod:8020/user/yu/.staging/job_1489616796019_14222, expected: hdfs://sc2prod<br/>
    at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)<br/>
    at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:465)<br/>
    at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceInit(JobHistoryEventHandler.java:143)<br/>
    at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)</p>
</blockquote>

<p>My guess is that for the wrong hadoop dependency, the jar can not find the correct hadoop config path/file for the configurations. Then it just uses the default, which is not right.</p>

<p>After I changed the <code>pom.xml</code> to below, it works fine.</p>

<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
  &lt;version&gt;2.5.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt;
  &lt;version&gt;2.5.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
  &lt;version&gt;2.5.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p><strong>Note</strong> The newly added <code>hadoop-client</code> is a must, otherwise will have error like follow:</p>

<blockquote>
<p>2017-03-23 06:22:00,622 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster<br/>
java.lang.NoSuchMethodError: org.apache.hadoop.ipc.RPC.getServer(Ljava/lang/Class;Ljava/lang/Object;Ljava/lang/String;IIZLorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/security/token/SecretManager;)Lorg/apache/hadoop/ipc/RPC$Server;<br/>
    at org.apache.hadoop.mapred.TaskAttemptListenerImpl.startRpcServer(TaskAttemptListenerImpl.java:121)</p>
</blockquote>

<p>As noted in <a href="http://stackoverflow.com/questions/39574202/java-lang-nosuchmethoderror-org-apache-hadoop-ipc-rpc-getproxy-while-creating-h">this stackoverflow post</a>, the <code>hadoop-client</code> dependency need be added.</p>

<h1 id="toc_19">CPU bounded computation</h1>

<p>Simply use `<code>giraph.numComputeThreads</code> (by default it is 1) to speed it up. Randy tested it and it works very well.</p>

<hr/>

<ul>
<li>
<a href="#toc_0">Kill the background job</a>
</li>
<li>
<a href="#toc_1">giraph Unit testing</a>
</li>
<li>
<a href="#toc_2">giraph edge value not Mutable in place</a>
</li>
<li>
<a href="#toc_3">giraph shell runner / command line</a>
</li>
<li>
<a href="#toc_4">Unsolved problem with the input string split in InputFormat class</a>
</li>
<li>
<a href="#toc_5">giraph log file memory info free/total/max</a>
</li>
<li>
<a href="#toc_6">Giraph Edge and Message memory optimization</a>
<ul>
<li>
<a href="#toc_7">Edge memory management</a>
</li>
<li>
<a href="#toc_8">Message memory management</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">Use MessageStore</a>
</li>
<li>
<a href="#toc_10">The memory cost for java classes</a>
</li>
<li>
<a href="#toc_11">Mutate Graph</a>
<ul>
<li>
<a href="#toc_12">When there is a conflict for the mutate request</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">The timeout parameter for waiting resources</a>
</li>
<li>
<a href="#toc_14">duplicated job tracker setting is a must</a>
</li>
<li>
<a href="#toc_15">Use both Edge Input and Vertex Value Input</a>
</li>
<li>
<a href="#toc_16">Output during computation</a>
</li>
<li>
<a href="#toc_17">A lot of supersteps cause counters.LimitExceededException</a>
</li>
<li>
<a href="#toc_18">giraph jar Hadoop building dependency</a>
</li>
<li>
<a href="#toc_19">CPU bounded computation</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark]]></title>
    <link href="www.echo-ohce.com/14865029971468.html"/>
    <updated>2017-02-07T13:29:57-08:00</updated>
    <id>www.echo-ohce.com/14865029971468.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">After join drop some columns cause Error</a>
</li>
<li>
<a href="#toc_1">read Lzo into Dataset</a>
</li>
<li>
<a href="#toc_2">read CSV into Dataset</a>
</li>
<li>
<a href="#toc_3">Don&#39;t use user defined functions in agg yet</a>
</li>
<li>
<a href="#toc_4">Use iterator and for comprehension to get cross product</a>
</li>
<li>
<a href="#toc_5"><font color='salmon'>? Strange java.io.NotSerializableException</font></a>
</li>
<li>
<a href="#toc_6">Using Spark-shell REPL</a>
</li>
<li>
<a href="#toc_7">Running spark job in oozie</a>
</li>
<li>
<a href="#toc_8">Another error for running spark job in oozie</a>
</li>
<li>
<a href="#toc_9">More dependency conflict for running oozie spark workflow</a>
</li>
<li>
<a href="#toc_10">spark config</a>
</li>
<li>
<a href="#toc_11">graphX connectedComponent</a>
</li>
<li>
<a href="#toc_12">Using of Bloom Filter</a>
</li>
<li>
<a href="#toc_13">Add a jar inside the spark REPL</a>
</li>
<li>
<a href="#toc_14">Get size of an object inside spark REPL</a>
</li>
<li>
<a href="#toc_15">Configuration that WON&#39;T work</a>
</li>
<li>
<a href="#toc_16">My experience for OOM debugging</a>
</li>
<li>
<a href="#toc_17">Date in Spark Dataset</a>
</li>
<li>
<a href="#toc_18">Encoder Issue in Dataset</a>
<ul>
<li>
<a href="#toc_19">Unable to find Encoder at Compiling time</a>
</li>
<li>
<a href="#toc_20">Unable to find Encoder at Run time</a>
</li>
</ul>
</li>
<li>
<a href="#toc_21">Split Dataset and save them separately simultaneously</a>
</li>
<li>
<a href="#toc_22">Hint the correct side of broadcast join</a>
</li>
<li>
<a href="#toc_23">Schema after join of two Dataset</a>
</li>
<li>
<a href="#toc_24">Handling A file sequentially</a>
</li>
<li>
<a href="#toc_25">Type Class Reflect of Spark Scala</a>
</li>
<li>
<a href="#toc_26">Spark shell Class initialization Error</a>
</li>
<li>
<a href="#toc_27">Spark date timezone converting</a>
</li>
<li>
<a href="#toc_28">Another Spark from date string to <code>DateType</code> converting</a>
</li>
<li>
<a href="#toc_29">Config spark jars in a folder</a>
</li>
<li>
<a href="#toc_30">Using python inside Spark (Scala) code</a>
</li>
<li>
<a href="#toc_31">DataType inside Dataset Row</a>
</li>
<li>
<a href="#toc_32">Read and Write in LZO format</a>
</li>
<li>
<a href="#toc_33">UDF for Dataset&#39;s column operation</a>
</li>
<li>
<a href="#toc_34">Kill a problematic executor</a>
</li>
<li>
<a href="#toc_35">cache and persist is a transform</a>
</li>
<li>
<a href="#toc_36">Another possible issue about cache</a>
</li>
<li>
<a href="#toc_37">parse URL get domain</a>
</li>
<li>
<a href="#toc_38">Get Seq of Tuple from DataFrame Row</a>
</li>
<li>
<a href="#toc_39">Handling dirty data</a>
</li>
<li>
<a href="#toc_40">User Agent parse</a>
</li>
<li>
<a href="#toc_41">Explode a column of Array to columns without using RDD</a>
</li>
<li>
<a href="#toc_42"><code>WrappedArray</code> in DataSet need use <code>Seq</code> to get</a>
</li>
<li>
<a href="#toc_43">Change all column names once</a>
</li>
<li>
<a href="#toc_44">Read <code>.tar.gz</code> file</a>
</li>
<li>
<a href="#toc_45">Spark join timeout errors</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">After join drop some columns cause Error</h1>

<p>For spark 2.0 Error message is like:</p>

<blockquote>
<p>PartitioningCollection requires all of its partitionings have the same numPartitions.</p>
</blockquote>

<p>Fix it by calling <code>.repartition()</code> on the dataframe.<br/>
<a href="http://stackoverflow.com/a/40125736/4229125">stackoverflow post</a></p>

<p>Need call <code>.repartition()</code> after every <code>join</code> and <code>drop</code> like following:</p>

<pre><code class="language-scala">val injectPair = mbvDS.join(reducedIndexTable, $&quot;strId1&quot; === $&quot;stringId&quot;, &quot;inner&quot;)
        .toDF(&quot;strId1&quot;, &quot;strId2&quot;, &quot;score&quot;, &quot;matchingId1&quot;, &quot;longId1&quot;)
        .drop(&quot;strId1&quot;, &quot;matchingId1&quot;)
        .repartition()
        .join(reducedIndexTable, $&quot;strId2&quot; === $&quot;stringId&quot;, &quot;inner&quot;)
        .toDF(&quot;strId2&quot;, &quot;score&quot;, &quot;longId1&quot;, &quot;matchingId2&quot;, &quot;longId2&quot;)
        .drop(&quot;strId2&quot;, &quot;matchingId2&quot;)
        .repartition()
</code></pre>

<h1 id="toc_1">read Lzo into Dataset</h1>

<p>DB has a Util function<br/>
For example, I need read this label-handler <code>/user/oozie/data/dpp/pairing/label-solve/merge-label-output/2016-11-02/cookie/part-r-00366.lzo</code></p>

<pre><code class="language-scala">import ge.drawbrid.dpp.spark2.util.Util
import spark.implicits._

case class LabelData(handler:String, stringId:String)

val labelPath = &quot;/user/oozie/data/dpp/pairing/label-solve/merge-label-output/2016-11-09/cookie/part-r-003*&quot;

val labels = Util.readLzo(sc, labelPath, &quot;\t&quot;).map({arr =&gt; LabelData(arr(0), arr(1))}).toDS()
</code></pre>

<p>The implementation of this Util function is as:</p>

<pre><code class="language-scala">def readLzo(sc: SparkContext, path: String, delimiter: String) = {
    sc.newAPIHadoopFile(path, classOf[com.hadoop.mapreduce.LzoTextInputFormat],
        classOf[org.apache.hadoop.io.LongWritable],classOf[org.apache.hadoop.io.Text])
      .map(_._2.toString.split(delimiter))
  }
</code></pre>

<p>The output is of type <code>RDD[Array[String]]</code></p>

<h1 id="toc_2">read CSV into Dataset</h1>

<p>Need specify the schema to convert the <code>String</code> type into the appropriate type.</p>

<ul>
<li>Using the case class and <code>Encoders</code> to generate the schema</li>
</ul>

<pre><code class="language-scala">import org.apache.spark.sql.{Encoders, SparkSession}

case class LabelPair(longId1: Long, longId2: Long)

val sparkSession = SparkSession.builder.appName(&quot;Preprocess&quot;).getOrCreate()

import sparkSession.implicits._
val labelPairsPath = &quot;LP_tunning_17/spark_base/pairs/*&quot;
val labelPairsDS = sparkSession.read
                    .option(&quot;sep&quot;, &quot;\t&quot;)
                    .schema(Encoders.product[LabelPair].schema)
                    .csv (labelPairsPath).as[LabelPair]
</code></pre>

<ul>
<li>If the schema is less confusing, can simply use <code>.option(&quot;inferSchema&quot;, &quot;true&quot;)</code></li>
</ul>

<p><a href="http://stackoverflow.com/a/40656642/4229125">reference post</a></p>

<h1 id="toc_3">Don&#39;t use user defined functions in agg yet</h1>

<p><font color='salmon'><strong>Do not use the user defined functions yet!</strong></font></p>

<p>The <code>groupByKey</code> returns <code>KeyValueGroupedDataset</code>, which is not very mature as of now. <code>agg</code> can be applied on <code>KeyValueGroupedDataset</code> only with <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.scalalang.typed$">typed expression</a>. Avoid using it. </p>

<p>However, the <code>groupBy</code> is preferred, because it returns <code>RelationalGroupedDataset</code>, <code>agg</code> can be applied on <code>RelationalGroupedDataset</code> with all sql.functions. <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">Full list of sql.functions</a></p>

<p>List the following pages anyway:  </p>

<ul>
<li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.package">sql.expression package</a></li>
<li><a href="https://docs.cloud.databricks.com/docs/spark/1.6/examples/Dataset%20Aggregator.html">Databrick A example actually not run</a></li>
<li><a href="https://github.com/apache/spark/blob/v2.1.0/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala">The source code</a></li>
<li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator">Aggregator spark scala doc</a></li>
<li><a href="https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/">A post not so clear</a></li>
<li><a href="https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html">Databricks manual</a></li>
</ul>

<p><font color='salmon'><strong>Use <code>mapGroup</code> or <code>flatMapGroup</code> for complex logic.</strong></font></p>

<h1 id="toc_4">Use iterator and for comprehension to get cross product</h1>

<p>iterator by contract only iterate once.</p>

<p>The following two examples illustrate this:</p>

<pre><code class="language-scala">val iter = List(1,2,3,4,5,6).iterator
val (iter1, iter2) = iter.duplicate
(for { i &lt;- iter1; j &lt;- iter2} yield (i, j)).toList

#List[(Int, Int)] = List((1,1), (1,2), (1,3), (1,4), (1,5), (1,6))

(for { i &lt;- List(1,2,3,4,5,6).iterator; j &lt;- List(1,2,3,4,5,6).iterator} yield (i, j)).toList

# List[(Int, Int)] = List((1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (2,1), (2,2), (2,3), (2,4), (2,5), (2,6), (3,1), (3,2), (3,3), (3,4), (3,5), (3,6), (4,1), (4,2), (4,3), (4,4), (4,5), (4,6), (5,1), (5,2), (5,3), (5,4), (5,5), (5,6), (6,1), (6,2), (6,3), (6,4), (6,5), (6,6))

</code></pre>

<h1 id="toc_5"><font color='salmon'>? Strange java.io.NotSerializableException</font></h1>

<p>After I did <code>groupByKey</code> \(\rightarrow\) <code>KeyValueGroupedDataset</code>, then use <code>mapGroups</code>. If the function in <code>mapGroups</code> includes a custom defined function, I get <code>java.io.NotSerializableException</code></p>

<p>I summarized the question on <a href="http://stackoverflow.com/questions/42144617/spark-2-0-a-named-function-inside-mapgroups-for-sql-keyvaluegroupeddataset-caus">stackoverflow</a></p>

<h1 id="toc_6">Using Spark-shell REPL</h1>

<ul>
<li><p>Remember to <code>.unpersist()</code> if it was <code>cache()</code> before. Otherwise, even the variable is redefined, it won&#39;t update</p></li>
<li><p>Sometimes, restart the shell make things different</p></li>
</ul>

<h1 id="toc_7">Running spark job in oozie</h1>

<p>Cannot have both <code>spark2.jar</code> and <code>dpp-giraph-0.0.1-with-giraph-core.jar</code> cached. Some jar have versioning conflict</p>

<p>Error message like this (they are pretty confusing, and are all for the same reason):</p>

<blockquote>
<p>Caused by: java.io.InvalidClassException: org.apache.commons.lang3.time.FastDateFormat; local class incompatible: stream classdesc serialVersionUID = 2, local class serialVersionUID = 1<br/><br/>
ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoSuchFieldError: USE_DEFAULTS java.lang.NoSuchFieldError: USE_DEFAULTS at com.fasterxml.jackson.databind.introspect.JacksonAnnotationIntrospector.findSerializationInclusion(JacksonAnnotationIntrospector.java:498)<br/><br/>
ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.<init>(ZIIIIIII)V<br/>
java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.<init>(ZIIIIIII)V</p>
</blockquote>

<p>The workaround. Put these two jars in the oozie workflow&#39;s root folder (<strong>Not the <code>./lib/</code> folder</strong>) and in the workflow.xml at different <code>&lt;action&gt;</code> call the distributed cache using <code>&lt;file&gt;my.jar#my.jar&lt;/file&gt;</code>. Refer to example of <code>/home/yu/LP_tunning_17/sparkWorkflow</code></p>

<h1 id="toc_8">Another error for running spark job in oozie</h1>

<p>Error like below:</p>

<blockquote>
<p>java.lang.ClassNotFoundException: Class org.apache.spark.deploy.SparkSubmit not found</p>
</blockquote>

<p>This is because the lib for org.apache.spark path is not specified in the oozie job.properties</p>

<p>Wrong job.properties:</p>

<pre><code class="language-bash">oozie.libpath=/user/oozie/share/lib
</code></pre>

<p>Correct job.properties:</p>

<pre><code class="language-bash">oozie.libpath=/user/oozie/share/spark2
</code></pre>

<p><font color='red'><strong>Special Note:</strong> We cannot mix the two <code>libpath</code>, because the jars inside these two pathes have confliction! The below <code>job.properties</code> is wrong!</font></p>

<pre><code class="language-bash">oozie.libpath=/user/oozie/share/lib,/user/oozie/share/spark2
</code></pre>

<p>Mixing these two path will cause error like below:</p>

<blockquote>
<p>Caused by: java.io.InvalidClassException: org.apache.commons.lang3.time.FastDateFormat; local class incompatible: stream classdesc serialVersionUID = 2, local class serialVersionUID = 1</p>
</blockquote>

<p>or:</p>

<blockquote>
<p>java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.<init>(ZIIIIIII)V</p>
</blockquote>

<h1 id="toc_9">More dependency conflict for running oozie spark workflow</h1>

<ul>
<li><p><code>pig-0.12.0-cdh5.2.0.jar</code> jar are conflict with <code>spark</code> action. So it cannot be left inside the workflow&#39;s <code>lib</code> folder. It Will cause errors like below. Needed can use <code>&lt;file&gt;</code> action in workflow to distribute it.</p>

<p>Conflict Error</p>

<blockquote>
<p>java.lang.NoSuchMethodError: org.apache.hadoop.fs.FSOutputSummer.<init>(Lorg/apache/hadoop/util/DataChecksum;)V</p>
</blockquote>

<p>Missing Error</p>

<blockquote>
<p>java.lang.NoClassDefFoundError: org/apache/pig/PigServer</p>
</blockquote></li>
<li><p>Note that <code>dpp-pig-udf-0.0.1-SNAPSHOT.jar</code> has no conflict with spark actions and can be safely kept inside the <code>lib</code></p></li>
</ul>

<h1 id="toc_10">spark config</h1>

<p><a href="https://docs.google.com/document/d/1aER-bobtyf6omZxyFIOFlHPgwbgAwUmq7Aruom9cLu4/edit">zhuo&#39;s doc</a> remember to check his version for update.</p>

<h1 id="toc_11">graphX connectedComponent</h1>

<p><code>connectedComponents</code> requires the EdgeType not be null, VertexType can be null<br/>
Refer to the following code:</p>

<ul>
<li>Code does not work</li>
</ul>

<pre><code class="language-scala">// followers has type org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Nothing]]
// which means the EdgeType ED is Nothing
val followers = sc.parallelize(Array(Edge(2, 1), Edge(4, 1), Edge(1, 2), Edge(6, 3), Edge(7, 3), Edge(7, 6), Edge(6, 7), Edge(3, 7)))

// Note that the method fromEdges need specify the EdgeType and the VertexType, its signature is fromEdges[VD, ED](edges: RDD[Edge[ED]], defaultValue: VD)
val graph = Graph.fromEdges[Null, Nothing](followers, null)

// connectedComponents cannot be used on this type of graph
// error: value connectedComponents is not a member of org.apache.spark.graphx.Graph[Null,Nothing]
</code></pre>

<ul>
<li>Correct code:</li>
</ul>

<pre><code class="language-scala">// followers has type org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]]
// which means the EdgeType ED is Int, and I used a dummy value of 0
val followers = sc.parallelize(Array(Edge(2, 1, 0), Edge(4, 1, 0), Edge(1, 2, 0), Edge(6, 3, 0), Edge(7, 3, 0), Edge(7, 6, 0), Edge(6, 7, 0), Edge(3, 7, 0)))

// Note that the method fromEdges need specify the EdgeType and the VertexType, its signature is fromEdges[VD, ED](edges: RDD[Edge[ED]], defaultValue: VD)
val graph = Graph.fromEdges[Null, Int](followers, null)

// connectedComponents can be used now
val cc = graph.connectedComponents()
</code></pre>

<p><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.Graph$@fromEdges%5BVD,ED%5D(edges:org.apache.spark.rdd.RDD%5Borg.apache.spark.graphx.Edge%5BED%5D%5D,defaultValue:VD,edgeStorageLevel:org.apache.spark.storage.StorageLevel,vertexStorageLevel:org.apache.spark.storage.StorageLevel)(implicitevidence$16:scala.reflect.ClassTag%5BVD%5D,implicitevidence$17:scala.reflect.ClassTag%5BED%5D):org.apache.spark.graphx.Graph%5BVD,ED%5D">fromEdges document</a></p>

<h1 id="toc_12">Using of Bloom Filter</h1>

<p><a href="https://llimllib.github.io/bloomfilter-tutorial/">Basic explanation of bloom filter</a><br/>
The false positive rate is about \((1-e^{-\frac{-kn}{m}})^k\), in which <br/>
\(n\) is number of element expected to test, <br/>
\(k\) is the number of hash functions, <br/>
\(m\) is the number of bits to use.</p>

<p>I used the <code>breeze</code> implementation. The <a href="https://github.com/scalanlp/breeze/blob/releases/v0.13-RC1/math/src/main/scala/breeze/util/BloomFilter.scala">source code is here</a></p>

<p><a href="http://stackoverflow.com/a/31825468/4229125">Here is the reference code from stackoverflow</a></p>

<p>Zhuo&#39;s example is at <code>ge.drawbrid.dpp.spark2.example.BloomFilterJoin</code></p>

<p>My own usage:</p>

<pre><code class="language-scala">import breeze.util.BloomFilter

val bfLongIdCnt = sampleDS.count()
val bfLongId = sampleDS.mapPartitions({ iter =&gt; {
 val b = BloomFilter.optimallySized[Long](bfLongIdCnt.toDouble * 2, 0.05)
 iter.foreach(pair =&gt; {
   b += pair.longId1
   b += pair.longId2
 })
 Iterator(b)
}
}).rdd.treeReduce(_ | _)
val bfLongIdBroadcasted = sc.broadcast(bfLongId)

val indexTableDS = sc.textFile(indexTablePath).map(_.split(&quot;\t&quot;)).map(tokens =&gt; IndexTableData(tokens(0).trim.toLong, tokens(1).trim)).toDS()
val reducedIndexTable = indexTableDS.filter(indexTableData =&gt; bfLongIdBroadcasted.value.contains(indexTableData.longId)).
 mapPartitions({ iter =&gt; {
   var partitionMap = Map[String, Long]()
   iter.foreach(indexTableData =&gt; partitionMap += (indexTableData.stringId -&gt; indexTableData.longId))
   Iterator(partitionMap)
 }
 }).rdd.treeReduce(_ ++ _)
</code></pre>

<h1 id="toc_13">Add a jar inside the spark REPL</h1>

<p>Using the follow command inside the repl</p>

<pre><code class="language-bash">scala&gt; :require ./spark2.jar
Added &#39;/jbod/home/yu/spark_pg/./spark2.jar&#39; to classpath.
</code></pre>

<h1 id="toc_14">Get size of an object inside spark REPL</h1>

<ul>
<li>Use the <a href="https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/util/SizeEstimator.html"><code>SizeEstimator</code></a></li>
</ul>

<pre><code class="language-scala">import org.apache.spark.util.SizeEstimator.estimate

estimate(testObj)/(1024*1024).toFloat // convert to MB
</code></pre>

<ul>
<li>Another way is <code>cache</code> it and check the memory usage from the web UI</li>
</ul>

<h1 id="toc_15">Configuration that WON&#39;T work</h1>

<ul>
<li><p>phased out already    </p></li>
</ul>

<ol>
<li><code>--conf spark.sql.shuffle.partitions=12000</code></li>
</ol>

<h1 id="toc_16">My experience for OOM debugging</h1>

<ol>
<li>Spark 2.0 will automatically do the broadcasting for you. Normally we don&#39;t need worry about that. However, this type of broadcasting is lazy, for some large broadcasting object causing OOM (java heap space) it will be hard to identify its cause. So <font color='salmon'><strong>explicitly broadcast estimated large object helps.</strong></font></li>
<li>For large spark programs, break it up into sections. Run only one section to identify the OOM cause</li>
<li>Use the Application Master log page. It aggregates logs from all workers, can get valuable information, especially about memory usage. But sometimes the Error or Exceptions won&#39;t cause the failure of the whole program (especially network Errors and Exceptions) Spark will automatically restart part of the calculation.</li>
</ol>

<h1 id="toc_17">Date in Spark Dataset</h1>

<p>Use <code>java.sql.Date</code> instead of <code>java.util.Date</code>. Otherwise, there will be Encoder Error.<br/>
<a href="http://stackoverflow.com/questions/31312108/java-util-date-is-not-supported">reference to Stackoverflow</a></p>

<h1 id="toc_18">Encoder Issue in Dataset</h1>

<p><a href="http://stackoverflow.com/questions/36648128/how-to-store-custom-objects-in-a-dataset">Stackoverflow has a good post for this</a></p>

<h2 id="toc_19">Unable to find Encoder at Compiling time</h2>

<p>Error occurs in the Gradle build (compiling) time. Message is as below:</p>

<blockquote>
<p>Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.</p>
</blockquote>

<p>I don&#39;t know the exactly cause, but seems it is related with the <code>mapPartitions</code> on <code>Dataset</code>, which is different than <code>mapPartitions</code> on <code>RDD</code>. If we change the <code>Dataset</code> into <code>RDD</code> first, then use the <code>mapPartitions</code> there will be no issue.</p>

<p>Also worth noticing, the <code>Dataset</code>&#39;s <code>mapPartitions</code> is an experimental and evolving method as of today according to the official doc.</p>

<h2 id="toc_20">Unable to find Encoder at Run time</h2>

<p>A easy workaround is to avoid Dataset for now and turn back to RDD.</p>

<p>An example of code does not work in Spark 2.0 because of Encoder issue:</p>

<pre><code class="language-scala">case class ClusterBaseStats(clusterId: Long, cookieBag: Set[Long], deviceBag: Set[Long])

val isCookie = (id: Long) =&gt; id % 2 == 1 

/**
    * Assumes the LP output has no duplicates
    *
    * @param lpOutputPath          : String. output from LP on HDFS. Schema should be tab separated (longId, longClusterID, intNumNeighbourWithThisId)
    * @param clusterSizeUpperBound : Int. Default 40. Inclusive bound of cluster size that will be considered of getting the stats.
    * @return ClusterBaseStats: base stats (for cluster size &lt;= clusterSizeUpperBound and cluster size &gt; 1) as an intermediate results which will be used as input to get the real stats and avoid repeat calculation
    */
  def getClusterBaseStats(lpOutputPath: String, clusterSizeUpperBound: Int = 40, sparkSession: SparkSession): Dataset[ClusterBaseStats] = {
    import sparkSession.implicits._
    val lpOutputSchema = StructType(Array(
      StructField(&quot;longId&quot;, LongType, nullable = false),
      StructField(&quot;clusterId&quot;, LongType, nullable = false),
      StructField(&quot;numNeighbour&quot;, IntegerType, nullable = true)))

    sparkSession.read.option(&quot;sep&quot;, &quot;\t&quot;).schema(lpOutputSchema).csv(lpOutputPath).drop(&quot;numNeighbour&quot;).
      rdd.map(row =&gt; (row.getLong(1), row.getLong(0))).
      aggregateByKey((mutable.HashSet.empty[Long], mutable.HashSet.empty[Long]))(seqOp = {
        case ((cookieBag, deviceBag), longId) =&gt;
          if (isCookie(longId)) (cookieBag += longId, deviceBag)
          else (cookieBag, deviceBag += longId)
      },
        combOp = {
          case ((cookieBag1, deviceBag1), (cookieBag2, deviceBag2)) =&gt;
            (cookieBag1 ++= cookieBag2, deviceBag1 ++= deviceBag2)
        }).
      map({ case (clusterId, (cookieBag, deviceBag)) =&gt; ClusterBaseStats(clusterId, cookieBag.toSet, deviceBag.toSet) }).
      toDS().
      filter(clusterBaseStats =&gt; {
        val clusterSize = clusterBaseStats.cookieBag.size + clusterBaseStats.deviceBag.size
        clusterSize &lt;= clusterSizeUpperBound &amp;&amp; clusterSize &gt; 1
      })
  }
</code></pre>

<p>This code compiles okay, but Error message at Run time:</p>

<blockquote>
<p>java.lang.UnsupportedOperationException: No Encoder found for Set[scala.Long]<br/>
- field (class: &quot;scala.collection.immutable.Set&quot;, name: &quot;cookieBag&quot;)<br/>
- root class: &quot;ge.drawbrid.dpp.spark2.util.ClusterBaseStats&quot;</p>
</blockquote>

<p>I rewrote it using RDD, them everything is fine:</p>

<pre><code class="language-scala">/**
    * Assumes the LP output has no duplicates
    *
    * @param lpOutputPath          : String. output from LP on HDFS. Schema should be tab separated (longId, longClusterID, intNumNeighbourWithThisId)
    * @param clusterSizeUpperBound : Int. Default 40. Inclusive bound of cluster size that will be considered of getting the stats.
    * @return RDD[(Long, (mutable.HashSet[Long], mutable.HashSet[Long]))]: (clusterId: Long, cookieBag: Set[Long], deviceBag: Set[Long]), for cluster size &lt;= clusterSizeUpperBound and cluster size &gt; 1) as an intermediate results which will be used as input to get the real stats and avoid repeat calculation
    */
  def getClusterBaseStats(lpOutputPath: String, clusterSizeUpperBound: Int = 40, sparkSession: SparkSession): RDD[(Long, (mutable.HashSet[Long], mutable.HashSet[Long]))] = {
    val lpOutputSchema = StructType(Array(
      StructField(&quot;longId&quot;, LongType, nullable = false),
      StructField(&quot;clusterId&quot;, LongType, nullable = false),
      StructField(&quot;numNeighbour&quot;, IntegerType, nullable = true)))

    sparkSession.read.option(&quot;sep&quot;, &quot;\t&quot;).schema(lpOutputSchema).csv(lpOutputPath).drop(&quot;numNeighbour&quot;).
      rdd.map(row =&gt; (row.getLong(1), row.getLong(0))).
      aggregateByKey((mutable.HashSet.empty[Long], mutable.HashSet.empty[Long]))(seqOp = {
        case ((cookieBag, deviceBag), longId) =&gt;
          if (isCookie(longId)) (cookieBag += longId, deviceBag)
          else (cookieBag, deviceBag += longId)
      },
        combOp = {
          case ((cookieBag1, deviceBag1), (cookieBag2, deviceBag2)) =&gt;
            (cookieBag1 ++= cookieBag2, deviceBag1 ++= deviceBag2)
        }).
      filter({ case (clusterId, (cookieBag, deviceBag)) =&gt; {
        val clusterSize = cookieBag.size + deviceBag.size
        clusterSize &lt;= clusterSizeUpperBound &amp;&amp; clusterSize &gt; 1
      }
      })
  }
</code></pre>

<h1 id="toc_21">Split Dataset and save them separately simultaneously</h1>

<p>In spark there is no way to split RDD/Dataset into multiple RDDs/Datasets. But if we just wanna split them and save them into different folders we can do the following:</p>

<ol>
<li>Generate new column whose value can be used to split the RDD/Dataset</li>
<li>use <code>.partitionBy()</code></li>
<li>Then save</li>
</ol>

<p>Example:</p>

<pre><code class="language-scala">case class GpairingInputMultipleFiles(saveType: String, ts: Long, id: String, ip: String, pfc: Int, freq: Int, devtp: String)

case class GpairingInput(ip: String, ts: Long, id: String, pfc: Int, freq: Int, devtp: String) {
  val convert: (String) =&gt; GpairingInputMultipleFiles = { saveType: String =&gt;
    GpairingInputMultipleFiles(saveType, ts, id, ip, pfc, freq, devtp)
  }
}

    val gpairingInputMultipleFiles = gpairingInput.groupByKey(_.id).flatMapGroups((id, iter) =&gt; {
      val groupArr = ArrayBuffer[GpairingInputMultipleFiles]()
      if (id.startsWith(&quot;uuid&quot;) &amp;&amp; iter.length == 1) groupArr += iter.next().convert(&quot;singleObs&quot;)
      else {
        iter.foreach(gpairingInput =&gt; {
          if (gpairingInput.pfc == 1) groupArr += gpairingInput.convert(&quot;dev&quot;)
          else if (gpairingInput.pfc == 2) groupArr += gpairingInput.convert(&quot;mob&quot;)
          else if (gpairingInput.pfc == 3) groupArr += gpairingInput.convert(&quot;web&quot;)
        })
      }
      groupArr.iterator
    })

    val gpairingInputPath = &quot;alibaba_2/firstRun/gpairing_input/&quot;
    gpairingInputMultipleFiles.write.partitionBy(&quot;saveType&quot;).mode(SaveMode.Overwrite).format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;\t&quot;).save(gpairingInputPath)

</code></pre>

<h1 id="toc_22">Hint the correct side of broadcast join</h1>

<p>The trick is calling <code>broadcast()</code> on the smaller RDD/Dataset/DataFrame</p>

<pre><code class="language-scala">import org.apache.spark.sql.functions.broadcast

largedataframe.join(broadcast(smalldataframe), ...)
</code></pre>

<p><a href="http://stackoverflow.com/a/39404486/4229125">stackoverflow</a></p>

<h1 id="toc_23">Schema after join of two Dataset</h1>

<p>for Dataset, we have the <code>left.join(right, usingColumns: Seq[String]): DataFrame</code></p>

<p>The official document says:</p>

<blockquote>
<p>Inner equi-join with another DataFrame using the given column.<br/>
 Different from other join functions, the join column will only appear once in the output, i.e. similar to SQL&#39;s JOIN USING syntax.</p>
</blockquote>

<pre><code class="language-scala">// Joining df1 and df2 using the column &quot;user_id&quot;
df1.join(df2, &quot;user_id&quot;)
</code></pre>

<blockquote>
<p>usingColumn: Name of the column to join on. This column must exist on both sides.</p>
</blockquote>

<p>The schema after join is in this order<br/>
<code>usingColumns(eg. &quot;user_id&quot;), df1 (except for the usingColumns), df2 (except for the usingColumns)</code></p>

<p>This is based on the <a href="http://docs.oracle.com/javadb/10.6.2.1/ref/rrefsqljusing.html">SQL USING syntax documentation</a> </p>

<blockquote>
<p>When a USING clause is specified, an asterisk (*) in the select list of the query will be expanded to the following list of columns (in this order):</p>

<p>All the columns in the USING clause<br/>
All the columns of the first (left) table that are not specified in the USING clause<br/>
All the columns of the second (right) table that are not specified in the USING clause</p>
</blockquote>

<h1 id="toc_24">Handling A file sequentially</h1>

<p>Giraph output during Computation append all superSteps into one file. Need be able to divide the superSteps without changing the vertex value class (If it is possible, it should be much easier to just change the vertex value class to attach a field of current superStep)</p>

<p>One thing worths notice is that not all vertices output at every super step. If at a super step a vertex is not awaken, it will not output.</p>

<p>The following code works. Used mapPartitions</p>

<pre><code class="language-scala">  def getLpOutputWithSS(sparkSession: SparkSession, lpOutputPath: String): Dataset[LpOutputWithSS] = {
    import sparkSession.implicits._
    val lpOutputPath = &quot;LP_tunning_17/scrummageLPonNF/testLP/&quot;
    val lpOutput = sparkSession.read.option(&quot;sep&quot;, &quot;\t&quot;).
      schema(Encoders.product[LpOutputSchema].schema).
      csv(lpOutputPath).drop(&quot;numNeighbour&quot;)

    val lpOutputWithSS = lpOutput.mapPartitions(rowIter =&gt; {
      val partitionArr = ArrayBuffer[LpOutputWithSS]()
      var ssCounter = Map[Long, Int]()
      rowIter.foreach(row =&gt; {
        val longId = row.getLong(0)
        val clusterId = row.getLong(1)
        val ss = ssCounter.getOrElse(longId, -1) + 1
        ssCounter += (longId -&gt; ss)
        partitionArr.append(LpOutputWithSS(longId, clusterId, ss))
      })
      partitionArr.iterator
    })
    lpOutputWithSS
  }
</code></pre>

<p>The following uses RDD, gives exactly the same result</p>

<pre><code class="language-scala">  def getLpOutputWithSS(sparkSession: SparkSession, lpOutputPath: String): Dataset[LpOutputWithSS] = {
    import sparkSession.implicits._

    val sc = sparkSession.sparkContext
    val lpOutput = sc.textFile(lpOutputPath).mapPartitions(stringIter =&gt; {
      val partitionArr = ArrayBuffer[LpOutputWithSS]()
      var ssCounter = Map[Long, Int]()
      stringIter.foreach(str =&gt; {
        val tokens = str.split(&quot;\t&quot;)
        val longId = tokens(0).toLong
        val clusterId = tokens(1).toLong
        val ss = ssCounter.getOrElse(longId, -1) + 1
        ssCounter += (longId -&gt; ss)
        partitionArr.append(LpOutputWithSS(longId, clusterId, ss))
      })
      partitionArr.iterator
    }).toDS()
    lpOutput
  }

</code></pre>

<p>Using RDD maybe more reliable. <a href="http://www.infoobjects.com/spark-core-sc-textfile-vs-sc-wholetextfiles/">A blog talks about the sequentially reading</a></p>

<blockquote>
<p>sc.textFile<br/>
SparkContext’s TextFile method, i.e., sc.textFile in Spark Shell, creates a RDD with each line as an element. If there are 10 files in movies folder, 10 partitions will be created.</p>

<p>sc.wholeTextFiles<br/>
SparkContext’s whole text files method, i.e., sc.wholeTextFiles in Spark Shell, creates a PairRDD with the key being the file name with a path. It’s a full path like “hdfs://m1.zettabytes.com:9000/user/hduser/movies/movie1.txt”. The value is the whole content of file in String. Here the number of partitions will be 1 or more depending upon how many executor cores you have.</p>
</blockquote>

<h1 id="toc_25">Type Class Reflect of Spark Scala</h1>

<p>I know this is kind of big topic. I don&#39;t have time to understand it all. Just keep a record where I am and some temporary workaround</p>

<p>From Jenny:<br/>
<code>ge.drawbrid.dpp.spark2.feature.TextFormatWriter#writeTextOutput</code></p>

<pre><code class="language-scala">def writeTextOutput[ViewMapping &lt;: Product : TypeTag](spark: SparkSession, data: Dataset[ViewMapping], outputPath: String)={

    import spark.implicits._
    data.write.mode(SaveMode.Overwrite)
      .format(&quot;com.databricks.spark.csv&quot;)
      .option(&quot;header&quot;, &quot;false&quot;)
      .option(&quot;codec&quot;, &quot;com.hadoop.compression.lzo.LzopCodec&quot;)
      .option(&quot;delimiter&quot;, COL_DELIM)
      .save(outputPath);

  }
</code></pre>

<h1 id="toc_26">Spark shell Class initialization Error</h1>

<p>Sometimes it is because duplicated definition of case class. For example, if you cached a dataset, which uses a case class A, and later, you redefined the case class using exactly the same signature as A. Both these two classes previous A and current A exist in the spark shell and confuses application.</p>

<p>Likewise, some other wired behaviors if only in spark shell, may be due to the fact of cached old values. The work around is <code>:reset</code>. Or just simply re-start the shell.</p>

<h1 id="toc_27">Spark date timezone converting</h1>

<p>It is kind of tricky to convert the timezone. <a href="http://thread.gmane.org/gmane.comp.lang.scala.spark.user/27374">Discussion thread</a></p>

<p>Some suggest to use <code>joda-time</code> library, <a href="http://stackoverflow.com/a/34669345/4229125">like in this post</a> however, the central idea is using this library to convert it to formatted standard string, then converted to other datetype</p>

<p>My workaround is because our hadoop system are all using UTC. Therefore I can use the <code>sql.functions._</code> to directly convert from Java time (long in millisecond) to Unix time (long in second), to spark timestamp (using local timezone, which is UTC), then to the timezone that I want.</p>

<pre><code class="language-scala">umidTraffic.withColumn(&quot;BJ_date&quot;, from_utc_timestamp(from_unixtime($&quot;ts&quot; / 1000), &quot;Asia/Shanghai&quot;)).show()
</code></pre>

<p>The standard way of using <code>joda-time</code> library is like this:</p>

<pre><code class="language-scala">import org.joda.time.DateTime
import org.joda.time.DateTimeZone
val beijingTZ = DateTimeZone.forID(&quot;Asia/Shanghai&quot;)

def longTsToBJDateStr(ts:Long) : String = {
    (new DateTime(ts*1000, beijingTZ)).toString(&quot;yyyyMMdd&quot;)
}

longTsToBJDateStr(1487422472)
</code></pre>

<h1 id="toc_28">Another Spark from date string to <code>DateType</code> converting</h1>

<p>The sql function <code>to_date()</code> converts the string format to <code>DateType</code>, however it only take standard date string format like <code>yyyy-MM-dd</code>, if we have string format like <code>yyyyMMdd</code> we need add bridge to convert, like below:</p>

<pre><code class="language-scala">val umid_ip_date = umid_log.select(&quot;umid&quot;, &quot;ip&quot;, &quot;date&quot;).distinct.select($&quot;umid&quot;, $&quot;ip&quot;, to_date(from_unixtime(unix_timestamp($&quot;date&quot;, &quot;yyyyMMdd&quot;))).as(&quot;date&quot;))
val umid_ip_oneDayShift = umid_ip_date.select($&quot;umid&quot;, $&quot;ip&quot;, date_add($&quot;date&quot;, 1).as(&quot;date&quot;)).
                            union(umid_ip_date.select($&quot;umid&quot;, $&quot;ip&quot;, date_add($&quot;date&quot;, -1).as(&quot;date&quot;)))   
</code></pre>

<h1 id="toc_29">Config spark jars in a folder</h1>

<p>This is the correct way of specifying a lot of jars in a folder:<br/>
<code>spark.yarn.jars</code> \(\rightarrow\) <code>hdfs://sc2prod/user/oozie/share/spark2/*.jar</code></p>

<h1 id="toc_30">Using python inside Spark (Scala) code</h1>

<p>This is really cool. I was trying to parse the User Agent to see whether it is a mobile device or a desktop device (another topic). <a href="http://detectmobilebrowsers.com/">This website : detectmobilebrowsers</a> is recommended by a lot of posts. It has a python script implementation, I downloaded the script and want to use it inside spark.</p>

<p>The way of using it is as follow, (only works for RDD though):</p>

<ul>
<li><p>I saved the python script on local folder: <code>/home/yu/alibaba_3/gpairing_input/python/mob_desktop.py</code>.</p></li>
<li><p>Inside the python script main body, I am reading from <code>sys.stdin</code> and processing it</p></li>
</ul>

<pre><code class="language-python">#! /usr/local/bin/python2.7
import re
import sys

reg_b = re.compile(r&quot;(android|bb\\d+|meego).+mobile|avantgo|bada\\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\\.(browser|link)|vodafone|wap|windows ce|xda|xiino&quot;, re.I|re.M)
reg_v = re.compile(r&quot;1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\\-(n|u)|c55\\/|capi|ccwa|cdm\\-|cell|chtm|cldc|cmd\\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\\-s|devi|dica|dmob|do(c|p)o|ds(12|\\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\\-|_)|g1 u|g560|gene|gf\\-5|g\\-mo|go(\\.w|od)|gr(ad|un)|haie|hcit|hd\\-(m|p|t)|hei\\-|hi(pt|ta)|hp( i|ip)|hs\\-c|ht(c(\\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\\-(20|go|ma)|i230|iac( |\\-|\\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\\/)|klon|kpt |kwc\\-|kyo(c|k)|le(no|xi)|lg( g|\\/(k|l|u)|50|54|\\-[a-w])|libw|lynx|m1\\-w|m3ga|m50\\/|ma(te|ui|xo)|mc(01|21|ca)|m\\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\\-2|po(ck|rt|se)|prox|psio|pt\\-g|qa\\-a|qc(07|12|21|32|60|\\-[2-7]|i\\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\\-|oo|p\\-)|sdk\\/|se(c(\\-|0|1)|47|mc|nd|ri)|sgh\\-|shar|sie(\\-|m)|sk\\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\\-|v\\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\\-|tdg\\-|tel(i|m)|tim\\-|t\\-mo|to(pl|sh)|ts(70|m\\-|m3|m5)|tx\\-9|up(\\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\\-|your|zeto|zte\\-&quot;, re.I|re.M)

def isMobile(ua):
    b = reg_b.search(ua)
    v = reg_v.search(ua[0:4])
    if b or v:
        return True
    return False

for line in sys.stdin:
    print isMobile(line)
</code></pre>

<ul>
<li>In spark, I put this python script to all executors by <code>SparkContext.addFile(path)</code>, so that it can be run on all executors. After the <code>addFile</code>, the script file will be send to all executor to the current working directory. </li>
</ul>

<pre><code class="language-scala">import org.apache.spark.SparkFiles

val distScript = &quot;/home/yu/alibaba_3/gpairing_input/python/mob_desktop.py&quot;
sc.addFile(distScript)
val distScriptName = &quot;./mob_desktop.py&quot; // The script is at cwd for all exectuors
val pythonRes = acookieInput.map(_.getAs[String](&quot;useragent&quot;)).rdd.pipe(distScriptName).map(str =&gt; str.toBoolean).take(200) // pipe is what does the job.
</code></pre>

<p><a href="http://stackoverflow.com/a/32978183/4229125">reference post</a></p>

<h1 id="toc_31">DataType inside Dataset Row</h1>

<p><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row@apply(i:Int):Any">official doc.</a></p>

<pre><code>BooleanType -&gt; java.lang.Boolean
ByteType -&gt; java.lang.Byte
ShortType -&gt; java.lang.Short
IntegerType -&gt; java.lang.Integer
FloatType -&gt; java.lang.Float
DoubleType -&gt; java.lang.Double
StringType -&gt; String
DecimalType -&gt; java.math.BigDecimal

DateType -&gt; java.sql.Date
TimestampType -&gt; java.sql.Timestamp

BinaryType -&gt; byte array
ArrayType -&gt; scala.collection.Seq (use getList for java.util.List)
MapType -&gt; scala.collection.Map (use getJavaMap for java.util.Map)
StructType -&gt; org.apache.spark.sql.Row
</code></pre>

<h1 id="toc_32">Read and Write in LZO format</h1>

<p>Need have the <code>hadoop-lzo</code> jar loaded into spark (see notes in zeppelin) <br/>
<code>z.load(&quot;/usr/lib/hadoop/lib/hadoop-lzo.jar&quot;)</code> </p>

<p>Use the <code>option(&quot;codec&quot;, &quot;com.hadoop.compression.lzo.LzopCodec&quot;)</code> to read and write.</p>

<p>Example code of reading <code>LzoPigStorage(&#39;\u0001&#39;)</code> outputed file.</p>

<pre><code class="language-scala">import org.apache.spark.sql.Encoders

case class GpairingInput(ts: Long, id: String, ip: String, pfc:Int, freq: Int, devtp: String)

val first_singleObs_path = &quot;/user/yu/alibaba_2/firstRun/freq/single-obs&quot;
val first_singleObs = sparkSession.read.option(&quot;sep&quot;, &quot;\u0001&quot;).
        option(&quot;codec&quot;, &quot;com.hadoop.compression.lzo.LzopCodec&quot;).
        schema(Encoders.product[GpairingInput].schema).
        csv(first_singleObs_path).as[GpairingInput]
        
val acookieType = acookieFromMY.select(concat(lit(&quot;uuid|&quot;), $&quot;acookie&quot;).as(&quot;id&quot;), $&quot;isMob&quot;)
val first_singleObsWType = first_singleObs.join(acookieType, usingColumn = &quot;id&quot;).
        select(&quot;ts&quot;, &quot;id&quot;, &quot;ip&quot;, &quot;pfc&quot;, &quot;freq&quot;, &quot;devtp&quot;, &quot;isMob&quot;)

val second_singleObs_outputpath_mweb = &quot;/user/yu/alibaba_2/secondRun/mweb/freq/single-obs&quot;
val second_singleObs_outputpath_dweb = &quot;/user/yu/alibaba_2/secondRun/dmweb/freq/single-obs&quot;

first_singleObsWType.filter($&quot;isMob&quot;).drop(&quot;isMob&quot;).write.format(&quot;csv&quot;).
    option(&quot;sep&quot;, &quot;\t&quot;).option(&quot;codec&quot;, &quot;com.hadoop.compression.lzo.LzopCodec&quot;).
    mode(&quot;overwrite&quot;).save(second_singleObs_outputpath_mweb)
first_singleObsWType.filter(not($&quot;isMob&quot;)).drop(&quot;isMob&quot;).write.format(&quot;csv&quot;).
    option(&quot;sep&quot;, &quot;\t&quot;).option(&quot;codec&quot;, &quot;com.hadoop.compression.lzo.LzopCodec&quot;).
    mode(&quot;overwrite&quot;).save(second_singleObs_outputpath_dweb)
</code></pre>

<p><strong><font color='red'>Very Important</font></strong><br/>
It is <code>LzopCodec</code> not <code>LzoCodec</code>. If accidentally used the latter one, it will generate <code>lzo_deflate</code> files, which can not be read using normal <code>LzoPigStorage()</code> method.<br/><br/>
<a href="http://stackoverflow.com/questions/16676967/how-to-decompress-lzo-deflate-file">Stackoverflow post</a></p>

<h1 id="toc_33">UDF for Dataset&#39;s column operation</h1>

<p>Easy to learn from example</p>

<pre><code class="language-scala">import org.apache.spark.sql.functions._

val uuidStripper: (String =&gt; String) = (uuid: String) =&gt; uuid.substring(uuid.lastIndexOf(&quot;|&quot;)+1)
val uuidStripperFunc = udf(uuidStripper)


val feat2_maxpair360_path=&quot;alibaba_2/thirdRun/rank/features-all/CD/ALL&quot;
val feat2_maxpair360 = sparkSession.read.option(&quot;sep&quot;, &quot;\t&quot;).csv(feat2_maxpair360_path)
val feat2_maxpair360_join = feat2_maxpair360.withColumn(&quot;umid&quot;, uuidStripperFunc(col(feat2_maxpair360.columns(0)))).
        withColumn(&quot;acookie&quot;, uuidStripperFunc(col(feat2_maxpair360.columns(1))))
        
</code></pre>

<p><em>Note</em>: The <code>udf</code> function is from <code>org.apache.spark.sql.functions</code> package</p>

<p>The above udf actually is exactly <code>substring_index</code> <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$@substring_index(str:org.apache.spark.sql.Column,delim:String,count:Int):org.apache.spark.sql.Column">doc</a></p>

<pre><code>val feat2_maxpair360_join = feat2_maxpair360.
        withColumn(&quot;umid&quot;, substring_index(col(feat2_maxpair360.columns(0)), &quot;|&quot;, -1)).
        withColumn(&quot;acookie&quot;, substring_index(col(feat2_maxpair360.columns(1)), &quot;|&quot;, -1))
</code></pre>

<h1 id="toc_34">Kill a problematic executor</h1>

<p>When I run a very simple task from spark repl (inside Zeppelin notebook), one executor always blocks the process.</p>

<p>Go to the spark webUI, and found that the problematic executor is <code>55 / sc2-hmr15.drawbrid.ge</code>. It always shows as <code>RUNNING</code>. get the <code>Executors</code> page and find this exectuor and use the thread dump, found that the <code>RUNNABLE</code> task are blocked at:</p>

<pre><code>java.net.SocketInputStream.socketRead0(Native Method)  
java.net.SocketInputStream.socketRead(SocketInputStream.java:116)  
java.net.SocketInputStream.read(SocketInputStream.java:170)  
java.net.SocketInputStream.read(SocketInputStream.java:141)  
</code></pre>

<p>There are a lot posts online talking about a bug in the <code>HttpClient</code> lib etc, like <a href="http://stackoverflow.com/questions/38606653/spark-stateful-streaming-job-hangs-at-checkpointing-to-s3-after-long-uptime">this one at stackoverflow</a> or <a href="http://asyncified.io/2016/08/13/leveraging-spark-speculation-to-identify-and-re-schedule-slow-running-tasks/">this post in more detail</a>. What I want to do is just kill the executor and let the spark reassign a new executor, then everything will be fine.</p>

<p>So I <br/>
1. find the container&#39;s id for this executor on this spark job, by checking the executor&#39;s stdout page<br/>
2. Logged into the machine. Go to <code>/var/log/hadoop-yarn</code> and grep the containerId from the log file, from there I can find the <code>ProcessTree 11626 for container-id</code><br/>
3. Double check the pid is correct, by <code>ps 11626 | less</code><br/>
4. Then kill this pid: <code>sudo kill 11626</code></p>

<p>A new executor is assigned! Problem solved.</p>

<h1 id="toc_35">cache and persist is a transform</h1>

<p>I often see that code runs fine in <code>spark-submit</code> but not in spark REPL<br/>
One of the possible reason is the <code>cache()</code> or <code>persist()</code> command.</p>

<p><code>cache()</code> or <code>persist()</code> are a <code>transform</code>, should assign them back to themselves. If put it in a separate line in REPL it will cause problem. In <code>spark-submit</code>, because the code is compiled, the error is silently fixed.</p>

<p>Wrong code:</p>

<pre><code class="language-scala">val reducedConvertedLabelDS =
      if (LPOutputMetricUtil.fileExist(sparkSession, reducedConvertedLabelPath + SUCCEED_STRING))
        sparkSession.read.parquet(reducedConvertedLabelPath).as[ConvertedLabelData]
      else
        LPOutputMetricUtil.reduceConvertLabel(sparkSession, labelInputPath,
          reducedIndexTable = reducedIndexTableDS, labelOutputPath = reducedConvertedLabelPath)
          
reducedConvertedLabelDS.cache()
</code></pre>

<h1 id="toc_36">Another possible issue about cache</h1>

<p>maybe not related with <code>cache()</code></p>

<p>The error message looks like:</p>

<blockquote>
<p>java.lang.IllegalArgumentException: requirement failed: PartitioningCollection requires all of its partitionings have the same numPartitions.<br/>
  at scala.Predef$.require(Predef.scala:219)</p>
</blockquote>

<p>I had this problem because:</p>

<pre><code class="language-scala">val idx_path = &quot;/user/yu/alibaba_2/thirdRun_maxPair360/pair/idx&quot;
val idxStrcutType = StructType(Seq(StructField(&quot;strId&quot;, StringType, false), StructField(&quot;longId&quot;, LongType, false)))
val idx =  sparkSession.read.option(&quot;sep&quot;, &quot;\u0001&quot;).schema(idxStrcutType).csv(idx_path)
val gpcString = gpc.join(idx, $&quot;smallId&quot; === $&quot;longId&quot;).drop(&quot;smallId&quot;, &quot;longId&quot;).
        withColumnRenamed(&quot;strId&quot;, &quot;smallId&quot;).
        join(idx, $&quot;largeId&quot; === $&quot;longId&quot;).drop(&quot;largeId&quot;, &quot;longId&quot;).
        withColumnRenamed(&quot;strId&quot;, &quot;largeId&quot;).
        map(row =&gt; {
            if (row.getAs[String](&quot;smallId&quot;).startsWith(&quot;u&quot;))
                (row.getAs[String](&quot;largeId&quot;), row.getAs[String](&quot;smallId&quot;))
            else
                 (row.getAs[String](&quot;smallId&quot;), row.getAs[String](&quot;largeId&quot;))
        }).toDF(&quot;umid&quot;, &quot;acookie&quot;).filter(row =&gt; row.getAs[String](&quot;acookie&quot;).startsWith(&quot;u&quot;)).
        select(substring_index($&quot;umid&quot;, &quot;|&quot;, -1).as(&quot;umid&quot;), substring_index($&quot;acookie&quot;, &quot;|&quot;, -1).as(&quot;acookie&quot;)).cache()
</code></pre>

<p>If I took the last <code>cache()</code> off, it runs okay</p>

<p>However, after checking on <a href="http://stackoverflow.com/questions/39780784/spark-2-0-0-error-partitioningcollection-requires-all-of-its-partitionings-have">this post on stackoverflow</a>. Seems this is not caused by <code>cache()</code>. Maybe something wrong with the REPL. Using <code>.repartition()</code> can solve this problem.</p>

<h1 id="toc_37">parse URL get domain</h1>

<p>Take the reference from <a href="http://stackoverflow.com/a/4826122/4229125">this stackoverflow post</a></p>

<p>rewrote in Scala. It is more robust than <code>java.net.URL</code></p>

<pre><code class="language-scala">def getHost(url:String): String = {
    if(url == null || url.length == 0)
        return &quot;&quot;

    val doubleslash = 
        if (url.indexOf(&quot;//&quot;) == -1) 0
        else url.indexOf(&quot;//&quot;) + 2

    val end = 
        if (url.indexOf(&#39;/&#39;, doubleslash) == -1) url.length
        else url.indexOf(&#39;/&#39;, doubleslash)

    val port = url.indexOf(&#39;:&#39;, doubleslash)
    
    val realEnd =
        if (port &gt; 0 &amp;&amp; port &lt; end) port
        else end

    url.substring(doubleslash, realEnd);
}

def getDomain(url:String): String = {
    val host = getHost(url)

    var startIndex = 0
    var nextIndex = host.indexOf(&#39;.&#39;)
    var lastIndex = host.lastIndexOf(&#39;.&#39;)
    while (nextIndex &lt; lastIndex) {
        startIndex = nextIndex + 1
        nextIndex = host.indexOf(&#39;.&#39;, startIndex)
    }
    
    if (startIndex &gt; 0)
        host.substring(startIndex)
    else
        host
}
</code></pre>

<h1 id="toc_38">Get Seq of Tuple from DataFrame Row</h1>

<p>One column of the DataFrame is generated by <code>Map.toSeq</code>, so that column is a <code>Seq</code> of <code>tuple2 (key, value)</code>. When I want to extract the <code>key</code> out of the column, I cannot use <code>row.getAs[Seq(String, Int)](&quot;colName&quot;)</code></p>

<p>setup:</p>

<pre><code class="language-scala">val umid_log_path = &quot;/user/yu/alibaba_2/test_a/drawbridge_test_data_umid_log.csv&quot;
val umid_log = sparkSession.read.option(&quot;sep&quot;, &quot;;&quot;).option(&quot;quote&quot;, &quot;\&quot;&quot;).option(&quot;header&quot;, true).csv(umid_log_path)
val umid_convert = umid_log.map(row =&gt; {
    val id = row.getAs[String](&quot;umid&quot;)
    val app_id = row.getAs[String](&quot;app_id&quot;)
    val province = row.getAs[String](&quot;province&quot;).substring(0, 2)
    (id, app_id, province)}).toDF(&quot;id&quot;, &quot;app_id&quot;, &quot;province&quot;)
val umid_aggregated = umid_convert.repartition($&quot;id&quot;).groupByKey(row =&gt; row.getAs[String](&quot;id&quot;)).
        mapGroups({case (id, iterRow) =&gt; {
            var app_idMap = mutable.HashMap[String, Int]()
            val provinceMap = mutable.HashMap[String, Int]()
            for (row &lt;- iterRow) {
                app_idMap.update(row.getAs[String](&quot;app_id&quot;), app_idMap.getOrElse(row.getAs[String](&quot;app_id&quot;), 0)+1)
                provinceMap.update(row.getAs[String](&quot;province&quot;), provinceMap.getOrElse(row.getAs[String](&quot;province&quot;), 0)+1)
            }
            (id, app_idMap.toSeq, provinceMap.toSeq)
        } }).toDF(&quot;id&quot;, &quot;app_idBag&quot;, &quot;provinceBag&quot;)

umid_aggregated.show()
</code></pre>

<p>Does <strong><font color='red'>NOT</font></strong> work:</p>

<pre><code class="language-scala">val umid_appData = umid_aggregated.map(row =&gt; {
            val id = row.getAs[String](&quot;id&quot;)
            val app_id = row.getAs[Seq[(String, Int)]](&quot;app_idBag&quot;).map(_._1)
            (id, app_id)
            }).toDF(&quot;id&quot;, &quot;app_id&quot;)
</code></pre>

<p>Error message:</p>

<blockquote>
<p>java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be cast to scala.Tuple2</p>
</blockquote>

<p>Does work</p>

<pre><code>val umid_appData = umid_aggregated.map(row =&gt; {
            val id = row.getAs[String](&quot;id&quot;)
            val app_id = row.getAs[Seq[Row]](&quot;app_idBag&quot;).map(_.getString(0))
            (id, app_id)
            }).toDF(&quot;id&quot;, &quot;app_id&quot;)
</code></pre>

<p><a href="http://stackoverflow.com/a/37553262/4229125">Reference to stackoverflow post</a></p>

<h1 id="toc_39">Handling dirty data</h1>

<ol>
<li>Whenever the data source is not from yourself, be careful, handler them <strong>one by one</strong>. Otherwise it is gonna be painful to debug</li>
<li>Use <code>count</code> frequently, because this is the cheapest way to scan through all the data and will check all possible malformed data</li>
</ol>

<p>One example of data cleaning</p>

<pre><code class="language-scala">val acookie_mingyang_lsa_rdd = sparkSession.read.option(&quot;sep&quot;, &quot;\t&quot;).
        schema(Encoders.product[Lsa_mingyang].schema).
        csv(acookie_mingyang_lsa_output_path).as[Lsa_mingyang].
        filter(_.reducedFeat != null).rdd.
        map(row =&gt; Row.fromSeq(row.id +: row.reducedFeat.split(&quot;,&quot;).map(strFeat =&gt; 
                                            try {
                                                strFeat.toDouble
                                            } catch {
                                                case _ : Throwable  =&gt; 0.0
                                            }).toSeq))
val acookie_mingyang_lsa_rdd_removedMalformated = acookie_mingyang_lsa_rdd.filter(_.length == 51)
val acookie_mingyang_lsa_dupcolname = sparkSession.createDataFrame(acookie_mingyang_lsa_rdd_removedMalformated, featureStructType)
</code></pre>

<h1 id="toc_40">User Agent parse</h1>

<p>One of the option is using <code>us-parser</code> Scala implementation. The project is located <a href="https://github.com/ua-parser">here</a>   </p>

<p>Need import the jar. </p>

<pre><code class="language-scala">%spark.dep
z.load(&quot;org.uaparser:uap-scala_2.11:0.1.0&quot;)

import org.uaparser.scala.CachingParser
val parser = CachingParser.get(1000) // This get a caching parser with size of 1000
val test_ua = &quot;&quot;&quot;Mozilla/5.0 (Linux; U; Android 5.1; zh-CN; m1 note Build/LMY47D) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/40.0.2214.89 UCBrowser/11.2.5.884 Mobile Safari/537.36&quot;&quot;&quot;
val parsedUa = parser.parse(test_ua)

// parsedUa: org.uaparser.scala.Client = Client(UserAgent(UC Browser,Some(11),Some(2),Some(5)),OS(Android,Some(5),Some(1),None,None),Device(m1 note,Some(Generic_Android),Some(m1 note)))
</code></pre>

<h1 id="toc_41">Explode a column of Array to columns without using RDD</h1>

<p>We can directly select the element of the Array inside this column by index as:</p>

<pre><code class="language-scala">df.select($&quot;Col1&quot;, $&quot;Col2&quot;(0) as &quot;Col2&quot;, $&quot;Col2&quot;(1) as &quot;Col3&quot;, $&quot;Col2&quot;(2) as &quot;Col3&quot;)
</code></pre>

<p><a href="http://stackoverflow.com/a/37392793/4229125">StackOverflow post</a></p>

<h1 id="toc_42"><code>WrappedArray</code> in DataSet need use <code>Seq</code> to get</h1>

<p>The <code>sql.functions.collect_set</code> aggregate function collects distinct elements and saved them into a <code>WrappedArray</code>. This <code>WrappedArray</code> is from <code>mutable</code> package, so when we need get it out, we can not use <code>Array</code> but need use <code>Seq</code>.</p>

<p>The error message is like:</p>

<blockquote>
<p>org.apache.spark.SparkException: Job aborted due to stage failure: Task 87 in stage 63.0 failed 4 times, most recent failure: Lost task 87.3 in stage 63.0 (TID 17961, sc2-hmr175.drawbrid.ge): java.lang.ClassCastException: scala.collection.mutable.WrappedArray$ofRef cannot be cast to [Ljava.lang.Object;</p>
</blockquote>

<p>The set up is as follow:</p>

<pre><code class="language-scala">val umid_date_ipSet = umid_log.select(&quot;umid&quot;, &quot;ip&quot;, &quot;date&quot;).distinct.groupBy($&quot;umid&quot;, $&quot;date&quot;).agg(collect_set($&quot;ip&quot;).as(&quot;umid_ipSet&quot;))
val acookie_date_ipSet = acookie_log.select(&quot;acookie&quot;, &quot;ip&quot;, &quot;date&quot;).distinct.groupBy($&quot;acookie&quot;, $&quot;date&quot;).agg(collect_set($&quot;ip&quot;).as(&quot;acookie_ipSet&quot;))
val mapping_join = mapping.drop(&quot;taobaoid&quot;)
val mapping_umidIpSet_acookieIpSet = umid_date_ipSet.join(broadcast(mapping_join), usingColumn = &quot;umid&quot;).join(acookie_date_ipSet, usingColumns = Seq(&quot;acookie&quot;, &quot;date&quot;))
val tp_umid_acookie_dt = mapping_umidIpSet_acookieIpSet.rdd.map(row =&gt; {
        val umid_Ip4Set = row.getAs[Array[String]](&quot;umid_ipSet&quot;).toSet
        val acookie_Ip4Set = row.getAs[Array[String]](&quot;acookie_ipSet&quot;).toSet
        val co_ip4 = umid_Ip4Set.intersect(acookie_Ip4Set).size
        val umid_Ip3Set = row.getAs[Array[String]](&quot;umid_ipSet&quot;).map(ip =&gt; ip.substring(0, ip.lastIndexOf(&#39;.&#39;))).toSet
        val acookie_Ip3Set = row.getAs[Array[String]](&quot;acookie_ipSet&quot;).map(ip =&gt; ip.substring(0, ip.lastIndexOf(&#39;.&#39;))).toSet
        val co_ip3 = umid_Ip3Set.intersect(acookie_Ip3Set).size
        (row.getAs[String](&quot;umid&quot;), row.getAs[String](&quot;acookie&quot;), co_ip3, co_ip4)
    }).toDF(&quot;umid&quot;, &quot;acookie&quot;, &quot;co_ip3&quot;, &quot;co_ip4&quot;).groupBy($&quot;umid&quot;, $&quot;acookie&quot;).max(&quot;co_ip3&quot;, &quot;co_ip4&quot;).cache()
</code></pre>

<p>Change the <code>row.getAs[Array[String]]</code> to <code>row.getAs[Seq[String]]</code> code passes without issue.</p>

<p>For checking what the output type of <code>collect_set()</code> is, I did:</p>

<pre><code class="language-scala">val test = mapping_umidIpSet_acookieIpSet.take(1)
test: Array[org.apache.spark.sql.Row] = Array([WrappedArray(112.26.78.182),WrappedArray(36.63.47.140, 36.63.95.185)])
</code></pre>

<p><a href="http://stackoverflow.com/questions/33204205/read-array-of-string-from-spark">StackOverflow post</a></p>

<h1 id="toc_43">Change all column names once</h1>

<p>Using Dataset&#39;s method: <code>def toDF(colNames: String*): DataFrame</code>. The key point is how to use <code>*</code></p>

<p>Old dataset has schema [id: string, reducedFeat_1: Double, reducedFeat_2, ... reducedFeat_50: Double], what to add prefix &quot;umid_&quot; in all column names except for the first one</p>

<pre><code class="language-scala">val umid_lsa_renamed = umid_lsa_ori.toDF(&quot;id&quot; +: umid_lsa_ori.columns.tail.map(&quot;umid_&quot; + _): _*)
</code></pre>

<h1 id="toc_44">Read <code>.tar.gz</code> file</h1>

<p>Theoretically, spark is able to handle <code>.tar.gz</code> file naturally. It is in its <a href="https://github.com/apache/spark/blob/branch-2.2/core/src/main/scala/org/apache/spark/util/Utils.scala#L473">source code</a>: <code>if (fileName.endsWith(&quot;.tar.gz&quot;) || fileName.endsWith(&quot;.tgz&quot;))</code></p>

<p>However, <code>.tar.gz</code> is <a href="https://hvivani.com.ar/2014/11/23/mapreduce-compression-and-input-splits/">not a splittable compression format</a>, if we are using <code>sparkSession.read</code>, only a single executor will read the whole <code>.tar.gz</code> file. Therefore, if the size of the file is greater than <code>2GB</code>, we will have errors </p>

<p>The work around is use <code>sc.textFile(path, minPartitions)</code> to read it in then save it to HDFS again using <code>saveAsTextFile</code> and convert it to a splittable compression format.</p>

<p>Or try to use <code>pig</code> to read and convert it to a better compression format.</p>

<h1 id="toc_45">Spark join timeout errors</h1>

<p>See this error a lot when doing join, especially with large dataframe</p>

<blockquote>
<p>Caused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]</p>
</blockquote>

<p>There are two ways to solve the issue</p>

<ol>
<li><p><a href="http://stackoverflow.com/a/41126034/4229125">Post</a> set <code>spark.sql.broadcastTimeout</code> to <code>3600</code> instead of <code>300</code>. This proved to work on Spark 2.0.2 version. </p></li>
<li><p><a href="http://stackoverflow.com/a/36424994/4229125">Post</a> persist both dataframe to force a <code>ShuffleHashJoin</code></p></li>
</ol>

<p>Some more details showing in this type of timeout error</p>

<blockquote>
<p>org.apache.spark.SparkException: Exception thrown in awaitResult: <br/>
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:194)<br/>
    at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:120)<br/>
    at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:229)</p>
</blockquote>

<hr/>

<ul>
<li>
<a href="#toc_0">After join drop some columns cause Error</a>
</li>
<li>
<a href="#toc_1">read Lzo into Dataset</a>
</li>
<li>
<a href="#toc_2">read CSV into Dataset</a>
</li>
<li>
<a href="#toc_3">Don&#39;t use user defined functions in agg yet</a>
</li>
<li>
<a href="#toc_4">Use iterator and for comprehension to get cross product</a>
</li>
<li>
<a href="#toc_5"><font color='salmon'>? Strange java.io.NotSerializableException</font></a>
</li>
<li>
<a href="#toc_6">Using Spark-shell REPL</a>
</li>
<li>
<a href="#toc_7">Running spark job in oozie</a>
</li>
<li>
<a href="#toc_8">Another error for running spark job in oozie</a>
</li>
<li>
<a href="#toc_9">More dependency conflict for running oozie spark workflow</a>
</li>
<li>
<a href="#toc_10">spark config</a>
</li>
<li>
<a href="#toc_11">graphX connectedComponent</a>
</li>
<li>
<a href="#toc_12">Using of Bloom Filter</a>
</li>
<li>
<a href="#toc_13">Add a jar inside the spark REPL</a>
</li>
<li>
<a href="#toc_14">Get size of an object inside spark REPL</a>
</li>
<li>
<a href="#toc_15">Configuration that WON&#39;T work</a>
</li>
<li>
<a href="#toc_16">My experience for OOM debugging</a>
</li>
<li>
<a href="#toc_17">Date in Spark Dataset</a>
</li>
<li>
<a href="#toc_18">Encoder Issue in Dataset</a>
<ul>
<li>
<a href="#toc_19">Unable to find Encoder at Compiling time</a>
</li>
<li>
<a href="#toc_20">Unable to find Encoder at Run time</a>
</li>
</ul>
</li>
<li>
<a href="#toc_21">Split Dataset and save them separately simultaneously</a>
</li>
<li>
<a href="#toc_22">Hint the correct side of broadcast join</a>
</li>
<li>
<a href="#toc_23">Schema after join of two Dataset</a>
</li>
<li>
<a href="#toc_24">Handling A file sequentially</a>
</li>
<li>
<a href="#toc_25">Type Class Reflect of Spark Scala</a>
</li>
<li>
<a href="#toc_26">Spark shell Class initialization Error</a>
</li>
<li>
<a href="#toc_27">Spark date timezone converting</a>
</li>
<li>
<a href="#toc_28">Another Spark from date string to <code>DateType</code> converting</a>
</li>
<li>
<a href="#toc_29">Config spark jars in a folder</a>
</li>
<li>
<a href="#toc_30">Using python inside Spark (Scala) code</a>
</li>
<li>
<a href="#toc_31">DataType inside Dataset Row</a>
</li>
<li>
<a href="#toc_32">Read and Write in LZO format</a>
</li>
<li>
<a href="#toc_33">UDF for Dataset&#39;s column operation</a>
</li>
<li>
<a href="#toc_34">Kill a problematic executor</a>
</li>
<li>
<a href="#toc_35">cache and persist is a transform</a>
</li>
<li>
<a href="#toc_36">Another possible issue about cache</a>
</li>
<li>
<a href="#toc_37">parse URL get domain</a>
</li>
<li>
<a href="#toc_38">Get Seq of Tuple from DataFrame Row</a>
</li>
<li>
<a href="#toc_39">Handling dirty data</a>
</li>
<li>
<a href="#toc_40">User Agent parse</a>
</li>
<li>
<a href="#toc_41">Explode a column of Array to columns without using RDD</a>
</li>
<li>
<a href="#toc_42"><code>WrappedArray</code> in DataSet need use <code>Seq</code> to get</a>
</li>
<li>
<a href="#toc_43">Change all column names once</a>
</li>
<li>
<a href="#toc_44">Read <code>.tar.gz</code> file</a>
</li>
<li>
<a href="#toc_45">Spark join timeout errors</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Oozie]]></title>
    <link href="www.echo-ohce.com/14867660612004.html"/>
    <updated>2017-02-10T14:34:21-08:00</updated>
    <id>www.echo-ohce.com/14867660612004.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Run python script by oozie</a>
</li>
<li>
<a href="#toc_1">checking oozie job, kill job</a>
</li>
<li>
<a href="#toc_2">oozie EL expression/function</a>
</li>
<li>
<a href="#toc_3">conflict of two libraries in two jars</a>
</li>
<li>
<a href="#toc_4">global configuration do NOT work with shell action</a>
</li>
<li>
<a href="#toc_5">Oozie memory setting</a>
</li>
<li>
<a href="#toc_6">Put multiple libpath in oozie job</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Run python script by oozie</h1>

<p>Need use distributed cache, to send the python script to all workers. The important workflow lines are:</p>

<pre><code class="language-xml">&lt;exec&gt;script.py&lt;/exec&gt;
&lt;argument&gt;arg1&lt;/argument&gt;
&lt;argument&gt;arg2&lt;/argument&gt;
...
&lt;file&gt;scripts/script.py#script.py&lt;/file&gt;
</code></pre>

<p>The <code>#</code> means distribute the hdfs script file to the workers&#39; local file system using the sepcified name.</p>

<p>The python script writes to the workers&#39; local disk, no in the hdfs. The workaround is inside the python script, first write to a local temp file, then using subprocess to put the file onto hdfs, then delete the local temp file. This is the method I used for the <code>LP_tunning_17</code> project.</p>

<ul>
<li><a href="https://community.hortonworks.com/articles/12927/oozie-python-workflow-example-walkthrough.html">Hortonworks post for an oozie-python example</a></li>
<li><a href="https://community.hortonworks.com/questions/24182/where-is-the-output-of-an-oozie-workflow-is-stored.html">The output location of shell/python in oozie workflow</a></li>
<li><a href="http://blog.cloudera.com/blog/2013/03/how-to-use-oozie-shell-and-java-actions/">How to use shell and java action by cloudera. Pretty good</a></li>
</ul>

<h1 id="toc_1">checking oozie job, kill job</h1>

<p><a href="http://stackoverflow.com/a/24552758/4229125">A list of oozie commands</a></p>

<pre><code class="language-bash">oozie jobs | grep yu
oozie job -kill oozie-job-id
</code></pre>

<h1 id="toc_2">oozie EL expression/function</h1>

<p>Oozie has some specific EL expression/function, which can be find from their <a href="https://oozie.apache.org/docs/3.2.0-incubating/WorkflowFunctionalSpec.html#a4.2_Expression_Language_Functions">documents</a>. For the general EL expression/function, it is using <strong>JSP Expression Language</strong> syntax.. Check the <code>JSP 2.0 specification</code> to get the syntax.</p>

<ul>
<li><a href="http://stackoverflow.com/a/41902598/4229125">See my answer to a stackoverflow question</a></li>
<li><a href="http://blog.cloudera.com/blog/2013/09/how-to-write-an-el-function-in-apache-oozie/">Some example from Cloudera blog</a></li>
</ul>

<h1 id="toc_3">conflict of two libraries in two jars</h1>

<p>I encounter this issue when deploy an oozie workflow include both a giraph action and a spark action. Both action needs custom build jars <code>dpp-giraph-0.0.1-with-giraph-core.jar</code>, <code>spark2.jar</code>. I put these two jars in the <code>lib\</code> folder in oozie standard way. But the workflow failed with strange error message as some library&#39;s versionId is not consistent.</p>

<p>It actually is because in these two custom built jar, we used the same library but with conflicting versions. The solution for this is <strong>NOT</strong> put the jar in the <code>lib\</code>folder, but copy the specific jar in each action stage using the <code>&lt;file&gt;</code> option.</p>

<p>Example:</p>

<pre><code class="language-xml">&lt;action name=&quot;preprocess&quot;&gt;
   &lt;java&gt;
       &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
       &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
       &lt;prepare&gt;
           &lt;delete path=&quot;${nameNode}${lpOutputPath}/sCut_${sCut}/labelPairs&quot;/&gt;
           &lt;delete path=&quot;${nameNode}${lpOutputPath}/sCut_${sCut}/sample&quot;/&gt;
           &lt;delete path=&quot;${nameNode}${lpOutputPath}/sCut_${sCut}/labelIds&quot;/&gt;
       &lt;/prepare&gt;
       &lt;configuration&gt;
           &lt;property&gt;
               &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
               &lt;value&gt;${queueName}&lt;/value&gt;
           &lt;/property&gt;
       &lt;/configuration&gt;
       &lt;main-class&gt;org.apache.spark.deploy.SparkSubmit&lt;/main-class&gt;
       &lt;arg&gt;--conf&lt;/arg&gt;
       &lt;arg&gt;spark.driver.maxResultSize=20g&lt;/arg&gt;
       &lt;arg&gt;--conf&lt;/arg&gt;
       &lt;arg&gt;spark.driver.memory=20g&lt;/arg&gt;
       &lt;arg&gt;--conf&lt;/arg&gt;
       &lt;arg&gt;spark.yarn.executor.memoryOverhead=10240&lt;/arg&gt;
...
       &lt;arg&gt;--driver-cores&lt;/arg&gt;
       &lt;arg&gt;4&lt;/arg&gt;
       &lt;arg&gt;--num-executors&lt;/arg&gt;
       &lt;arg&gt;30&lt;/arg&gt;
       &lt;arg&gt;--executor-memory&lt;/arg&gt;
       &lt;arg&gt;5g&lt;/arg&gt;
       &lt;arg&gt;--executor-cores&lt;/arg&gt;
       &lt;arg&gt;10&lt;/arg&gt;
       &lt;!-- specify code source jar --&gt;
       &lt;arg&gt;spark2.jar&lt;/arg&gt;
       &lt;!--input path--&gt;
       &lt;arg&gt;LP_tunning_17/spark_base/labelPairs&lt;/arg&gt;
       &lt;arg&gt;LP_tunning_17/spark_base/sample&lt;/arg&gt;
       &lt;arg&gt;LP_tunning_17/spark_base/labelIds&lt;/arg&gt;
       &lt;arg&gt;${sCut}&lt;/arg&gt;
       &lt;!--output path--&gt;
       &lt;arg&gt;${lpOutputPath}/sCut_${sCut}/labelPairs&lt;/arg&gt;
       &lt;arg&gt;${lpOutputPath}/sCut_${sCut}/sample&lt;/arg&gt;
       &lt;arg&gt;${lpOutputPath}/sCut_${sCut}/labelIds&lt;/arg&gt;
       &lt;!-- dependencies must be distributed cached and added to the job using &#39;jar&#39; option above --&gt;
       &lt;!-- this jar contains the code --&gt;
       &lt;file&gt;spark2.jar#spark2.jar&lt;/file&gt;
   &lt;/java&gt;
   &lt;ok to=&quot;lp&quot;/&gt;
   &lt;error to=&quot;email-error&quot;/&gt;
&lt;/action&gt;

&lt;action name=&quot;lp&quot;&gt;
   &lt;java&gt;
       &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
       &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
       &lt;prepare&gt;
           &lt;delete path=&quot;${nameNode}${lpOutputPath}/sCut_${sCut}/${LPVersion}/lp-output&quot;/&gt;
       &lt;/prepare&gt;
       &lt;configuration&gt;
           &lt;property&gt;
               &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
               &lt;value&gt;${queueName}&lt;/value&gt;
           &lt;/property&gt;
           &lt;property&gt;
               &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;
               &lt;value&gt;${lpMemoryMB}&lt;/value&gt;
           &lt;/property&gt;
           &lt;property&gt;
               &lt;name&gt;mapred.task.timeout&lt;/name&gt;
               &lt;value&gt;1800000&lt;/value&gt;
           &lt;/property&gt;
           &lt;property&gt;
               &lt;name&gt;mapreduce.task.timeout&lt;/name&gt;
               &lt;value&gt;1800000&lt;/value&gt;
           &lt;/property&gt;
           &lt;property&gt;
               &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;
               &lt;value&gt;-Xmx${lpHeap}m&lt;/value&gt;
           &lt;/property&gt;
       &lt;/configuration&gt;
       &lt;main-class&gt;com.adsymp.dpp.giraph.lp.LPRunner&lt;/main-class&gt;
       &lt;arg&gt;com.adsymp.dpp.giraph.lp.WeightedLPComputation&lt;/arg&gt;
       &lt;arg&gt;${lpZKList}&lt;/arg&gt;
       &lt;arg&gt;-eif&lt;/arg&gt;
       &lt;arg&gt;com.adsymp.dpp.giraph.LongFloatTextEdgeInputFormat&lt;/arg&gt;
       &lt;arg&gt;-eip&lt;/arg&gt;
       &lt;arg&gt;${lpOutputPath}/sCut_${sCut}/sample&lt;/arg&gt;
       &lt;arg&gt;-vof&lt;/arg&gt;
       &lt;arg&gt;org.apache.giraph.io.formats.IdWithValueTextOutputFormat&lt;/arg&gt;
       &lt;arg&gt;-op&lt;/arg&gt;
       &lt;arg&gt;${lpOutputPath}/sCut_${sCut}/${LPVersion}/lp-output&lt;/arg&gt;
       &lt;arg&gt;-w&lt;/arg&gt;
       &lt;arg&gt;${lpWorkers}&lt;/arg&gt;
   ...
       &lt;arg&gt;-ca&lt;/arg&gt;
       &lt;arg&gt;mapreduce.map.memory.mb=${lpMemoryMB}&lt;/arg&gt;
       &lt;arg&gt;-ca&lt;/arg&gt;
       &lt;arg&gt;mapreduce.map.java.opts=-Xmx${lpHeap}m&lt;/arg&gt;
       &lt;file&gt;dpp-giraph-0.0.1-with-giraph-core.jar#dpp-giraph-0.0.1-with-giraph-core.jar&lt;/file&gt;
   &lt;/java&gt;
   &lt;ok to=&quot;postprocess&quot;/&gt;
   &lt;error to=&quot;email-error&quot;/&gt;
&lt;/action&gt;
</code></pre>

<p>And in the folder: <code>/home/yu/LP_tunning_17/sparkWorkflow</code></p>

<pre><code class="language-bash">[yu@sc2-hive1 sparkWorkflow]$ ls
dpp-giraph-0.0.1-with-giraph-core.jar  job_properties_template  nohup.out        scripts     workflow.xml
job.properties                         lib                      oozie-runner.py  spark2.jar
</code></pre>

<h1 id="toc_4">global configuration do NOT work with shell action</h1>

<p><a href="https://oozie.apache.org/docs/3.3.0/WorkflowFunctionalSpec.html#a19_Global_Configurations">As given here</a> the <code>Global configuration</code> supposed to avoid repetition. However, if it is a shell action, you still need repeat <code>&lt;job-tracker&gt;</code> and <code>&lt;name-node&gt;</code></p>

<p>Wrong example</p>

<pre><code class="language-bash">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;alibaba&quot;&gt;

    &lt;global&gt;
        &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
        &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
        &lt;configuration&gt;
            &lt;property&gt;
                &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                &lt;value&gt;${queueName}&lt;/value&gt;
            &lt;/property&gt;
        &lt;/configuration&gt;
    &lt;/global&gt;

    &lt;start to=&quot;base&quot;/&gt;

    &lt;action name=&quot;base&quot;&gt;
        &lt;java&gt;
            &lt;prepare&gt;
                &lt;delete path=&quot;${nameNode}/${pairOut}/base&quot; /&gt;
            &lt;/prepare&gt;
            &lt;main-class&gt;ge.drawbrid.dpp.graph.miaozhen.pairing.Base&lt;/main-class&gt;
            &lt;arg&gt;mapred&lt;/arg&gt;
            &lt;arg&gt;${freqOut}/output/*&lt;/arg&gt;
            &lt;arg&gt;${pairOut}/&lt;/arg&gt;
            &lt;arg&gt;${ipdtThreshold}&lt;/arg&gt; &lt;!-- ipdt is always higher than ipwk --&gt;
            &lt;arg&gt;${numReducer}&lt;/arg&gt;
            &lt;arg&gt;${partnerId}&lt;/arg&gt;
            &lt;arg&gt;${partnerKey}&lt;/arg&gt;
            &lt;file&gt;${ipFile}#ip3-type.txt&lt;/file&gt;
        &lt;/java&gt;
        &lt;ok to=&quot;end&quot; /&gt;
        &lt;error to=&quot;email-error&quot; /&gt;
    &lt;/action&gt;
    
    &lt;action name=&#39;train-cd&#39;&gt;
        &lt;shell xmlns=&quot;uri:oozie:shell-action:0.1&quot;&gt;
            &lt;prepare&gt;
                &lt;delete path=&quot;${nameNode}/${modelOut}/${version}/CD&quot;/&gt;
                &lt;mkdir path=&quot;${nameNode}/${modelOut}/${version}/CD&quot;/&gt;
            &lt;/prepare&gt;
            &lt;configuration&gt;
                &lt;property&gt;
                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${queueName}&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.memory.mb&lt;/name&gt;
                    &lt;value&gt;32768&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.java.opts&lt;/name&gt;
                    &lt;value&gt;-Xmx32256m&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;mapreduce.task.timeout&lt;/name&gt;
                    &lt;value&gt;36000000&lt;/value&gt;
                &lt;/property&gt;
            &lt;/configuration&gt;
            &lt;exec&gt;train.py&lt;/exec&gt;
            &lt;argument&gt;${rankOut}/${version}/features-all/CD&lt;/argument&gt;
            &lt;argument&gt;${modelOut}/${version}/CD&lt;/argument&gt;
            &lt;env-var&gt;HADOOP_USER_NAME=${wf:user()}&lt;/env-var&gt;
            &lt;file&gt;${trainScript}#train.py&lt;/file&gt;
            &lt;capture-output/&gt;&lt;/shell&gt;
        &lt;ok to=&quot;pred-cd&quot; /&gt;
        &lt;error to=&quot;email-error&quot; /&gt;
    &lt;/action&gt;
</code></pre>

<p>The second action will fail, and will see error message like this:</p>

<blockquote>
<p>Error: E0701 : E0701: XML schema error, cvc-complex-type.2.4.a: Invalid content was found starting with element &#39;prepare&#39;. One of &#39;{&quot;uri:oozie:shell-action:0.1&quot;:job-tracker}&#39; is expected.</p>
</blockquote>

<p>For the <code>shell</code> action, we still need repeat <code>&lt;job-tracker&gt;</code> and <code>&lt;name-node&gt;</code>. Correct code:</p>

<pre><code class="language-bash">
    &lt;action name=&#39;train-cd&#39;&gt;
        &lt;shell xmlns=&quot;uri:oozie:shell-action:0.1&quot;&gt;
            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
            &lt;prepare&gt;
                &lt;delete path=&quot;${nameNode}/${modelOut}/${version}/CD&quot;/&gt;
                &lt;mkdir path=&quot;${nameNode}/${modelOut}/${version}/CD&quot;/&gt;
            &lt;/prepare&gt;
            &lt;configuration&gt;
                &lt;property&gt;
                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${queueName}&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.memory.mb&lt;/name&gt;
                    &lt;value&gt;32768&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.java.opts&lt;/name&gt;
                    &lt;value&gt;-Xmx32256m&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;mapreduce.task.timeout&lt;/name&gt;
                    &lt;value&gt;36000000&lt;/value&gt;
                &lt;/property&gt;
            &lt;/configuration&gt;
            &lt;exec&gt;train.py&lt;/exec&gt;
            &lt;argument&gt;${rankOut}/${version}/features-all/CD&lt;/argument&gt;
            &lt;argument&gt;${modelOut}/${version}/CD&lt;/argument&gt;
            &lt;env-var&gt;HADOOP_USER_NAME=${wf:user()}&lt;/env-var&gt;
            &lt;file&gt;${trainScript}#train.py&lt;/file&gt;
            &lt;capture-output/&gt;&lt;/shell&gt;
        &lt;ok to=&quot;pred-cd&quot; /&gt;
        &lt;error to=&quot;email-error&quot; /&gt;
    &lt;/action&gt;
</code></pre>

<p><a href="http://stackoverflow.com/questions/35344565/oozie-global-configurations">Here is the stackoverflow post</a></p>

<h1 id="toc_5">Oozie memory setting</h1>

<p><a href="http://www.openkb.info/2016/07/memory-allocation-for-oozie-launcher-job.html">Here is a good post</a></p>

<ul>
<li>oozie.launcher.mapreduce.map.memory.mb</li>
<li>oozie.launcher.mapreduce.map.java.opts</li>
<li>oozie.launcher.yarn.app.mapreduce.am.resource.mb</li>
<li>oozie.launcher.mapreduce.map.java.opts</li>
</ul>

<p>The above 4 parameters set in workflow.xml for each Oozie job controls oozie memory.</p>

<ul>
<li>This is the Oozie case<br/></li>
</ul>

<p><img src="media/14867660612004/14903082502623.png" alt=""/></p>

<ul>
<li>This is the normal case<br/></li>
</ul>

<p><img src="media/14867660612004/14903082595315.png" alt=""/></p>

<h1 id="toc_6">Put multiple libpath in oozie job</h1>

<p>Use <code>,</code> as delimiter.</p>

<p>in job.properties, set:</p>

<pre><code class="language-bash">oozie.libpath=/path/to/jars,another/path/to/jars
oozie.use.system.libpath=true
</code></pre>

<p><a href="http://stackoverflow.com/a/33732743/4229125">Stackoverflow post</a><br/>
<a href="http://blog.cloudera.com/blog/2014/05/how-to-use-the-sharelib-in-apache-oozie-cdh-5/">Original post</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Run python script by oozie</a>
</li>
<li>
<a href="#toc_1">checking oozie job, kill job</a>
</li>
<li>
<a href="#toc_2">oozie EL expression/function</a>
</li>
<li>
<a href="#toc_3">conflict of two libraries in two jars</a>
</li>
<li>
<a href="#toc_4">global configuration do NOT work with shell action</a>
</li>
<li>
<a href="#toc_5">Oozie memory setting</a>
</li>
<li>
<a href="#toc_6">Put multiple libpath in oozie job</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[maven & gradle]]></title>
    <link href="www.echo-ohce.com/14742423250660.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250660.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">build jar include dependencies</a>
</li>
<li>
<a href="#toc_1">hadoop-common, hadoop-core, hadoop-client</a>
<ul>
<li>
<a href="#toc_2">Another example just found from giraph jar</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Maven build skip Test compilation</a>
</li>
<li>
<a href="#toc_4">DB gradle build jar without testing</a>
</li>
<li>
<a href="#toc_5">Gradle build jar with dependencies self contained</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">build jar include dependencies</h1>

<p>For the <code>pom.xml</code> use the <code>maven-assembly-plugin</code>. The lines with respect to this plugin is like follow:  </p>

<pre><code class="language-xml">&lt;build&gt;
   &lt;plugins&gt;
       &lt;plugin&gt;
           &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
           &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
           &lt;configuration&gt;
               &lt;descriptorRefs&gt;
                   &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
               &lt;/descriptorRefs&gt;
           &lt;/configuration&gt;
       &lt;/plugin&gt;
   &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p>The import line is <code>&lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</code>. This means to include and pack all dependencies into the project jar.</p>

<p>the maven command to run is:  </p>

<pre><code class="language-bash">mvn clean package assembly:single -DskipTests
</code></pre>

<h1 id="toc_1">hadoop-common, hadoop-core, hadoop-client</h1>

<p>In order to build a Hadoop map-reduce application you need only hadoop client dependency. (Use new API). Dependencies like hadoop-hdfs,hadoop-common,hadoop-clientapp,hadoop-yarn-api are resolved from this.</p>

<p>To help provide some additional details regarding the differences between Hadoop-common, Hadoop-core and Hadoop-client, from a high-level perspective:</p>

<ul>
<li>Hadoop-common refers to the commonly used utilities and libraries that support the Hadoop modules.</li>
<li>Hadoop-core is the same as Hadoop-common; It was renamed to Hadoop-common in July 2009.</li>
<li>Hadoop-client refers to the client libraries used to communicate with Hadoop&#39;s common components (HDFS, MapReduce, YARN) including but not limited to logging and codecs for example.</li>
</ul>

<p>Generally speaking, for developers who build apps that submit to YARN, run a MR job, or access files from HDFS use Hadoop-client libraries.</p>

<p><a href="http://stackoverflow.com/a/34229580/4229125">stackoverflow post</a></p>

<h2 id="toc_2">Another example just found from giraph jar</h2>

<p>Error message like:</p>

<blockquote>
<p>java.lang.NoSuchMethodError: org.apache.hadoop.ipc.RPC.getServer</p>
</blockquote>

<p>This is because the <code>pom.xml</code> had <code>hadoop-common</code> dependency, should change to <code>hadoop-client</code></p>

<h1 id="toc_3">Maven build skip Test compilation</h1>

<p><code>-DskipTests</code> just skips the test execution: the tests are still compiled.<br/>
<code>-Dmaven.test.skip</code> skips both compilation and execution of the tests.</p>

<p>I need this because I have .gitignore of all the test folders.</p>

<p>This is the command to be used:</p>

<pre><code class="language-bash">mvn clean package assembly:single -Dmaven.test.skip
</code></pre>

<p><a href="http://stackoverflow.com/a/2593834/4229125">stackoverflow post</a></p>

<h1 id="toc_4">DB gradle build jar without testing</h1>

<pre><code class="language-bash"># delete previous built jars
gradle clean

gradle build -x test
</code></pre>

<h1 id="toc_5">Gradle build jar with dependencies self contained</h1>

<p>For this simple purpose, do not need use <code>shadowJar</code> plugin</p>

<p>My example is trying to include <a href="https://mvnrepository.com/artifact/org.uaparser/uap-scala_2.11"><code>group: &#39;org.uaparser&#39;, name: &#39;uap-scala_2.11&#39;, version: &#39;0.1.0&#39;</code></a> in my <code>spark2.jar</code>, and of course all dependencies of the <code>uaparser</code> jar</p>

<pre><code class="language-gradle">...

configurations {
    // configuration that holds jars to include in the jar
    extraLibs
}
dependencies {
    ...
    extraLibs group: &#39;org.uaparser&#39;, name: &#39;uap-scala_2.11&#39;, version: &#39;0.1.0&#39;
    configurations.compile.extendsFrom(configurations.extraLibs)
    ...
}

...

jar {
    from {
        configurations.extraLibs.collect { it.isDirectory() ? it : zipTree(it) }
    }
}
</code></pre>

<p>Now what you put behind <code>extraLibs</code> will be both compiled and included into your jar as a dependency.</p>

<p>The command for building the jar is:   </p>

<pre><code class="language-bash">gradle jar   
</code></pre>

<p>This is because <code>jar</code> is an task, and the <code>fatJar</code> is created at <code>build/libs/</code></p>

<p><a href="https://discuss.gradle.org/t/how-to-include-dependencies-in-jar/19571/5">reference</a></p>

<p>My own example is at <code>repo/drawbridge/dpp/spark2/build.gradle</code> and the git branch <code>ali2</code></p>

<hr/>

<ul>
<li>
<a href="#toc_0">build jar include dependencies</a>
</li>
<li>
<a href="#toc_1">hadoop-common, hadoop-core, hadoop-client</a>
<ul>
<li>
<a href="#toc_2">Another example just found from giraph jar</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Maven build skip Test compilation</a>
</li>
<li>
<a href="#toc_4">DB gradle build jar without testing</a>
</li>
<li>
<a href="#toc_5">Gradle build jar with dependencies self contained</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zeppelin]]></title>
    <link href="www.echo-ohce.com/14896415461580.html"/>
    <updated>2017-03-15T22:19:06-07:00</updated>
    <id>www.echo-ohce.com/14896415461580.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Fix jackson issue</a>
</li>
<li>
<a href="#toc_1">Load two jars to run Zeppelin on dblab</a>
</li>
<li>
<a href="#toc_2">Not working spark configuration</a>
</li>
<li>
<a href="#toc_3">Working spark configuration</a>
</li>
<li>
<a href="#toc_4">Keep zeppelin running</a>
</li>
<li>
<a href="#toc_5">Set zeppelin webui</a>
</li>
<li>
<a href="#toc_6">Zeppelin for Pig using Lzo</a>
</li>
<li>
<a href="#toc_7">Make plots</a>
</li>
</ul>


<h1 id="toc_0">Fix jackson issue</h1>

<p>Downloaded the binary but failed to run. Check the spark log file find the error of:  </p>

<blockquote>
<p><code>com.fasterxml.jackson.databind.JsonMappingException: Jackson version is too old 2.5.3</code></p>
</blockquote>

<p>Seems it is a dependency issue. <a href="http://stackoverflow.com/questions/38819763/com-fasterxml-jackson-databind-jsonmappingexception-jackson-version-is-too-old">stackoverflow</a> <br/>
The solution is just go into the <code>lib</code> folder (for example: <code>/home/adsymp/lib/zeppelin-0.7.0-bin-all/lib</code>) and manually delete the <code>2.5.3</code> version and download <code>2.6.5</code> version jars for the following three jars:<br/>
1. <code>jackson-databind-2.6.5.jar</code><br/>
2. <code>jackson-core-2.6.5.jar</code><br/>
3. <code>jackson-annotations-2.6.5.jar</code></p>

<h1 id="toc_1">Load two jars to run Zeppelin on dblab</h1>

<p>Before starting any spark command, run this line:</p>

<pre><code class="language-scala">%spark.dep
z.load(&quot;/usr/lib/hadoop/lib/hadoop-lzo.jar&quot;)
z.load(&quot;/home/adsymp/lib/joda-time-2.2.jar&quot;)
</code></pre>

<p>It has to be two <code>load</code> lines in order to import both jars</p>

<h1 id="toc_2">Not working spark configuration</h1>

<ul>
<li><code>spark.driver.extraClassPath</code> \(\rightarrow\)   <code>/usr/lib/hadoop/lib/hadoop-lzo.jar:/home/adsymp/lib/joda-time-2.2.jar</code></li>
<li><code>spark.driver.extraLibraryPath</code> \(\rightarrow\) <code>/usr/lib/hadoop/lib/native/</code></li>
</ul>

<h1 id="toc_3">Working spark configuration</h1>

<ul>
<li><code>spark.yarn.jars</code> \(\rightarrow\) <code>hdfs://sc2prod/user/oozie/share/spark2/*.jar</code></li>
</ul>

<h1 id="toc_4">Keep zeppelin running</h1>

<p><code>zeppelin.interpreter.persistent</code> \(\rightarrow\) <code>true</code> <a href="http://apache-zeppelin-users-incubating-mailing-list.75479.x6.nabble.com/Passing-variables-between-interpreters-td334.html">reference</a></p>

<h1 id="toc_5">Set zeppelin webui</h1>

<p>It is in the config file of <code>conf/zeppelin-site.xml</code></p>

<pre><code class="language-xml">&lt;configuration&gt;

&lt;property&gt;
  &lt;name&gt;zeppelin.server.addr&lt;/name&gt;
  &lt;value&gt;10.35.80.17&lt;/value&gt;
  &lt;description&gt;Server address&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;zeppelin.server.port&lt;/name&gt;
  &lt;value&gt;8189&lt;/value&gt;
  &lt;description&gt;Server port.&lt;/description&gt;
&lt;/property&gt;
</code></pre>

<p>Note: <code>10.35.80.17</code> is graph-edge1.sc2&#39;s ip</p>

<h1 id="toc_6">Zeppelin for Pig using Lzo</h1>

<p>Has not find solutions yet.</p>

<h1 id="toc_7">Make plots</h1>

<ul>
<li>Using its own <code>%table</code></li>
</ul>

<pre><code class="language-scala">println(&quot;%table\nx\ty&quot;)
val histo = histo1.groupBy($&quot;value&quot;)
    .count().orderBy($&quot;count&quot;.desc)
    .collect().take(40).foreach(x=&gt;println(x.getAs[Long](&quot;value&quot;) + &quot;\t&quot; + x.getAs[Long](&quot;count&quot;)))
</code></pre>

<p><strong>Note</strong>   </p>

<ol>
<li>In this paragraph, it should only have one output, otherwise, it will mess up the table output, and therefore the plot</li>
<li>It must be <code>collect()</code> into local, can not be a distributed <code>RDD</code></li>
<li><p>Cannot collect too many data, under the hood, all content in the notebook is saved as <code>json</code> format, even all the data for the plotting. If there are too many data and the notebook can not run. Need go to the home page, and from the <code>notebook</code> section, using the option of cleaning all output before open the notebook again.</p></li>
</ol>

<ul>
<li>Better one is using <code>pyspark</code> and <code>matplotlib</code> </li>
</ul>

<pre><code class="language-python">%pyspark
def get_value_from_array_tuple2(scala_arr):
    return [(scala_tup2.container.container.productElement(0), scala_tup2.container.container.productElement(1)) for scala_tup2 in scala_arr]
</code></pre>

<pre><code class="language-scala">import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

val scoreAndLabels = testCD.map({case ScoredPairWithTrainAndLabel(umid, acookie, isTrain, label, score) =&gt; {
        val realLabel = 
            if (label == 1) 1
            else 0
        (score.toDouble, realLabel.toDouble)}}).rdd
val metrics = new BinaryClassificationMetrics(scoreAndLabels, 200)
val pr = metrics.pr().collect()
val roc = metrics.roc().collect()
val areaUnderRoc = metrics.areaUnderROC()
val areaUnderPR = metrics.areaUnderPR()
z.put(&quot;pr_arr&quot;, pr)
z.put(&quot;roc_arr&quot;, roc)
z.put(&quot;areaUnderRoc&quot;, areaUnderRoc)
z.put(&quot;areaUnderPR&quot;, areaUnderPR)
</code></pre>

<pre><code class="language-python">%pyspark

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

pr_arr = z.get(&quot;pr_arr&quot;)
roc_arr = z.get(&quot;roc_arr&quot;)
areaUnderRoc = z.get(&quot;areaUnderRoc&quot;)
areaUnderPR = z.get(&quot;areaUnderPR&quot;)
recall_precision = get_value_from_array_tuple2(pr_arr)
fpr_tpr = get_value_from_array_tuple2(roc_arr)
rp_df = pd.DataFrame(recall_precision, columns=[&#39;recall&#39;, &#39;precision&#39;])
fpr_tpr_df = pd.DataFrame(fpr_tpr, columns=[&#39;fpr&#39;, &#39;tpr&#39;])

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
ax1.plot(rp_df.recall, rp_df.precision)
ax1.set_title(&#39;PR Curve on test set&#39;)
ax1.set_xlabel(&#39;recall&#39;)
ax1.set_ylabel(&#39;precision&#39;)
ax1.set_aspect(&#39;equal&#39;)
ax1.text(x = 0.6, y = 0.8, s = &#39;auc={:.3f}&#39;.format(areaUnderPR))
ax2.plot(fpr_tpr_df.fpr, fpr_tpr_df.tpr)
ax2.set_title(&#39;ROC Curve on test set&#39;)
ax2.set_xlabel(&#39;FPR&#39;)
ax2.set_ylabel(&#39;TPR&#39;)
ax2.set_aspect(&#39;equal&#39;)
ax2.text(x = 0.6, y = 0.4, s = &#39;auc={:.3f}&#39;.format(areaUnderRoc))
fig.suptitle(&quot;curr. maiozhen pipeline with 102 feature&quot;)
</code></pre>

<p><strong>Note</strong>   </p>

<ol>
<li>Need convert scala object to python object.<br/>
In this example, <code>scala_arr</code> is <code>Array[(Double, Double)]</code> in scala, array of tuple2. Using this code return into python object of <code>List((float, float))</code> list of tuple</li>
<li>Using the <code>z</code> object to transfer the objects</li>
<li>Using <code>matplotlib</code> to plot.</li>
</ol>

<hr/>

<ul>
<li>
<a href="#toc_0">Fix jackson issue</a>
</li>
<li>
<a href="#toc_1">Load two jars to run Zeppelin on dblab</a>
</li>
<li>
<a href="#toc_2">Not working spark configuration</a>
</li>
<li>
<a href="#toc_3">Working spark configuration</a>
</li>
<li>
<a href="#toc_4">Keep zeppelin running</a>
</li>
<li>
<a href="#toc_5">Set zeppelin webui</a>
</li>
<li>
<a href="#toc_6">Zeppelin for Pig using Lzo</a>
</li>
<li>
<a href="#toc_7">Make plots</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[python]]></title>
    <link href="www.echo-ohce.com/14742423250719.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250719.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">shell expansion insdie python with  <code>subprocess.call</code></a>
</li>
<li>
<a href="#toc_1">plotly using LaTex</a>
</li>
<li>
<a href="#toc_2">plotly cufflinks pandas dataframe customize the plot</a>
</li>
<li>
<a href="#toc_3">timezone</a>
</li>
<li>
<a href="#toc_4">From String to timestamp to epoch unix time</a>
</li>
<li>
<a href="#toc_5">datetime(python), pd.Timestamp(pandas), np.datetime64(numpy)</a>
</li>
<li>
<a href="#toc_6">To make a linspace of pd.Timestamp</a>
</li>
<li>
<a href="#toc_7">plotly tick setting: string format, number of ticks and location of ticks.</a>
</li>
<li>
<a href="#toc_8">plotly datetime type tick setting</a>
</li>
<li>
<a href="#toc_9">plotly change the padding size between subplots</a>
</li>
<li>
<a href="#toc_10">read in gz / gzip file</a>
</li>
<li>
<a href="#toc_11">plotly bar chart, use whether value is negative or positive to determine its color</a>
</li>
<li>
<a href="#toc_12">plotly subplot shared axis setting</a>
</li>
<li>
<a href="#toc_13">plotly remove missed dates from time series plot</a>
</li>
<li>
<a href="#toc_14">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</a>
</li>
<li>
<a href="#toc_15">Use <code>:</code> or <code>slice(None)</code></a>
</li>
<li>
<a href="#toc_16">String for pandas datetime period and frequency</a>
</li>
<li>
<a href="#toc_17">Python Library for Probabilistic Graphical Models</a>
</li>
<li>
<a href="#toc_18">Python blocking and non-blocking subprocess calls</a>
</li>
<li>
<a href="#toc_19">nohup python that includes a nohup shell script</a>
</li>
<li>
<a href="#toc_20">ipython jupyter notebook pandas showing options</a>
</li>
<li>
<a href="#toc_21">matplotlib style and colors</a>
</li>
<li>
<a href="#toc_22">matplotlib backend choices</a>
</li>
<li>
<a href="#toc_23">matplotlib with mpld3 <code>twinx</code> two side axes issue</a>
</li>
<li>
<a href="#toc_24">Conda install python package CondaHTTPError Error</a>
</li>
<li>
<a href="#toc_25">utf8 unicode in string formatter &amp; file read/write</a>
</li>
<li>
<a href="#toc_26">Use a relative path in a python module</a>
</li>
<li>
<a href="#toc_27">Mysterious Error pandas groupby error</a>
</li>
<li>
<a href="#toc_28">matplotlib add axes (subplot) on a made figure and sharex</a>
</li>
<li>
<a href="#toc_29">python parse webpage static content</a>
</li>
<li>
<a href="#toc_30">python get and parse webpage dynamic content</a>
</li>
<li>
<a href="#toc_31">python with statement and context manager type</a>
</li>
<li>
<a href="#toc_32">python selenium selector find element by multiple class names</a>
</li>
<li>
<a href="#toc_33">Speed up selenium and Time out</a>
</li>
<li>
<a href="#toc_34">An example of using regex parse table</a>
</li>
<li>
<a href="#toc_35">start and shutdown the selenium-server</a>
</li>
<li>
<a href="#toc_36">phantomJS options</a>
</li>
<li>
<a href="#toc_37">Pycharm unittest not finding the module path</a>
</li>
<li>
<a href="#toc_38">Conda rename environment</a>
</li>
<li>
<a href="#toc_39">Deploy an egg package to distributed nodes on hadoop</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">shell expansion insdie python with  <code>subprocess.call</code></h1>

<p>For python older than 2.7 (like the default on on DB cluster, but we can change the shebang to specify 2.7), we can only use <code>subprocess.call()</code>.<br/><br/>
when using shell expansion with this command, need have <code>shell=True</code>. Better way to have shell expansion inside python is to use module <code>glob</code><br/><br/>
But the following works, because it is not the shell expansion but the <code>imagemagick</code> does the expansion.  </p>

<pre><code class="language-python">subprocess.call([&#39;convert&#39;, &#39;-delay&#39;, &#39;10&#39;, &#39;-loop&#39;, &#39;10&#39;, &#39;temp_1*.png&#39;, &#39;cluster_size_anim.gif&#39;])
</code></pre>

<h1 id="toc_1">plotly using LaTex</h1>

<p>It is not possible to use LaTex in offline mode. (don&#39;t waste time to figure out a way to do that anymore, maybe wait for newer update)<br/><br/>
It works when using the online mode.</p>

<h1 id="toc_2">plotly cufflinks pandas dataframe customize the plot</h1>

<ol>
<li>using the <code>asFigure=True</code> option to get the handle of the figure</li>
<li>using <code>update</code> method to update the layout python dictionary</li>
<li>using <code>po.iplot()</code> to plot the figure</li>
</ol>

<p>Example:  </p>

<pre><code class="language-python">figure = data.iplot(kind=&#39;scatter&#39;, fill=True, asFigure=True)
figure.layout.update(xaxis=dict(nticks=len(data.asfreq(&#39;M&#39;).index)))
po.iplot(figure)
</code></pre>

<h1 id="toc_3">timezone</h1>

<p>A classical scenario is get timestamp in UTC, want to convert to local timezone. Panda can do it for time series</p>

<pre><code class="language-python">ts2 = ts1.
        tz_localize(&#39;UTC&#39;).
        tz_convert(&#39;US/Pacific&#39;)
</code></pre>

<p>the <code>tz_localize(&#39;UTC&#39;)</code> make it timezone aware, the <code>tz_convert(&#39;US/Pacific&#39;)</code> convert it to local timezone.</p>

<h1 id="toc_4">From String to timestamp to epoch unix time</h1>

<p>Use <code>pd.Timestamp()</code> to convert string into pandas timestamp, then use <code>value</code> to epoch unix time in nanosecond</p>

<pre><code class="language-python">pd.Timestamp(&#39;2002-01-01&#39;).value/1e6
# this convert to epoch microseconds
</code></pre>

<h1 id="toc_5">datetime(python), pd.Timestamp(pandas), np.datetime64(numpy)</h1>

<p>At least between python and pandas is easy:</p>

<pre><code class="language-python">from dateutil import parser as dateparser
from datetime import datetime

dt = dateparser.parse(&#39;2015-01-01&#39;) # python datetime
dt2 = datetime(2015, 1, 1)          # Also python datetime
ts = pd.to_datetime(dt)             # pandas Timestamp
dt = ts.to_pydatetime()             # back to python datetime
</code></pre>

<p>A very good graph to show the converting relationship:</p>

<p><img src="media/14742423250719/14757939209171.jpg" alt=""/></p>

<h1 id="toc_6">To make a linspace of pd.Timestamp</h1>

<p>The <code>pd.date_range(start=None, end=None, periods=None, freq=&#39;D&#39;)</code> method will not work. This method is for different purpose.</p>

<pre><code class="language-python">rng = pd.date_range(&#39;1/1/2011&#39;, periods=72, freq=&#39;H&#39;)

# the same as pd.date_range(start, end, freq=&#39;D&#39;)
rng = pd.date_range(start, end)
</code></pre>

<p>To make the linespace of pd.Timestamp:<br/>
1. convert pd.Timestamp to long type<br/>
2. use np.linspace to make the array of long<br/>
3. convert back to pd.Timestamp array<br/>
4. Special note: If the original timestamp has timezone info, the step 3 need use different method to restore the timezone info</p>

<pre><code class="language-python"># the index of memory_df dataframe is of type pd.Timestamp
# the ticks_array is what we want linspace array of pd.Timestamp
# this is for cases without timezone information
ticks_array = pd.to_datetime(np.linspace(start=memory_df.index[0].value, 
                                         stop=memory_df.index[-1].value, 
                                         num=n_ticks, 
                                         endpoint=True))
                                         
# if need keep timezone info, to_datetime() method is not good anymore.
# for example the index of memory_df dataframe has timezone info
memory_df.index = memory_df.index.tz_localize(&#39;UTC&#39;).tz_convert(&#39;US/Pacific&#39;)

ticks_array = [pd.Timestamp(timetick, tz=memory_df.index.tz) for timetick in 
               (np.linspace(start=memory_df.index[0].value,
                            stop=memory_df.index[-1].value,
                            num=n_ticks, endpoint=True))]
</code></pre>

<h1 id="toc_7">plotly tick setting: string format, number of ticks and location of ticks.</h1>

<ul>
<li>The easiest way to set <code>nticks</code> and <code>tickformat</code>. Like the following example:</li>
</ul>

<pre><code class="language-python">plot_layout = go.Layout(xaxis=dict(nticks=5,
                                   tickformat=&#39;.2f&#39;))
</code></pre>

<ul>
<li>The hard way is to set <code>tickmode=&#39;array&#39;, ticktext=[...], tickvals=[...]</code>. </li>
</ul>

<h1 id="toc_8">plotly datetime type tick setting</h1>

<p>directly use <code>datetime</code> datatype and the plotly tick setting options have subtle bugs, especially when dealing with timezone awared data.</p>

<p>The workaround:<br/>
1. convert the index to string using <code>strftime</code> (these string need to have high enough resolution like <code>&#39;%H:%M:%S:%f&#39;</code>, otherwise, the plot will loose resolution because of having the same xaxis values)<br/>
2. generating <code>ticktext</code> and <code>tickvals</code>,specifing any format you like for <code>ticktext</code>.  </p>

<p>Look the below example:  </p>

<pre><code class="language-python">def visualization(job_id, basepath=&#39;.&#39;):
    memory_file = os.sep.join([basepath, &#39;memory_usage_&#39;+job_id+&#39;.csv&#39;])
    
    memory_df = pd.read_csv(memory_file)
    timestamp = pd.to_datetime(memory_df.timestamp, infer_datetime_format=True)
    memory_df = memory_df.set_index(timestamp).drop(&#39;timestamp&#39;, axis=1)
    memory_df=memory_df/float(1024*1024*1024*1024)
    memory_df.index = memory_df.index.tz_localize(&#39;UTC&#39;).tz_convert(&#39;US/Pacific&#39;)
    
    # change the index to string for plotting purpose
    memory_plot_df = memory_df.copy()
    memory_plot_df.index = pd.Series(data=[ts.strftime(&#39;%H:%M:%S:%f&#39;) for ts in memory_plot_df.index],name=&#39;time&#39;)
    mem_fig = memory_plot_df.iplot(asFigure=True)
    
    # set the xaxis ticks
    n_ticks = 10
    picked_tick_index = ((np.linspace(start=0, stop=len(memory_plot_df.index)-1, num=n_ticks, endpoint=True))
                        .astype(int))
    ticktext=[&#39;:&#39;.join(ts.split(&#39;:&#39;)[:2]) for ts in memory_plot_df.index[picked_tick_index]] # only take hour and mins
    tickvals=memory_plot_df.index[picked_tick_index]
    mem_fig.layout.update(title=&#39;Memory usage for &#39;+job_id,
                          yaxis=dict(title=&#39;TB&#39;),
                          xaxis=dict(title=&#39;time&#39;,
                                     tickmode=&#39;array&#39;,
                                     ticktext=ticktext,
                                     tickvals=tickvals))

    po.iplot(mem_fig, show_link=False)
</code></pre>

<h1 id="toc_9">plotly change the padding size between subplots</h1>

<p>Use <code>specs</code> and <code>vertical_spacing</code> or <code>horizontal_spacing</code> inside <code>tls.make_subplots()</code></p>

<p>Example:  </p>

<pre><code class="language-python">import plotly.tools as tls

figure = tls.make_subplots(rows=2, cols=1, shared_xaxes=True,
                           specs=[[{}], [{}]],
                           vertical_spacing=0.01)
</code></pre>

<h1 id="toc_10">read in gz / gzip file</h1>

<pre><code class="language-python">import gzip

with gzip.open(file_path, &#39;rb&#39;) as f:
    for line in f:
</code></pre>

<h1 id="toc_11">plotly bar chart, use whether value is negative or positive to determine its color</h1>

<pre><code class="language-python"># here sh_MACD.ch is a time series, with values can be positive or negative.
# want to generate the bar chart that is red if its value is positive, or green if negative

colors=(sh_MACD.ch&gt;0).map({True:&#39;red&#39;, False:&#39;green&#39;}).values
trace_macd = go.Bar(x=sh_MACD.index, y=sh_MACD.ch, name = &#39;MACD&#39;,
                    marker=dict(color=colors))
</code></pre>

<p>Key points:<br/>
* use map to convert logical array to the color literals<br/>
* use values to get np.ndarray not pd.Series (because plotly need np.ndarray)<br/>
* use <code>marker</code> filed</p>

<h1 id="toc_12">plotly subplot shared axis setting</h1>

<p>For example, if the subplot is 2 row 1 col, with shared x axis, the axis&#39; names are: [(1,1) x1,y1 ], [(2,1) x1,y2 ]. To set the axis properties in the <code>layout</code> dictionary, using <code>xaxis1</code> instead of <code>xaxis</code>.</p>

<p>The following example is from <code>01-Project/01-Study/PersonalProject/EDA.ipynb</code></p>

<pre><code class="language-python">overlay_plot[&#39;layout&#39;].update(xaxis1=dict(type=&#39;category&#39;))
</code></pre>

<h1 id="toc_13">plotly remove missed dates from time series plot</h1>

<p>change the xaxis&#39; type to <code>category</code>. (refer to the example above)</p>

<h1 id="toc_14">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</h1>

<pre><code>In [65]: df
Out[65]: 
                     A         B         C
first second                              
bar   one     0.895717  0.410835 -1.413681
      two     0.805244  0.813850  1.607920
baz   one    -1.206412  0.132003  1.024180
      two     2.565646 -0.827317  0.569605
foo   one     1.431256 -0.076467  0.875906
      two     1.340309 -1.187678 -2.211372
qux   one    -1.170299  1.130127  0.974466
      two    -0.226169 -1.436737 -2.006747
</code></pre>

<p>select all <code>two</code> data. Two methods:</p>

<ul>
<li>MultiIndex is tuple, using <code>loc</code> and tuple to select, and use <code>slice(None)</code> inside tuple to represent <code>:</code> that we normally use (because <code>:</code> is not recognized inside tuple). <font color='red'> The dataframe MultiIndex must be sorted first.</font></li>
</ul>

<pre><code class="language-python">df=df.sort_index()
selection = df.loc[(slice(None), &#39;two&#39;), :]
</code></pre>

<ul>
<li>Use <code>xs</code> method and use <code>level</code> argument to specify the level. (note that <code>xs</code> by default works on rows, if want to use it on columns, use the <code>axis=1</code> argument)<br/></li>
</ul>

<pre><code class="language-python">selection = df.xs(&#39;two&#39;, level=&#39;second&#39;)
</code></pre>

<h1 id="toc_15">Use <code>:</code> or <code>slice(None)</code></h1>

<p>They mean the same thing, normally in <code>[ ]</code>, we just use <code>:</code>, like <code>[5, :]</code><br/>
However, inside tuple, <code>:</code> is not recognized, we have to use <code>slice(None)</code> instead.</p>

<h1 id="toc_16">String for pandas datetime period and frequency</h1>

<p>Useful official documents</p>

<ul>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases">frequency strings</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offsets">Anchored offset frequency strings</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offset-semantics">Rolling forward or backward to snap on the anchors</a></li>
</ul>

<h1 id="toc_17">Python Library for Probabilistic Graphical Models</h1>

<p><a href="https://github.com/pgmpy/pgmpy">github link</a></p>

<h1 id="toc_18">Python blocking and non-blocking subprocess calls</h1>

<p><code>Popen</code> is nonblocking; <code>call</code> and <code>check_call</code> are blocking.<br/><br/>
<a href="http://stackoverflow.com/a/21936682/4229125">A good stackoverflow post explaining this</a></p>

<h1 id="toc_19">nohup python that includes a nohup shell script</h1>

<ul>
<li>when using nohup with python, the output will be buffered, in order to see the output in real time using <code>python -u</code> option to turn off the output buffering</li>
</ul>

<pre><code class="language-bash"># calling the nohup python script like this:
nohup python -u subprocess_test.py 1&gt;py_nohup.out 2&gt;&amp;1 &amp;
</code></pre>

<ul>
<li>Do not use <code>nohup</code> inside the python script for calling a shell script. Instead using the <code>Popen</code> with <code>shell=True</code> option, and redirects the stdout to a file.</li>
</ul>

<p>The following example is located at <code>Dropbox/01-Project/01-Study/playground/py_subprocess_call</code></p>

<p>The python script <code>subprocess_test.py</code>:</p>

<pre><code class="language-python">#!/usr/local/bin/python

from subprocess import Popen;
from time import sleep, strftime, localtime;

Popen(&#39;./keep_logging.sh 1&gt;shell_logging_out 2&gt;&amp;1&#39;, shell=True)

for i in range(20):
    print &#39;python logging {:d} time&#39;.format(i)
    print strftime(&quot;%a %b %d %H:%M:%S PDT %Y&quot;, localtime())
    sleep(3)
</code></pre>

<p>The called shell script inside python is <code>keep_logging.sh</code>: </p>

<pre><code class="language-bash">for ((c=1;c&lt;=20 ;c++ ))
do
        echo &quot;shell logging $c times&quot;
        date
        sleep 3
done
</code></pre>

<p>The command of running the python script (in turn running the shell script) is:</p>

<pre><code class="language-bash">nohup python -u subprocess_test.py 1&gt;py_nohup.out 2&gt;&amp;1 &amp;
</code></pre>

<p>Using <code>tail -f</code> for both logging file <code>shell_logging_out</code> and <code>py_nohup.out</code> shows they are <strong>non-blocking</strong> and the output are in real time.</p>

<h1 id="toc_20">ipython jupyter notebook pandas showing options</h1>

<p><a href="http://pandas.pydata.org/pandas-docs/stable/options.html">official documents</a></p>

<ul>
<li>set jupyter notebook show float in dataframe with 2 digits precision (globally)<br/>
<code>pd.options.display.float_format = &#39;{:.2f}&#39;.format</code> <a href="http://stackoverflow.com/a/31671975/4229125">stackoverflow reference</a></li>
<li>get/set number of dataframe rows to show in notebook<br/>
<code>pd.get_option(&quot;display.max_rows&quot;)</code><br/>
<code>pd.set_option(&quot;display.max_rows&quot;,999)</code><br/></li>
</ul>

<h1 id="toc_21">matplotlib style and colors</h1>

<ul>
<li><a href="https://tonysyu.github.io/raw_content/matplotlib-style-gallery/gallery.html">Here</a> is a good post for showing how all the matplotlib styles look like.</li>
<li>The recommended way of choosing color is through <code>seaborn</code>. <a href="http://seaborn.pydata.org/tutorial/color_palettes.html">The office document</a> which is very useful.</li>
<li>To temporarily change the matplotlib&#39;s default <a href="http://seaborn.pydata.org/tutorial/color_palettes.html#palette-contexts">color palette</a> and <a href="http://matplotlib.org/users/style_sheets.html#temporary-styling">style</a>, use <code>with</code>:<br/></li>
</ul>

<pre><code class="language-python">
with sns.color_palette(&quot;PuBuGn_d&quot;):
    # generate some plots with this color_palette
    
with plt.style.context((&#39;ggplot&#39;)):
    # generate some plots using the ggplot style  
    
</code></pre>

<h1 id="toc_22">matplotlib backend choices</h1>

<p>If a separate figure window needed (not the jupyter embedded window) use <code>Qt5Agg</code> as:</p>

<pre><code class="language-python">from matplotlib import pyplot as plt

plt.switch_backend(&#39;Qt5Agg&#39;)
</code></pre>

<p>If want to see the figure embedded inside the jupyter notebook, use magic line will be good enough:</p>

<pre><code class="language-python">%matplotlib notebook
</code></pre>

<h1 id="toc_23">matplotlib with mpld3 <code>twinx</code> two side axes issue</h1>

<p><code>mpld3</code> can correctly convert matplotlib twinx figs, plotly cannot</p>

<pre><code class="language-python">import matplotlib as mpl
from matplotlib import pyplot as plt
import mpld3
import plotly.tools as tls
import plotly.offline as po

%matplotlib notebook

fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot([1,3,2], &#39;b-&#39;)
ax2.plot([16, 4, 1], &#39;r:&#39;)
ax.set_ylabel(&#39;y1&#39;)
ax2.set_ylabel(&#39;y2&#39;)
ax2.patch.set_alpha(0.0) # This is the main trick for showing two axes
ax.set_xlabel(&#39;x&#39;)
fig.show()

###
mpld3.display(fig)

###
po.init_notebook_mode()
plotly_fig = tls.mpl_to_plotly(fig)
po.iplot(plotly_fig, show_link=False)
</code></pre>

<h1 id="toc_24">Conda install python package CondaHTTPError Error</h1>

<p>Mysterious error like:</p>

<blockquote>
<p>CondaHTTPError: HTTP 404 None<br/>
for url &lt;None&gt;<br/>
The remote server could not find the channel you requested.</p>
</blockquote>

<p>The reason is that some package installed previous changed the <code>~/.condarc</code> setting and added a channel, which <a href="http://conda.pydata.org/docs/config.html?highlight=channel%20url#channel-locations-channels">&quot;Listing channel locations in the .condarc file will override conda defaults&quot;</a></p>

<p>the modified <code>~/.condarc</code> that cause problem:  </p>

<pre><code class="language-bash">channels:
  - https://repo.continuum.io/pkgs/
  - defaults
</code></pre>

<p>just need delete the extra line, make it look like</p>

<pre><code class="language-bash">channels:
  - defaults
</code></pre>

<p>Problem solved.</p>

<h1 id="toc_25">utf8 unicode in string formatter &amp; file read/write</h1>

<ul>
<li>Use the <code>io</code> package for utf8 file read/write</li>
</ul>

<pre><code class="language-python">import io
with io.open(filename,&#39;r&#39;,encoding=&#39;utf8&#39;) as f:
    text = f.read()
# process Unicode text
with io.open(filename,&#39;w&#39;,encoding=&#39;utf8&#39;) as f:
    f.write(text)
</code></pre>

<p><a href="http://stackoverflow.com/a/19591815/4229125">Stackoverflow post</a></p>

<ul>
<li>Use <code>u</code> for both strings in string formatter</li>
</ul>

<pre><code class="language-python">s = u&#39;\u2265&#39;

# UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\u2265&#39; in position 0...
print &#39;{0}&#39;.format(s)

# Correct
print u&#39;{0}&#39;.format(s)
</code></pre>

<ul>
<li>To make the code itself utf8 (such as including chinese characters inside the code), add this line at the beginning of the python file</li>
</ul>

<pre><code class="language-python"># -*- coding: utf-8 -*-
</code></pre>

<h1 id="toc_26">Use a relative path in a python module</h1>

<p>Use case: need refer to my personal key which is located in another folder <code>resources</code></p>

<p>Store the absolute path to the module directory at the very beginning of the module:</p>

<pre><code class="language-python">package_directory = os.path.dirname(os.path.abspath(__file__))
with open(os.path.join(package_directory, &#39;../resources/tk.key&#39;), &#39;r&#39;) as f:
    token = f.readline()
    ts.set_token(token)
</code></pre>

<p><a href="http://stackoverflow.com/a/4187345/4229125">stackoverflow post</a></p>

<p>The same trick can be used for import a relative path module. <a href="http://stackoverflow.com/a/7506029/4229125">This neat trick is from stackoverflow here</a></p>

<p>At the head of the python file:</p>

<pre><code class="language-python"># -*- coding: utf-8 -*-
import sys, os
import pymongo

try:
    import fromTDX
    from indicators import commonIndicator
    from Const import Const
except Exception:
    sys.path.append(os.path.join(os.path.dirname(__file__), &#39;..&#39;))
    import fromTDX
    from indicators import commonIndicator
    from Const import Const
</code></pre>

<h1 id="toc_27">Mysterious Error pandas groupby error</h1>

<p>For some version of Pandas actually this error will not occur. (pandas v.18.1 will have error, but v.19 has no problem)  </p>

<ul>
<li>Symptom:</li>
</ul>

<blockquote>
<p>lib/python2.7/site-packages/pandas/core/groupby.pyc in reset_identity(values)<br/>
AttributeError: &#39;NoneType&#39; object has no attribute &#39;_get_axis&#39;</p>
</blockquote>

<ul>
<li><p>Cause:<br/><br/>
The apply function return <code>None</code>. When groupby then apply this func, this will cause issue.<br/><br/>
<a href="http://stackoverflow.com/a/20225276/4229125">A similar discussion on stackoverflow</a></p></li>
<li><p>The fix:</p></li>
</ul>

<p>The apply func should return empty <code>pd.DataFrame()</code> or empty <code>pd.Series()</code>.   </p>

<p>Wrong:</p>

<pre><code class="language-python">def get_buy_in_grp(self, grp):
   cond_1_m0 = (grp[&#39;MA_2_&#39; + self.base_period] &gt; grp[&#39;MA_5_&#39; + self.base_period])
   cond_1 = cond_1_m0 &amp; (~cond_1_m0.shift().astype(bool))
   cond_2 = (grp[&#39;MA_5_&#39; + self.base_period] &gt;= self.MA5_p * grp[&#39;MA_5_&#39; + self.base_period].shift())
   cond = cond_1 &amp; cond_2

   if ~(cond.any()):
       return
   return grp[cond]
</code></pre>

<p>Correct:</p>

<pre><code class="language-python">def get_buy_in_grp(self, grp):
   cond_1_m0 = (grp[&#39;MA_2_&#39; + self.base_period] &gt; grp[&#39;MA_5_&#39; + self.base_period])
   cond_1 = cond_1_m0 &amp; (~cond_1_m0.shift().astype(bool))
   cond_2 = (grp[&#39;MA_5_&#39; + self.base_period] &gt;= self.MA5_p * grp[&#39;MA_5_&#39; + self.base_period].shift())
   cond = cond_1 &amp; cond_2

   if ~(cond.any()):
       return pd.DataFrame() # return None may cause issues for the groupby().apply() action
   return grp[cond]     
</code></pre>

<h1 id="toc_28">matplotlib add axes (subplot) on a made figure and sharex</h1>

<p>Use the <code>fig.add_axes()</code> method <a href="http://matplotlib.org/api/figure_api.html#matplotlib.figure.Figure.add_axes">official document</a></p>

<p>To sharex, just <code>sharex</code> parameter in the method. Note that if <code>adjustable</code> parameter is set, in order to use <code>sharex</code>, <code>adjustable = ‘datalim’</code></p>

<pre><code class="language-python">
# the fig and ax_close etc are returned by another method

ax_close_pos = ax_close.get_position()
ax_macd_pos = [ax_close_pos.x0, ax_close_pos.y0, ax_close_pos.width, ax_close_pos.height/5.0]
ax_macd = fig.add_axes(ax_macd_pos, frameon=True, sharex=ax_close)
macd_df = (test_record.df[[col_name for col_name in test_record.df.columns
                           if col_name[-1]==&#39;m&#39;]]
           .drop_duplicates()
           .set_index(&#39;Date_m&#39;))
ax_macd.bar(macd_df[macd_df.MACD_m&gt;0].index.values, 
            macd_df[macd_df.MACD_m&gt;0].MACD_m.values,
            width=15, color=&#39;salmon&#39;)
ax_macd.bar(macd_df[macd_df.MACD_m&lt;0].index.values, 
            macd_df[macd_df.MACD_m&lt;0].MACD_m.values,
            width=15, color=&#39;steelblue&#39;)
ax_macd.grid(True)
mpld3.display()
</code></pre>

<h1 id="toc_29">python parse webpage static content</h1>

<p>use <code>requests</code> to get the response from webpage, use <code>BeautifulSoup</code> to parse the returned content</p>

<p>For some webpage, like <code>http://xueqiu.com/</code>, it will check the header of the connect and will reject the connection if no appropriate header specified. The following is the one that I used to access <code>http://xueqiu.com/</code></p>

<pre><code class="language-python">import requests
import pandas as pd
import io

hdr = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11&#39;,
       &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
       &#39;Accept-Charset&#39;: &#39;ISO-8859-1,utf-8;q=0.7,*;q=0.3&#39;,
       &#39;Accept-Encoding&#39;: &#39;none&#39;,
       &#39;Accept-Language&#39;: &#39;en-US,en;q=0.8&#39;,
       &#39;Connection&#39;: &#39;keep-alive&#39;}
url = &#39;http://xueqiu.com/S/SH600033/historical.csv&#39;

with requests.Session() as s:
   s.headers.update(hdr)
   r = s.get(url)
   if r.status_code == 200: # 200 is of type int NOT type string
       csv = r.content.decode(&#39;utf8&#39;)  # the decode is a must
       # Must use io.StringIO() to wrap it into a string buffer
       df = pd.read_csv(io.StringIO(csv), parse_dates=[&#39;date&#39;], infer_datetime_format=True)
       df = df.drop(&#39;symbol&#39;, axis=1)
       df.columns = [name.capitalize() for name in df.columns]
       df = df.set_index(&#39;Date&#39;)
       return df
   else:
       return None
</code></pre>

<h1 id="toc_30">python get and parse webpage dynamic content</h1>

<p><font color='red'><strong>SELENIUM</strong></font></p>

<p><a href="http://www.seleniumhq.org/docs/03_webdriver.jsp#introducing-the-selenium-webdriver-api-by-example">The best official documentation with python java examples</a></p>

<hr/>

<p>Some webpage&#39;s table is dynamically generated by js, like <code>https://xueqiu.com/S/SH600652/FHPS</code>.<br/>
Like in this <a href="http://stackoverflow.com/a/25062761/4229125">stackoverflow post</a><br/>
(Note: this answer has an issue, instead of <code>table.get_attribute(&#39;innerHTML&#39;)</code>, it should be <code>table.get_attribute(&#39;outerHTML&#39;)</code>. Because pandas need the <code>&lt;table&gt; &lt;/table&gt;</code> to understand that it is a table)</p>

<p>So the best solution is to use <code>selenium</code> <a href="http://selenium-python.readthedocs.io/api.html#webdriver-api">WebDriver</a><br/>
Using conda install like:   </p>

<pre><code class="language-bash">conda install -c clinicalgraphics selenium
</code></pre>

<p><a href="https://docs.google.com/a/adsymptotic.com/presentation/d/18lrHBU7RhyFBJnSmjlB2gBmzMeP2K4Vo53C1OjRm4D4/edit?usp=sharing">Here is a ppt about using selenium in java</a></p>

<ul>
<li>To use chrome WebDriver (which is very easy to use) need install ChromeDriver. <a href="http://stackoverflow.com/a/38087347/4229125">Two different ways to get the ChromeDriver</a>,  I used this one:</li>
</ul>

<pre><code class="language-bash">brew install chromedriver
</code></pre>

<ul>
<li>To use firefox WebDriver, need install two things: Firefox and geckodriver. I used this:</li>
</ul>

<pre><code class="language-bash">brew install Caskroom/cask/firefo
brew install geckodriver
</code></pre>

<p>My complete example:</p>

<pre><code class="language-python">import pandas as pd
from pandas.io import html as pd_html

from contextlib import contextmanager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium import webdriver



@contextmanager
def wait_for_page_load(driver, timeout=10):
    # I used chrome to get this xpath
    check_ele = driver.find_element_by_xpath(&#39;//*[@id=&quot;center&quot;]/div[2]/div[2]/div[2]/div&#39;)
    check_text = check_ele.text
    if check_text == u&#39;暂无数据&#39;:
        old_td = None
    else:
        old_td = driver.find_element_by_xpath(&#39;//*[@id=&quot;center&quot;]/div[2]/div[2]/div[2]/div[1]/table/tbody/tr[1]/td[2]&#39;)
    yield 
    # yield nothing, just want keep the current state of old_td 
    # when exit the with wait_for_page_load block, the next line
    # make sure that the old_td will be changed, or &#39;no next page&#39; element showing up
    if old_td:
        WebDriverWait(driver, timeout=timeout).until(EC.staleness_of(old_td))
    

@contextmanager
def open_driver():
    driver = webdriver.Chrome() # ChromeDriver need be installed by Homebrew
    driver.implicitly_wait(10) # This line will cause it to wait upto 10 seconds if an element is not there
    yield driver
    driver.quit()


with open_driver() as driver:
    url_test = &#39;https://xueqiu.com/S/SZ000547/FHPS&#39;
    driver.get(url_test)

    dividen_dfs = []
    while True:
        with wait_for_page_load(driver):
            check_ele = driver.find_element_by_xpath(&#39;//*[@id=&quot;center&quot;]/div[2]/div[2]/div[2]/div&#39;)
            check_text = check_ele.text
            if check_text == u&#39;暂无数据&#39;:
                break
            table = driver.find_element_by_xpath(&#39;//table[@class=&quot;dataTable table table-bordered&quot;]&#39;)
            table_html = table.get_attribute(&#39;outerHTML&#39;)
            df = pd_html.read_html(table_html, na_values = &#39;-&#39;)

            # below is just some normal dataframe munging
            processed_df = df[0].T
            processed_df.columns = processed_df.iloc[0,:]
            processed_df = processed_df.drop(0, axis=0)
            processed_df = processed_df.reset_index(drop=True)
            dividen_dfs.append(processed_df)

            # get the link and click on the link
            link = driver.find_element_by_link_text(u&#39;下一页&#39;)
            link.click()
        
dividen_df = pd.concat(dividen_dfs).drop_duplicates().reset_index(drop=True)
dividen_df

</code></pre>

<p><font color='red'><strong>Some special notes:</strong></font></p>

<ul>
<li>Why we need wait?  Because if there is asynchronous content like JS modifying the DOM after the initial page load is complete, the driver.get() may miss the target, since the target is loaded later <a href="http://stackoverflow.com/a/16740004/4229125">Stackoverflow post</a>. </li>
<li>Implicitly_wait(): An implicit wait is to tell WebDriver to poll the DOM for a certain amount of time when trying to find an element or elements if they are not immediately available. The default setting is 0. Once set, the implicit wait is set for the life of the WebDriver object instance.</li>
<li>Remember to close the <code>driver</code>, otherwise it will have resource leaking</li>
<li>Chrome has a very cool feature, under <code>inspect</code> mode, use right click you can directly copy <font color='red'><strong>XPath, Selection, Element etc.</strong></font><br/></li>
</ul>

<p><font color='red'><strong>Updated notes</strong></font></p>

<ul>
<li>I heavily referred to <a href="http://www.obeythetestinggoat.com/how-to-get-selenium-to-wait-for-page-load-after-a-click.html">this post</a> for the much improved version of the code. The central idea is by checking <code>EC.staleness_of(old_td)</code> to make sure that the new content has been generated in the <code>wait_for_page_load</code> method.</li>
<li>In the python binding of <code>selenium</code>, to get the text of the page element, actually need go through the property as <code>.text</code> instead of using method <code>.getText()</code> which is different than the Java binding</li>
<li>It doesn&#39;t show in the code anymore, but the way of using <code>Locator</code> is construct a tuple, which is kind of confusing. Look at the below code snippet and pay attention to the double bracket</li>
</ul>

<pre><code class="language-python">from selenium.webdriver.common.by import By

EC.presence_of_element_located((By.ID, &quot;myDynamicElement&quot;))
</code></pre>

<h1 id="toc_31">python with statement and context manager type</h1>

<p><a href="https://docs.python.org/2.7/library/stdtypes.html#context-manager-types">Official document for Context Manager</a><br/>
<a href="https://docs.python.org/2.7/reference/datamodel.html#with-statement-context-managers">Official document for with statement Context Manager</a></p>

<p>In the above code snippet, I used the <code>@contextmanager</code> decorator to build the <code>with open_driver() as driver:</code> so that when the block of code exit, the driver will be automatically closed and preventing resource leaking.</p>

<p><a href="http://preshing.com/20110920/the-python-with-statement-by-example/">A good blog post about this</a></p>

<h1 id="toc_32">python selenium selector find element by multiple class names</h1>

<p>Imagine we have few elements as the followings:</p>

<ol>
<li><code>&lt;div class=&quot;value test&quot; /&gt;</code></li>
<li><code>&lt;div class=&quot;value test &quot; /&gt;</code></li>
<li><code>&lt;div class=&quot;first value test last&quot; /&gt;</code></li>
<li><code>&lt;div class=&quot;test value&quot; /&gt;</code></li>
</ol>

<p>How XPath matches</p>

<ul>
<li><p>Match only 1 (exact match)</p>

<pre><code>driver.findElement(By.xpath(&quot;//div[@class=&#39;value test&#39;]&quot;));
</code></pre></li>
<li><p>Match 1, 2 and 3 (match class contains <code>value test</code>, class order matters)</p>

<pre><code>driver.findElement(By.xpath(&quot;//div[contains(@class, &#39;value test&#39;)]&quot;));
</code></pre></li>
<li><p>Match 1, 2, 3 and 4 (as long as elements have class <code>value</code> and <code>test</code>)</p>

<pre><code>driver.findElement(By.xpath(&quot;//div[contains(@class, &#39;value&#39;) and contains(@class, &#39;test&#39;)]&quot;));
</code></pre></li>
</ul>

<p>Also, in cases like this, Css Selector is always in favor of XPath (fast, concise, native).</p>

<ul>
<li><p>Match 1</p>

<pre><code>driver.findElement(By.cssSelector(&quot;div[class=&#39;value test&#39;]&quot;));
</code></pre></li>
<li><p>Match 1, 2 and 3</p>

<pre><code>driver.findElement(By.cssSelector(&quot;div[class*=&#39;value test&#39;]&quot;));
</code></pre></li>
<li><p>Match 1, 2, 3 and 4</p>

<pre><code>driver.findElement(By.cssSelector(&quot;div.value.test&quot;));
</code></pre></li>
</ul>

<p><a href="http://stackoverflow.com/a/21714006/4229125">stackoverflow post</a></p>

<h1 id="toc_33">Speed up selenium and Time out</h1>

<p>I only tried with Firefox. According to a lot of post, the solution should be using the <code>FirefoxProfile</code> by <code>set_preference(&#39;webdriver.load.strategy&#39;, &#39;unstable&#39;)</code> as shown in <a href="http://stackoverflow.com/a/11455428/4229125">this stackoverflow post</a>. However, as I tested with firefox version 50.x, it does not work. According to <a href="https://github.com/SeleniumHQ/selenium/issues/2873">this issue</a>, it only work for firefox <font color='red'><strong>less than v. 46</strong></font></p>

<p>This still not completely solved. I did find a way to make it a little bit better.</p>

<p>Check the available firefox configuration parameters: open firefox and input <code>about:config</code> in the url address field.</p>

<p>Also there is <a href="http://kb.mozillazine.org/About:config_entries">the official documents for the full list and explainations</a></p>

<p>The useful parameters that I found are:  </p>

<ul>
<li><code>permissions.default.image, 2</code>, which make the loading of the page much faster</li>
<li><strong><code>network.http.connection-timeout, 5</code></strong>, this one is very cool. It force the connect to break after the specified amount of time, so you don&#39;t have to wait till the full page to load if you can already find what you need. <a href="http://stackoverflow.com/a/1343963/4229125">Reference to this post, then checked with firefox&#39;s about:config</a></li>
</ul>

<p>To figure out what is the current firefox profile that the WebDriver is using:</p>

<pre><code class="language-python">firefox_p = webdriver.FirefoxProfile()
firefox_p.default_preferences # this is a dictionary
</code></pre>

<p>To do the improvement setting for firefox is:</p>

<pre><code class="language-python">@contextmanager
def open_fast_driver():
    firefox_p = webdriver.FirefoxProfile()
    firefox_p.set_preference(&#39;permissions.default.image&#39;, 2)
    firefox_p.set_preference(&#39;network.http.connection-timeout&#39;, 1)
    driver = webdriver.Firefox(firefox_profile=firefox_p)
    yield driver
    driver.quit()
</code></pre>

<p><font color='red'><strong>UPDATE</strong></font><br/>
I decided to downgrade the firefox to v.46 and see how it goes.</p>

<pre><code class="language-bash">brew cask uninstall firefox # uninstall the previous version
brew tap goldcaddy77/firefox
brew cask install firefox-46
</code></pre>

<p>After install this version of Firefox, need add the following lines when use firefox WebDirver:</p>

<pre><code class="language-python">from selenium.webdriver.firefox.firefox_binary import FirefoxBinary

firefox_b = FirefoxBinary(r&#39;/Applications/Firefox-46.app/Contents/MacOS/firefox&#39;)
driver = webdriver.Firefox(firefox_binary=firefox_b)
</code></pre>

<p>But, hey! You guess what!, it still not working even with Firefox V.46. And it breaks the little trick that I found to disable images too.</p>

<p>I uninstall the V46, and come back to what I did earlier.</p>

<pre><code class="language-bash">cd ~
brew cask uninstall firefox-4
brew untap goldcaddy77/firefo
brew install Caskroom/cask/firefox
</code></pre>

<p><font color='red'><strong>UPDATE, UPDATE</strong></font></p>

<p>Tried to use Firefox add-on <code>KillSpinners</code> to stop loading the page after some time-out. Not successful. But learned how to use add-on/extension of selenium</p>

<p>For firefox extension on mac, it is located here: <code>~/Library/Application Support/Firefox/Profiles/[profile name]/extensions</code>. For my Mac it is here: </p>

<pre><code class="language-python">firefox_KillSpinners_ext_path = &#39;/Users/yugan/Library/Application Support/Firefox/Profiles/1qvac6rq.default/extensions/killspinners@byo.co.il.xpi&#39;
</code></pre>

<p>The python code to use this extension is:</p>

<pre><code class="language-python">firefox_p = webdriver.FirefoxProfile()
firefox_p.add_extension(firefox_KillSpinners_ext_path)
firefox_p.set_preference(&#39;permissions.default.image&#39;, 2)
firefox_p.set_preference(&#39;extensions.killspinners.timeout&#39;, 10)
firefox_p.set_preference(&#39;extensions.killspinners.disablenotify&#39;, True)
</code></pre>

<p>The two parameters are found in the firefox <code>about:config</code> page.<br/>
The reason it failed is because this add-on blocked the thread.</p>

<p><font color='red'><strong>UPDATE, UPDATE, UPDATE</strong></font></p>

<p>The best update so far.</p>

<p>The solution is changing the WebDriver. No matter whether it is <code>ChromeDriver</code> or <code>FirefoxDriver</code>, they are too heavy and slow. We should use <font color='red'><strong>non-GUI drivers</strong></font> to speed it up. selenium has excellent support of using the following two non-GUI WebDriver. Check <a href="http://www.seleniumhq.org/docs/03_webdriver.jsp#driver-specifics-and-tradeoffs">the official documents for Selenium-WebDriver support</a></p>

<ul>
<li>HtmlUnit - <a href="http://www.seleniumhq.org/docs/03_webdriver.jsp#javascript-in-the-htmlunit-driver">need be careful for the javascript handling</a> </li>
</ul>

<p>In order to use this WebDriver, need install and run Selenium Server:</p>

<pre><code class="language-bash">brew install selenium-server-standalone
selenium-server -port 4444
</code></pre>

<p>Then in python code, specify Using Selenium with <font color='red'><strong>remote</strong></font> WebDriver (<a href="http://selenium-python.readthedocs.io/getting-started.html#using-selenium-with-remote-webdriver">See the remote webdriver here</a>), which should connect to: <code>http://127.0.0.1:4444/wd/hub</code>. Specify the desired_capabilities to <code>HTMLUNITWITHJS</code>. The <a href="http://selenium-python.readthedocs.io/api.html#desired-capabilities">desired_capabilities</a> includes all broswers like (chrome, firefox, safari, phantomJS... etc)</p>

<blockquote>
<p>Note: Always use <code>.copy()</code> on the DesiredCapabilities object to avoid the side effects of altering the Global class instance </p>
</blockquote>

<p>Also, checking the cmd line output can help find some useful parameters for the WebDrivers</p>

<ul>
<li><font color='red'>phatomJS</font>- This so far is the best one. It respond the javascript pretty well. I find some configuration parameters which can further speed up the response </li>
</ul>

<p>install:</p>

<pre><code class="language-bash">brew install phantomjs
</code></pre>

<p>python code:</p>

<pre><code class="language-python">@contextmanager
def open_phantomJS_driver():
    
    capabilities = webdriver.DesiredCapabilities.PHANTOMJS.copy()
    capabilities[&#39;phantomjs.page.settings.loadImages&#39;] = False
    capabilities[&#39;phantomjs.page.settings.webSecurityEnabled&#39;] = False
    capabilities[&#39;phantomjs.page.settings.javascriptCanOpenWindows&#39;] = False
    capabilities[&#39;phantomjs.page.settings.javascriptCanCloseWindows&#39;] = False
    capabilities[&#39;phantomjs.page.settings.userAgent&#39;] = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/538.1 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/538.1&#39;

    driver = webdriver.Remote(&quot;http://localhost:4444/wd/hub&quot;, capabilities)
    yield driver
    driver.quit()
</code></pre>

<p><a href="https://realpython.com/blog/python/headless-selenium-testing-with-python-and-phantomjs/">A blog for phantomJS</a><br/>
<strong><a href="https://github.com/SeleniumHQ/selenium/wiki/DesiredCapabilities">For the Browser Specific DesiredCapabilities, there is an official documents (not exhaustive though)</a></strong><br/>
<strong><a href="https://sites.google.com/a/chromium.org/chromedriver/capabilities">ChromeDriver available Capabilities and options</a></strong><br/>
<a href="http://peter.sh/experiments/chromium-command-line-switches/#timeout">List of chrome command line switches, but seems outdated. At least the <code>--timeout</code> switch does not work anymore</a></p>

<p><font color='red'><strong>I tested some config for the HtmlUnit and ChromeOptions, using the above links, but most of them are either not working or not relevant to what I am doing. Just keep these as a record if I need use them in the future</strong></font></p>

<h1 id="toc_34">An example of using regex parse table</h1>

<p>This table is not well formed for pandas to read. Combining use both the <code>split()</code> and regex make it much easier</p>

<pre><code class="language-python">import re
from dateutil import parser as date_parser
import pandas as pd
from contextlib import contextmanager
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

@contextmanager
def open_fast_driver():
    firefox_p = webdriver.FirefoxProfile()
    firefox_p.set_preference(&#39;permissions.default.image&#39;, 2)
    firefox_p.set_preference(&#39;network.http.connection-timeout&#39;, 1)
    driver = webdriver.Firefox(firefox_profile=firefox_p)
    yield driver
    driver.quit()
    

zg_re = re.compile(ur&#39;转增(\d+)股&#39;)
sg_re = re.compile(ur&#39;送(\d+)股&#39;)
fh_re = re.compile(ur&#39;派息(.+)元&#39;)
date_re = re.compile(ur&#39;(\d{4}-\d{2}-\d{2})&#39;)


base_url = &#39;http://q.stock.sohu.com/cn/{}/fhsp.shtml&#39;
code = &#39;600589&#39;
url = base_url.format(code)
table_xpath = &#39;/html/body/div[4]/div[2]/div[2]/div[2]/div/div[2]/table&#39;
table = None
with open_fast_driver() as driver:
    driver.get(url)
    d_table = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH,table_xpath)))
    table = d_table.text

if table:
    line_iter = iter(table.split(&#39;\n&#39;))
    records = []
    while True:
        record = dict()
        date_m = None
        try:
            line = line_iter.next()
            tokens = line.strip().split()
            if len(tokens) == 1:
                continue
            if tokens[0] != u&#39;除权除息日&#39;:
                continue
            if len(tokens) == 2:
                next_line = line_iter.next()
                date_m = date_re.search(next_line)
            if not date_m:
                date_m = date_re.search(line)
            zg_m = zg_re.search(line)
            sg_m = sg_re.search(line)
            fh_m = fh_re.search(line)
            record[&#39;Date&#39;] = date_parser.parse(date_m.group(1))
            if zg_m:
                record[&#39;Zg&#39;] = float(zg_m.group(1))/10.
            if sg_m:
                record[&#39;Sg&#39;] = float(sg_m.group(1))/10.
            if fh_m:
                record[&#39;Fh&#39;] = float(fh_m.group(1))/10.
            records.append(record)
        except StopIteration:
            break
    fhps_df = pd.DataFrame(records)
    for field in [&#39;Fh&#39;, &#39;Zg&#39;, &#39;Sg&#39;]:
        if field not in fhps_df:
            fhps_df.loc[:, field] = 0.0
    fhps_df.loc[:, [&#39;Fh&#39;, &#39;Zg&#39;, &#39;Sg&#39;]] = fhps_df.loc[:, [&#39;Fh&#39;, &#39;Zg&#39;, &#39;Sg&#39;]].fillna(0.0)
    fhps_df.loc[:, &#39;Ps&#39;] = fhps_df[&#39;Zg&#39;]+fhps_df[&#39;Sg&#39;]
    fhps_df = fhps_df.sort_values(&#39;Date&#39;)

fhps_df[[&#39;Date&#39;, &#39;Fh&#39;, &#39;Ps&#39;]]
</code></pre>

<h1 id="toc_35">start and shutdown the selenium-server</h1>

<pre><code class="language-python">import subprocess
import time
from contextlib import contextmanager


@contextmanager
def open_selenium_server():
    null_io = open(&#39;/dev/null&#39;)
    server_proc = subprocess.Popen([&#39;selenium-server&#39;, &#39;-port&#39;, &#39;4444&#39;], stdout=null_io, stderr=null_io)
    time.sleep(0.5)
    yield
    server_proc.terminate()
</code></pre>

<p><a href="http://stackoverflow.com/a/6883164/4229125">referred to this post</a></p>

<h1 id="toc_36">phantomJS options</h1>

<p>In the readme page of <code>ghostdriver</code> project, it gave a section about how to set the webdriver properties of phantomJS. <a href="https://github.com/detro/ghostdriver#what-extra-webdriver-capabilities-ghostdriver-offers">It is a good reference</a></p>

<p>specificially, besides the setting that I figured out randomly before, the following setting seems pretty good:</p>

<pre><code class="language-python"># new settings
capabilities[&#39;phantomjs.page.settings.resourceTimeout&#39;] = 5000 # 5000 milliseconds
</code></pre>

<p><a href="http://phantomjs.org/api/webpage/property/settings.html">official list of all properties / settings</a></p>

<h1 id="toc_37">Pycharm unittest not finding the module path</h1>

<p>This is something wrong with Pycharm itself</p>

<p>Error message looks like:</p>

<blockquote>
<p>RuntimeWarning: Parent module &#39;tests&#39; not found while handling absolute import<br/>
  import unittest</p>
</blockquote>

<p>Fix:<br/>
download the <code>utrunner.py</code> file from <a href="https://youtrack.jetbrains.com/_persistent/utrunner.py?file=74-332199&amp;c=true">here</a><br/>
copy it to <code>/Applications/PyCharm CE.app/Contents/helpers/pycharm/</code> and overwrite the original <code>utrunner.py</code> file</p>

<p><a href="http://stackoverflow.com/a/38724508/4229125">stackoverflow post</a></p>

<h1 id="toc_38">Conda rename environment</h1>

<p>Not specific command to do that. Just copy and remove like follow</p>

<pre><code class="language-bash">conda create --name personalTrader --clone bktrader
conda remove --name bktrader --all
</code></pre>

<p>also need manually remove the list entries inside this file: <code>~/.conda/environments.txt</code>, as referred <a href="https://github.com/conda/conda/issues/2704">here</a></p>

<p><a href="https://github.com/conda/conda/issues/3097">reference post</a></p>

<h1 id="toc_39">Deploy an egg package to distributed nodes on hadoop</h1>

<p>In <code>oozie</code> workflow, use <code>&lt;file&gt;</code> action node to &#39;ship&#39; the egg to distributed nodes</p>

<pre><code class="language-xml">    &lt;action name=&#39;train-cd&#39;&gt;
        &lt;shell xmlns=&quot;uri:oozie:shell-action:0.1&quot;&gt;
            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
            &lt;prepare&gt;
                &lt;delete path=&quot;${nameNode}${modelOut}/CD&quot;/&gt;
                &lt;mkdir path=&quot;${nameNode}${modelOut}/CD&quot;/&gt;
            &lt;/prepare&gt;
            &lt;configuration&gt;
                &lt;property&gt;
                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${queueName}&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.memory.mb&lt;/name&gt;
                    &lt;value&gt;204800&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.java.opts&lt;/name&gt;
                    &lt;value&gt;-Xmx174080m&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;mapreduce.task.timeout&lt;/name&gt;
                    &lt;value&gt;36000000&lt;/value&gt;
                &lt;/property&gt;
            &lt;/configuration&gt;
            &lt;exec&gt;train.py&lt;/exec&gt;
            &lt;argument&gt;${rankOut}/features-all/CD&lt;/argument&gt;
            &lt;argument&gt;${modelOut}/CD&lt;/argument&gt;
            &lt;env-var&gt;HADOOP_USER_NAME=${wf:user()}&lt;/env-var&gt;
            &lt;file&gt;${trainScript}#train.py&lt;/file&gt;
            &lt;file&gt;${lightgbmegg}#lightgbm-0.1-py2.7.egg&lt;/file&gt;
            &lt;capture-output/&gt;&lt;/shell&gt;
        &lt;ok to=&quot;pred-cd&quot; /&gt;
        &lt;error to=&quot;email-error&quot; /&gt;
    &lt;/action&gt;
</code></pre>

<p>In python need do two things:</p>

<ol>
<li>unzip the egg file</li>
<li>append the <code>cwd</code> to the system path</li>
</ol>

<pre><code class="language-python">#!/usr/local/bin/python2.7

import sys
import os
import pandas as pd
import cPickle as pickle
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.grid_search import GridSearchCV
from sklearn.utils import shuffle
from sklearn.metrics import auc, precision_recall_curve
from subprocess import call
call(&#39;unzip lightgbm-0.1-py2.7.egg&#39;, shell=True)

cwd = os.getcwd()
sys.path.append(cwd)

from lightgbm import LGBMClassifier
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">shell expansion insdie python with  <code>subprocess.call</code></a>
</li>
<li>
<a href="#toc_1">plotly using LaTex</a>
</li>
<li>
<a href="#toc_2">plotly cufflinks pandas dataframe customize the plot</a>
</li>
<li>
<a href="#toc_3">timezone</a>
</li>
<li>
<a href="#toc_4">From String to timestamp to epoch unix time</a>
</li>
<li>
<a href="#toc_5">datetime(python), pd.Timestamp(pandas), np.datetime64(numpy)</a>
</li>
<li>
<a href="#toc_6">To make a linspace of pd.Timestamp</a>
</li>
<li>
<a href="#toc_7">plotly tick setting: string format, number of ticks and location of ticks.</a>
</li>
<li>
<a href="#toc_8">plotly datetime type tick setting</a>
</li>
<li>
<a href="#toc_9">plotly change the padding size between subplots</a>
</li>
<li>
<a href="#toc_10">read in gz / gzip file</a>
</li>
<li>
<a href="#toc_11">plotly bar chart, use whether value is negative or positive to determine its color</a>
</li>
<li>
<a href="#toc_12">plotly subplot shared axis setting</a>
</li>
<li>
<a href="#toc_13">plotly remove missed dates from time series plot</a>
</li>
<li>
<a href="#toc_14">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</a>
</li>
<li>
<a href="#toc_15">Use <code>:</code> or <code>slice(None)</code></a>
</li>
<li>
<a href="#toc_16">String for pandas datetime period and frequency</a>
</li>
<li>
<a href="#toc_17">Python Library for Probabilistic Graphical Models</a>
</li>
<li>
<a href="#toc_18">Python blocking and non-blocking subprocess calls</a>
</li>
<li>
<a href="#toc_19">nohup python that includes a nohup shell script</a>
</li>
<li>
<a href="#toc_20">ipython jupyter notebook pandas showing options</a>
</li>
<li>
<a href="#toc_21">matplotlib style and colors</a>
</li>
<li>
<a href="#toc_22">matplotlib backend choices</a>
</li>
<li>
<a href="#toc_23">matplotlib with mpld3 <code>twinx</code> two side axes issue</a>
</li>
<li>
<a href="#toc_24">Conda install python package CondaHTTPError Error</a>
</li>
<li>
<a href="#toc_25">utf8 unicode in string formatter &amp; file read/write</a>
</li>
<li>
<a href="#toc_26">Use a relative path in a python module</a>
</li>
<li>
<a href="#toc_27">Mysterious Error pandas groupby error</a>
</li>
<li>
<a href="#toc_28">matplotlib add axes (subplot) on a made figure and sharex</a>
</li>
<li>
<a href="#toc_29">python parse webpage static content</a>
</li>
<li>
<a href="#toc_30">python get and parse webpage dynamic content</a>
</li>
<li>
<a href="#toc_31">python with statement and context manager type</a>
</li>
<li>
<a href="#toc_32">python selenium selector find element by multiple class names</a>
</li>
<li>
<a href="#toc_33">Speed up selenium and Time out</a>
</li>
<li>
<a href="#toc_34">An example of using regex parse table</a>
</li>
<li>
<a href="#toc_35">start and shutdown the selenium-server</a>
</li>
<li>
<a href="#toc_36">phantomJS options</a>
</li>
<li>
<a href="#toc_37">Pycharm unittest not finding the module path</a>
</li>
<li>
<a href="#toc_38">Conda rename environment</a>
</li>
<li>
<a href="#toc_39">Deploy an egg package to distributed nodes on hadoop</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[pig]]></title>
    <link href="www.echo-ohce.com/14742423250700.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250700.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">File Name Pattern Substitution</a>
</li>
<li>
<a href="#toc_1"><code>ToDate(milliseconds)</code> gives wrong date</a>
</li>
<li>
<a href="#toc_2">DateTime SimpleDateFormat</a>
</li>
<li>
<a href="#toc_3">Parameter substitution Shell Runner subtle bug</a>
</li>
<li>
<a href="#toc_4">Becareful of <code>DateTime</code> data type in pig</a>
</li>
<li>
<a href="#toc_5">More pitfalls for <code>DateTime</code> data type</a>
</li>
<li>
<a href="#toc_6">Setting of memory</a>
</li>
<li>
<a href="#toc_7">Pig Unit Test <code>override()</code> method</a>
</li>
<li>
<a href="#toc_8">Error message for wrong reference operator</a>
</li>
<li>
<a href="#toc_9">Pig Java UDF Unit Test</a>
</li>
<li>
<a href="#toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></a>
</li>
<li>
<a href="#toc_11">specify function output data type</a>
</li>
<li>
<a href="#toc_12">using <code>u0001</code> as the delimiter</a>
</li>
<li>
<a href="#toc_13">Error with SUBSTRING and SIZE function</a>
</li>
<li>
<a href="#toc_14">oneline to order two fields</a>
</li>
<li>
<a href="#toc_15">sort with duplicates</a>
</li>
<li>
<a href="#toc_16">input and output are reserved keyword in pig</a>
</li>
<li>
<a href="#toc_17">Mystery ERROR 2999</a>
</li>
<li>
<a href="#toc_18">Cumsum</a>
</li>
<li>
<a href="#toc_19">A pretty good graph cluster to pair Pig example code</a>
</li>
<li>
<a href="#toc_20">A pretty good Score bucketize and cumsum precision Pig example code</a>
</li>
<li>
<a href="#toc_21">Parameter Substitution in Pig</a>
</li>
<li>
<a href="#toc_22">SUBSTRING function</a>
</li>
<li>
<a href="#toc_23">Nested FOREACH</a>
</li>
<li>
<a href="#toc_24">Function are allowed inside FOREACH</a>
</li>
<li>
<a href="#toc_25">Use Star Expression and Project-Range Expression</a>
</li>
<li>
<a href="#toc_26">Strange bug ERROR 2116 related with compression</a>
</li>
<li>
<a href="#toc_27">Strange Bug with Split of two logic expressions</a>
</li>
<li>
<a href="#toc_28">Use <code>AvroStorage()</code> inside pig</a>
</li>
<li>
<a href="#toc_29">Load Parquet data to pig</a>
</li>
<li>
<a href="#toc_30">A side-by-side datatype comparison among avro, parquet, and pig</a>
</li>
<li>
<a href="#toc_31">More about the parquet</a>
</li>
<li>
<a href="#toc_32">From one relation, put two fields into one bag by FLATTEN</a>
</li>
<li>
<a href="#toc_33">Learning from piggybank <code>HashFNV</code></a>
</li>
</ul>


<hr/>

<h1 id="toc_0">File Name Pattern Substitution</h1>

<p>Pig is using hadoop file <code>glob</code> utilities to process the file name pattern. It is <font color=red>NOT</font> using shell&#39;s <code>glob</code>. Hadoop&#39;s <code>glob</code> are documented <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html#globStatus%28org.apache.hadoop.fs.Path%29">here</a>. Specially, it does not support <code>..</code> operator for a range.</p>

<p><a href="http://stackoverflow.com/questions/3515481/pig-latin-load-multiple-files-from-a-date-range-part-of-the-directory-structur">reference post</a></p>

<h1 id="toc_1"><code>ToDate(milliseconds)</code> gives wrong date</h1>

<p>Bug looks like always return date of some time at <code>1970-01-17</code>. This is because the input is 10 digits in seconds, and input should be in milliseconds. Need \( \times 1000\) on the input to get the correct DateTime.</p>

<h1 id="toc_2">DateTime SimpleDateFormat</h1>

<p>Pig&#39;s <code>DateTime</code> type conforms to the following format:</p>

<pre><code class="language-java">SimpleDateFormat dateFormat = 
    new SimpleDateFormat(&quot;yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSSXXX&quot;);
</code></pre>

<p>The string format is: <code>2001-07-04T12:08:56.235-07:00</code><br/>
<a href="https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html">Java official example table</a></p>

<p>[YGMARK] <code>-07:00</code> timezone format is only supported after JDK 7.</p>

<h1 id="toc_3">Parameter substitution Shell Runner subtle bug</h1>

<p>I like to define <code>_</code> as the output file separator. I usually have parameter <code>SEP= &#39;_&#39;</code> in my pig script and shell runner. However, if this parameter is defined inside shell runner, <em>it has to be escaped</em>, like: <code>SEP=&#39;\_&#39;</code>. Otherwise, this mislead error message will show:  </p>

<blockquote>
<p>ERROR org.apache.pig.Main - ERROR 1000: Error during parsing. Lexical error at line 1, column 6.  Encountered: <EOF> after : &quot;&quot;  </p>
</blockquote>

<p>It does not need be escaped if only defined inside pig script.<br/>
Another side note for seeing the above message. It is very likely caused by unbalanced bracket, quote, <font color='red'>extra space between the bash line separator <code>\</code> and the new line</font> etc.</p>

<h1 id="toc_4">Becareful of <code>DateTime</code> data type in pig</h1>

<p>It is convenient to get the time duration of difference by using the <code>DateTime</code> data type. However, in pig it is a know bug that <a href="https://issues.apache.org/jira/browse/PIG-3953">it can not be compared correctly.</a> Therefore ordering, grouping type of work should use either <code>ToUnixTime()</code> or <code>ToString()</code> before ordering.</p>

<blockquote>
<p>Error: org.joda.time.DateTime.compareTo ...</p>
</blockquote>

<h1 id="toc_5">More pitfalls for <code>DateTime</code> data type</h1>

<ul>
<li>The 2nd argument of <code>ToString(datetime [, format string])</code> is not optional, but a must for pig 0.12 (<a href="https://issues.apache.org/jira/browse/PIG-3805">Bug fixed on 0.13</a>)</li>
<li>For my case, <code>ToString(date, &#39;yyyyMMdd&#39;)</code> works.</li>
<li>All capital function name is not accepted, like <code>TOSTRING()</code> fails.</li>
<li>Also, there is no way to directly read in the <code>DateTime</code> type to pig. It still need be read in as <code>chararray</code>, otherwise it will be just null. (I tested it). <a href="http://stackoverflow.com/a/22052965">Stackoverflow post</a></li>
<li><code>GetMilliSecond(DateTime datetime)</code> is not the function to get the millisecond from epoch. It ... en, as the name indicates ... just get the millisecond of the time. To get the <strong>second</strong> use <code>ToUnixTime(DateTime datetime)</code>. To get the <strong>millisecond</strong> use <code>ToMilliSeconds(DateTime datetime)</code>.</li>
</ul>

<h1 id="toc_6">Setting of memory</h1>

<p>In side pig script or grunt:  </p>

<pre><code class="language-bash">SET mapreduce.map.memory.mb 4096;
SET mapreduce.reduce.memory.mb 8192;
</code></pre>

<h1 id="toc_7">Pig Unit Test <code>override()</code> method</h1>

<p>Code normally like:  </p>

<pre><code class="language-java">PigTest test = new PigTest(pigScript, parameters);
test.override(&quot;alias&quot;, &quot;alias = LOAD ...&quot;);
</code></pre>

<p>Inside the override, can not use variable substitution, otherwise will see error like:  </p>

<blockquote>
<p>org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias  </p>
</blockquote>

<p>Wrong:  </p>

<pre><code class="language-java">test.override(&quot;alias&quot;, &quot;alias = LOAD &#39;$INPUT&#39; USING PigStorage() AS (...);&quot;);
</code></pre>

<p>Should always use actual file path:</p>

<pre><code class="language-java">test.override(&quot;alias&quot;, &quot;alias = LOAD &#39;input.txt&#39; USING PigStorage() AS (...);&quot;);
</code></pre>

<h1 id="toc_8">Error message for wrong reference operator</h1>

<p>If receive following error message:  </p>

<blockquote>
<p>Error: Scalar has more than one row in the output.  </p>
</blockquote>

<p>It is very likely that I got a dot <code>Alias.field</code> where you need a double semi-colon <code>Alias::field</code>.<br/><br/>
<a href="http://stackoverflow.com/questions/22522155/pig-scalar-has-more-than-one-row-in-the-output">stackoverflow post</a>  </p>

<h1 id="toc_9">Pig Java UDF Unit Test</h1>

<p>Find a good <a href="http://www.crackinghadoop.com/unit-test-java-udfs/">post</a> specifically for Java UDF unit test.<br/><br/>
The main point is to use <code>DefaultTuple</code> to inject test cases into the UDF.  </p>

<pre><code class="language-java">String inputText = &quot;09/15/2014&quot;;
DefaultTuple input = new DefaultTuple();
input.append(inputText);
</code></pre>

<h1 id="toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></h1>

<ol>
<li><code>join</code> (more specifically, inner join) acts on more than two relations:<br/>
<code>X = JOIN A BY fieldA, B BY fieldB, C BY fieldC;</code> </li>
<li><p><code>join</code> on duplicates gives cross product on the two relations.  </p>

<pre><code class="language-sql">A = LOAD &#39;data1&#39; AS (a1:int,a2:int,a3:int);
DUMP A;   
(1,2,3)
(4,2,1)
(4,3,3)

B = LOAD &#39;data2&#39; AS (b1:int,b2:int);
DUMP B;
(4,6)
(4,9)

X = JOIN A BY a1, B BY b1;
DUMP X;
(4,2,1,4,6)
(4,3,3,4,6)
(4,2,1,4,9)
(4,3,3,4,9)  
</code></pre></li>
<li><p><code>join</code> on null disregards (filters out) null values. See the <a href="https://pig.apache.org/docs/r0.11.1/basic.html#nulls_join">official document</a>.  </p></li>
<li><p><code>cogroup</code> on duplicates gives nested set of tuples for the two relations (a bag for each side of the relation). If <code>flatten</code> is then used, will generate cross product.  </p></li>
<li><p><code>cogroup</code> or <code>group</code> for <code>null</code> key, will be grouped together per relation. <a href="https://pig.apache.org/docs/r0.11.1/basic.html#nulls_group">official document</a>  </p></li>
<li><p><code>cogroup</code> or <code>group</code> for <code>null</code> value, will just keep the empty bag, unless <code>inner</code> keywords is used.  </p></li>
<li><p><code>flatten</code> of an empty bag will <font color='red'><strong>remove</strong></font> the whole record (for the cross product case of <code>flatten</code> two bags) because <code>flatten</code> an empty bag produces no output. <a href="http://datafu.incubator.apache.org/docs/datafu/guide/more-tips-and-tricks.html">Refer to this post</a> to under this further and a trick (<em>generating a bag with a null tuple</em>) to avoid this behavior for getting &#39;outer join&#39; effect.</p></li>
</ol>

<h1 id="toc_11">specify function output data type</h1>

<p>No matter whether it is builtin function or UDF, it is always better to specify this function&#39;s output data type.  </p>

<p>This is wrong, because the CONCAT output type is <code>bytearray</code>:    </p>

<pre><code class="language-sql">ip_db = FOREACH ip_db_m3 GENERATE MD5(CONCAT(ip_raw, &#39;umpdmp&#39;)) AS ip;   
</code></pre>

<p>This is right by specifying clearly that the output data type is <code>chararray</code>:  </p>

<pre><code class="language-sql">ip_db_m3 = FOREACH ip_db_m2 GENERATE CONCAT(ip_raw, &#39;umpdmp&#39;) AS ip:chararray;
ip_db = FOREACH ip_db_m3 GENERATE MD5(ip) AS ip;   
</code></pre>

<h1 id="toc_12">using <code>\u0001</code> as the delimiter</h1>

<ul>
<li>If want to use <code>LzoPigStorage</code> need be careful about in pig, to specify the parameter of the function, you cannot do it at the code where calls this function, but need do it in the <code>DEFINE</code> statement at the beginning, like below:<br/></li>
</ul>

<pre><code class="language-sql">DEFINE LzoPigStorage com.twitter.elephantbird.pig.store.LzoPigStorage(&#39;\u0001&#39;);   

store OUT into &#39;$OUTPUT&#39; using LzoPigStorage();   
</code></pre>

<ul>
<li>Note that <code>&#39;^A&#39;</code> will <strong>NOT</strong> work, Only <code>&#39;\u0001&#39;</code> works.<br/></li>
<li>If use <code>PigStorage</code> just specify it inside the function like below:<br/></li>
</ul>

<pre><code class="language-sql">A = LOAD &#39;input.txt&#39; USING PigStorage(&#39;,&#39;);   
STORE A INTO &#39;out&#39; USING PigStorage(&#39;\u0001&#39;);   
</code></pre>

<h1 id="toc_13">Error with SUBSTRING and SIZE function</h1>

<p>I was trying to remove the leading <code>.</code> of a ip string, and used <code>SUBSTRING(ip, 1, SIZE(ip)) AS ip_fixed:chararray</code>. The error message reads:  </p>

<blockquote>
<p>ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.SUBSTRING as multiple or none of them fit. Please use an explicit cast  </p>
</blockquote>

<p>The reason is because <code>SIZE()</code> function returns <code>long</code> type, but <code>SUBSTRING()</code> function needs <code>int</code> type. The following code passes: <code>SUBSTRING(ip, 1, (int) SIZE(ip)) AS ip_fixed:chararray</code>  </p>

<h1 id="toc_14">oneline to order two fields</h1>

<pre><code class="language-sql">CC_raw = LOAD &#39;$INPUT_CC&#39; USING PigStorage()
                   AS (id1:chararray, id2:chararray, score:double);
CC_m0 = FOREACH CC_raw GENERATE 
            FLATTEN(((id1 &lt; id2) ? (id1, id2) : (id2, id1))) AS (smallID, largeID),
            score AS score;
</code></pre>

<p>order the two fields <code>id1</code> and <code>id2</code>.</p>

<h1 id="toc_15">sort with duplicates</h1>

<p>sort with heavily duplicated field sometimes generate highly skewed partitions (they are partitioned based on bins, records with the same value of the sorting field will be inside the same bin). This will reduce the performance dramatically.</p>

<p>Solution is to sort with combined fields like <code>BY (f1, f2)</code> in which, <code>f1</code> is the actual field that need be sorted but with a lot of duplicates, <code>f2</code> is another field that is much more evenly distributed to help reduce the skew.</p>

<p>Thanks for MingYan :)  </p>

<h1 id="toc_16">input and output are reserved keyword in pig</h1>

<p>Do not use <code>input</code> or <code>output</code> as relations name, because they are reserved keyword. Error message is like:  </p>

<blockquote>
<p>ERROR org.apache.pig.PigServer - exception during parsing: Error during parsing. <file clean_vertex_result.pig, line 3, column 0>  mismatched input &#39;input&#39; expecting EOF</p>
</blockquote>

<h1 id="toc_17">Mystery ERROR 2999</h1>

<blockquote>
<p>ERROR 2999: Unexpected internal error. null</p>
</blockquote>

<p>One case (I wasted one full day on this!) is I used the same name for both a relation and a field!</p>

<p>Solution: add <code>_r</code> for all relation names</p>

<h1 id="toc_18">Cumsum</h1>

<p>use <code>Over, Stitch, ORDER BY</code></p>

<pre><code class="language-sql">REGISTER /home/adsymp/lib/piggybank.jar;

DEFINE OVER org.apache.pig.piggybank.evaluation.Over(&#39;long&#39;);
DEFINE STITCH org.apache.pig.piggybank.evaluation.Stitch();

output_to_LP_m0 = FOREACH (GROUP pairs_bucketized ALL) {
                        sorted = ORDER pairs_bucketized BY score_bucket DESC;
                        GENERATE FLATTEN(STITCH(sorted, OVER(sorted.tp_inbucket, &#39;sum(long)&#39;), OVER(sorted.fp_inbucket, &#39;sum(long)&#39;)));};
</code></pre>

<ul>
<li><code>STITCH</code> take bags as input, and its output cannot be directly assigned to a relation, i.e. it has to be inside a <code>FOREACH</code> statement <a href="http://stackoverflow.com/a/27798007">Stackoverflow post</a> </li>
<li><code>ORDER</code> makes the rows sorted for the <code>cumsum</code></li>
<li><code>OVER</code> does the <code>cumsum</code> <a href="https://pig.apache.org/docs/r0.12.0/api/org/apache/pig/piggybank/evaluation/Over.html">Official document</a></li>
<li><code>STITCH</code> make the <code>cumsum</code> one-on-one appending with the correct row.</li>
<li>When I tried <code>OVER</code> through <code>grunt</code>, I checked the output schema is <code>null</code>. This is not a bug! In fact, if we didn&#39;t input the string of type in the <code>Over()</code> constructor, this <code>piggybank</code> function does not know what is the output schema.</li>
</ul>

<p>One lesson learned: If it is the logic planning stage error, checking the alias.</p>

<h1 id="toc_19">A pretty good graph cluster to pair Pig example code</h1>

<p><code>/Users/yugan/repo/drawbridge/dpp/workflow-new/miaozhen-scripts/graph/Stats.pig</code></p>

<p>In this code:</p>

<ul>
<li>use of <code>TOKENIZE</code>, <code>LAST_INDEX_OF</code></li>
<li>The way of exploding graph <code>cluster</code> into pairs and get the TP/FP with label data is pretty clean and neat.</li>
<li>use <code>HashString</code> to sample</li>
</ul>

<h1 id="toc_20">A pretty good Score bucketize and cumsum precision Pig example code</h1>

<p><code>/Users/yugan/repo/drawbridge/dpp/workflow-new/miaozhen-scripts/rank/ToPrec2.pig</code></p>

<p>In this code:</p>

<ul>
<li>use <code>$SCORE_FACTOR</code> converting double to long for bucketize.</li>
<li>use of three UDF <code>GetScoreBuckets</code>, <code>AnalyzeScoreBucket</code>, and <code>GetPrecision</code>.</li>
<li>use <code>HashString</code> to sample</li>
<li>use <code>replicated</code> mode for the JOIN</li>
</ul>

<p>But I have also pretty good code using <code>Over</code>, <code>Stitch</code>, <code>ORDER BY</code> to implement cumsum as shown above.</p>

<h1 id="toc_21">Parameter Substitution in Pig</h1>

<p>Official documentation <a href="http://wiki.apache.org/pig/ParameterSubstitution">here</a></p>

<h1 id="toc_22">SUBSTRING function</h1>

<p>It is does not include the second index position<br/>
<code>SUBSTRING(string, startIndex, stopIndex)</code>, <code>startIndex</code> starts from <code>0</code>, and <code>stopIndex</code> is not included.<br/><br/>
Given a field named <code>alpha</code> whose value is <code>ABCDEF</code>, to return substring <code>BCD</code> use this statement: <code>SUBSTRING(alpha,1,4)</code>. Note that <code>1</code> is the index of <code>B</code> (the first character of the substring) and <code>4</code> is the index of <code>E</code> (the character <strong>following</strong> the last character of the substring).</p>

<p>Note that the description is different in the official documents both for Pig 0.12:<br/>
* <a href="https://pig.apache.org/docs/r0.12.0/func.html#substring">Correct official document</a> <br/>
* <a href="https://pig.apache.org/docs/r0.12.0/api/org/apache/pig/builtin/SUBSTRING.html">Wrong official document</a></p>

<h1 id="toc_23">Nested FOREACH</h1>

<ol>
<li>Allowed operation are <code>CROSS</code>, <code>DISTINCT</code>, <code>FILTER</code>, <code>FOREACH</code>, <code>LIMIT</code>, <code>ORDER BY</code>, and Project operation. <a href="http://pig.apache.org/docs/r0.15.0/basic.html#foreach">Official Document</a></li>
<li><code>SPLIT</code> does not work inside nested FOREACH</li>
</ol>

<h1 id="toc_24">Function are allowed inside FOREACH</h1>

<p><a href="http://stackoverflow.com/a/26996221/4229125">an example of using <code>SUM</code> inside FOREACH from stackoverflow</a>  </p>

<pre><code class="language-sql">file = LOAD &#39;input.txt&#39; USING PigStorage() AS (type: chararray, year: chararray,
match_count: float, volume_count: float);
grouped = GROUP file BY type;
group_operat = FOREACH grouped {
                                 sum_m = SUM(file.match_count);
                                 sum_v = SUM(file.volume_count);
                                 GENERATE group,(float)(sum_m/sum_v) as sum_mv;
                                }
</code></pre>

<p>Lesson learned: Do not use 2 levels of <code>FOREACH</code> if it is not absolutely necessary.</p>

<p>Wrong </p>

<pre><code class="language-sql">acookie_log_input = LOAD &#39;$INPUT_ACOOKIE&#39; USING LzoPigStorage()
                       AS (acookie:chararray, ip:chararray, time_stamp:DateTime, useragent:chararray, browser:chararray,
                           os:chararray, url:chararray, province:chararray, acookie_daily_tag:chararray,
                           url_catelevel1:chararray, url_catelevel2:chararray, date:DateTime);
acookie_log_input_m0 = FOREACH acookie_log_input GENERATE acookie AS acookie, SUBSTRING(province, 0, 2) AS province:chararray;
acookie_log_input_m1 = FILTER acookie_log_input_m0 BY province IS NOT NULL;
acookie_log_input_m2 = FOREACH (GROUP acookie_log_input_m1 BY (acookie, province)) GENERATE FLATTEN(group) AS (acookie, province),
                        COUNT(acookie_log_input_m1) AS acookie_province_cnt;
-- Does not work from here
acookie_province = FOREACH (GROUP acookie_log_input_m2 BY acookie) {
                    total_cnt = SUM(acookie_log_input_m2.acookie_province_cnt);
                    normalized = FOREACH acookie_log_input_m2 GENERATE (int) (acookie_province_cnt * 100 / total_cnt) AS hist:int;
                    sorted = ORDER normalized BY hist DESC;
                    GENERATE group AS acookie, BagToString(sorted.hist) AS hist:chararray;};
</code></pre>

<p>Correct</p>

<pre><code class="language-sql">acookie_province_m0 = FOREACH (GROUP acookie_log_input_m2 BY acookie) {
                    total_cnt = SUM(acookie_log_input_m2.acookie_province_cnt);
                    GENERATE group AS acookie, FLATTEN(acookie_log_input_m2.acookie_province_cnt) AS (acookie_province_cnt),
                    total_cnt AS total_cnt;};
acookie_province_m1 = FOREACH acookie_province_m0 GENERATE acookie AS acookie, (int) (acookie_province_cnt*100/total_cnt) AS hist:int;
acookie_province = FOREACH (GROUP acookie_province_m1 BY acookie) {
                    sorted = ORDER acookie_province_m1 BY hist DESC;
                    GENERATE group AS acookie, BagToString(sorted.hist) AS hist:chararray;};
</code></pre>

<h1 id="toc_25">Use Star Expression and Project-Range Expression</h1>

<p>to avoid copy all the fields again and again.<br/>
<a href="https://pig.apache.org/docs/r0.12.0/basic.html#expressions">0.12 official documentation</a></p>

<p>Star Expression: <code>*</code><br/>
Project-Range Expression: <code>..</code></p>

<p>A good example:</p>

<pre><code class="language-sql">
lkp_input = LOAD &#39;$INPUT_PAIR&#39; USING PigStorage() AS (id1:long, id2:long, c2NumPath:int,
            c2SumTot:float, c2MinTot:float, c2MaxTot:float, c2SumS1:float, c2MinS1:float, 
            c2MaxS1:float, c2SumS2:float, c2MinS2:float, c2MaxS2:float, c3NumPath:int, 
            c3NumNoRedundantPath:int, c3NumDistinctID:int, c3SumTot:float, c3MinTot:float, 
            c3MaxTot:float, c3SumS1:float, c3MinS1:float, c3MaxS1:float, c3SumS2:float, 
            c3MinS2:float, c3MaxS2:float, c3SumS3:float, c3MinS3:float, c3MaxS3:float);

-- avoid using duplicated field names!
lkp_index_table = LOAD &#39;$INPUT_INDEX_TABLE&#39; USING LzoPigStorage() 
                    AS (id:long, str_id:chararray);

lkp_str_l = FOREACH (JOIN lkp_input BY id1, lkp_index_table BY id) GENERATE
            str_id AS strId1, id2..c3MaxS3;
lkp_str = FOREACH (JOIN lkp_str_l BY id2, lkp_index_table BY id) GENERATE
            strId1 AS strId1, str_id AS strId2, c2NumPath..c3MaxS3;

</code></pre>

<h1 id="toc_26">Strange bug ERROR 2116 related with compression</h1>

<p>Error code like:</p>

<blockquote>
<p>ERROR 2116:<br/>
<file lkpAddLabelNoLzo.pig, line 38, column 0> Output Location Validation Failed for: &#39;hdfs://sc2prod/user/yu/linkprediction/processed_full/0.05_0.08_0.005_11/pairsNoLzo</p>

<p>org.apache.pig.impl.plan.VisitorException: ERROR 2116:<br/>
<file lkpAddLabelNoLzo.pig, line 38, column 0> Output Location Validation Failed for: &#39;hdfs://sc2prod/user/yu/linkprediction/processed_full/0.05_0.08_0.005_11/pairsNoLzo<br/>
        at org.apache.pig.newplan.logical.rules.InputOutputFileValidator$InputOutputFileVisitor.visit(InputOutputFileValidator.java:75)</p>
</blockquote>

<p>This error has nothing to do with the output location, but because the wrong compression setting</p>

<p>The wrong one caused the bug</p>

<pre><code class="language-sql">SET output.compression.enabled true;
</code></pre>

<p>The correct one:</p>

<pre><code class="language-sql">SET mapreduce.output.fileoutputformat.compress true;
</code></pre>

<h1 id="toc_27">Strange Bug with Split of two logic expressions</h1>

<p>The error message looks like this:</p>

<blockquote>
<p>2016-11-10 03:38:16,010 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: Split - scope-1576 Operator Key: scope-1576): org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error while executing ForEach at [null[-1,-1]]<br/>
    at ....PhysicalOperator.processInput(PhysicalOperator.java:289)<br/>
    at ....physicalLayer.relationalOperators.POSplit.getNextTuple(POSplit.java:214)<br/>
    at ....relationalOperators.POSplit.runPipeline(POSplit.java:255)<br/>
    ...<br/>
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error while executing ForEach at [null[-1,-1]]<br/>
    at ....physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:306)<br/>
    at ....PhysicalOperator.processInput(PhysicalOperator.java:281)<br/>
    ... 19 more<br/>
Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Float<br/>
    at java.lang.Float.compareTo(Float.java:50)<br/>
    at ....expressionOperators.NotEqualToExpr.doComparison(NotEqualToExpr.java:113)<br/>
    at ....expressionOperators.NotEqualToExpr.getNextBoolean(NotEqualToExpr.java:84)<br/>
    at ....expressionOperators.POAnd.getNextBoolean(POAnd.java:90)</p>
</blockquote>

<p>This error message is <strong><font color='red'>misleading</font></strong>. There is not <code>ClassCastException</code> issue, it is caused by the <code>Split</code>. I don&#39;t understand how this is happened, but <code>split</code> is merely a syntax sugar, we can just use the <code>filter</code> with the same efficiency.</p>

<p>Problem code:</p>

<pre><code class="language-sql">lkp_input = LOAD &#39;$INPUT_PAIR&#39; USING LzoPigStorage() AS (strID1:chararray, strID2:chararray, c2NumPath:int, c2SumTot:float,
                    c2MinTot:float, c2MaxTot:float, c2SumS1:float, c2MinS1:float, c2MaxS1:float, c2SumS2:float,
                    c2MinS2:float, c2MaxS2:float, c3NumPath:int, c3NumNoRedundantPath:int, c3NumDistinctID:int,
                    c3SumTot:float, c3MinTot:float, c3MaxTot:float, c3SumS1:float, c3MinS1:float, c3MaxS1:float,
                    c3SumS2:float, c3MinS2:float, c3MaxS2:float, c3SumS3:float, c3MinS3:float, c3MaxS3:float, label:int);
SPLIT lkp_input INTO
    c2_only_pairs IF ((c2NumPath != -1) AND (c3NumPath == -1)),
    c3_only_pairs IF ((c2NumPath == -1) AND (c3NumPath != -1)),
    c2_c3_pairs OTHERWISE;

c2_all_pairs = FILTER lkp_input BY (c2NumPath != -1);
c3_all_pairs = FILTER lkp_input BY (c3NumPath != -1);
</code></pre>

<p>Correct code:</p>

<pre><code class="language-sql">lkp_input = LOAD &#39;$INPUT_PAIR&#39; USING LzoPigStorage() AS (strID1:chararray, strID2:chararray, c2NumPath:int, c2SumTot:float,
                    c2MinTot:float, c2MaxTot:float, c2SumS1:float, c2MinS1:float, c2MaxS1:float, c2SumS2:float,
                    c2MinS2:float, c2MaxS2:float, c3NumPath:int, c3NumNoRedundantPath:int, c3NumDistinctID:int,
                    c3SumTot:float, c3MinTot:float, c3MaxTot:float, c3SumS1:float, c3MinS1:float, c3MaxS1:float,
                    c3SumS2:float, c3MinS2:float, c3MaxS2:float, c3SumS3:float, c3MinS3:float, c3MaxS3:float, label:int);

c2_only_pairs = FILTER lkp_input BY ((c2NumPath != -1) AND (c3NumPath == -1));
c3_only_pairs = FILTER lkp_input BY ((c2NumPath == -1) AND (c3NumPath != -1));
c2_c3_pairs = FILTER lkp_input BY ((c2NumPath != -1) AND (c3NumPath != -1));;
    
c2_all_pairs = FILTER lkp_input BY (c2NumPath != -1);
c3_all_pairs = FILTER lkp_input BY (c3NumPath != -1);
</code></pre>

<h1 id="toc_28">Use <code>AvroStorage()</code> inside pig</h1>

<p>Need register the following jars</p>

<pre><code class="language-sql">REGISTER /home/adsymp/lib/piggybank.jar;
REGISTER /home/adsymp/lib/avro-1.7.6.jar;
REGISTER /home/adsymp/lib/jackson-core-asl-1.7.3.jar;
REGISTER /home/adsymp/lib/jackson-mapper-asl-1.7.3.jar;
REGISTER /home/adsymp/lib/json-simple-1.1.jar;

DEFINE AvroStorage org.apache.pig.piggybank.storage.avro.AvroStorage(&#39;no_schema_check&#39;);
</code></pre>

<p>Also, don&#39;t mess with the <code>float</code> and <code>double</code> types with the AvroStorage input.</p>

<h1 id="toc_29">Load Parquet data to pig</h1>

<p>Need specify the schema string in the constructor</p>

<p>for example the following file:<br/>
<code>/user/oozie/pairing-modeling-gpairing/2016-12-23/cookie-cookie/mob-mob/output/part-r-00000-0eb9c76c-f170-4d4a-a20b-aa9f72d6654c.snappy.parquet</code></p>

<p>The schema of this parquet data is specified in the source code:<br/>
<code>/Users/yugan/repo/drawbridge/dpp/spark2/src/main/scala/ge/drawbrid/dpp/spark2/pairing/PairingFormat.scala</code></p>

<pre><code class="language-scala">case class PairData(id1: String, id2: String, id1Type: String, id2Type: String, pairingVersion: String,
                    id1TotalFrequency: Long, id2TotalFreqency: Long, precision: Double, rfc: Double, scores: scala.collection.Map[String, Double])
</code></pre>

<p>In order to read this data into pig we should do:</p>

<pre><code class="language-sql">A = LOAD &#39;/user/oozie/pairing-modeling-gpairing/2016-12-23/cookie-cookie/mob-mob/output/part-r-00000-0eb9c76c-f170-4d4a-a20b-aa9f72d6654c.snappy.parquet&#39; USING parquet.pig.ParquetLoader(&#39;id1:chararray, id2:chararray, id1Type:chararray, id2Type:chararray, pairingVersion:chararray, id1TotalFrequency:long, id2TotalFrequency:long, precision:double, rfc:double, scores:map[double]&#39;);
</code></pre>

<p>Don&#39;t mess up the field names and the float/double datatypes. Also pay attention to the <code>:map[double]</code> type specification (refer to the <a href="http://pig.apache.org/docs/r0.12.0/basic.html#schema-complex">pig schema documentation here</a>).</p>

<p><font color='red'><strong>The most important advantage is reading some columns out only</strong></font></p>

<pre><code class="language-sql">A = LOAD &#39;/user/oozie/pairing-modeling-gpairing/2016-12-23/cookie-cookie/mob-mob/output/part-r-00000-0eb9c76c-f170-4d4a-a20b-aa9f72d6654c.snappy.parquet&#39; USING parquet.pig.ParquetLoader(&#39;id1:chararray, id2:chararray, precision:double&#39;);
</code></pre>

<ul>
<li><strong>There is another way to load by column position</strong>
<a href="http://stackoverflow.com/a/32276119/4229125">stackoverflow post</a></li>
</ul>

<p>As specified in the <a href="https://github.com/Parquet/parquet-mr/blob/master/parquet-pig/src/main/java/parquet/pig/ParquetLoader.java#L119">source code</a>: just add a boolean string as the 2nd argument (I did not try it)</p>

<pre><code class="language-java">ParquetLoader(String requestedSchemaStr, String columnIndexAccess)

ParquetLoader(&#39;n1:int, n2:float, n3:double, n4:long&#39;, &#39;true&#39;)
</code></pre>

<ul>
<li><font color='red'><strong>read in the metadata in the pig mr-planning phase</strong></font></li>
</ul>

<p>when load parquet data into pig, all parquet files&#39; footer, i.e. metadata need be read in first (which in my case takes quite some time) <br/>
<a href="https://forums.databricks.com/questions/1097/stall-on-loading-many-parquet-files-on-s3.html">Very good reference</a><br/>
<a href="https://drill.apache.org/docs/optimizing-parquet-metadata-reading/">reference1</a><br/>
<a href="https://issues.apache.org/jira/browse/SPARK-9347">reference2</a><br/>
<a href="https://groups.google.com/forum/#!topic/parquet-dev/aQU9Q8f0bPY">reference3</a></p>

<blockquote>
<p>I might have an explanation. It appears to be because the current Spark/Hadoop versions are not designed to read thousands of tables (a handful of large/small tables instead). Before any job is actually submitted to the cluster, the driver will go through the parquet files one-by-one, reading the footers from each parquet part. If your parts are small, this is tantamount to the driver serially reading all your data from S3 before starting your job.<br/>
The driver log contains these lines:<br/>
Jun 27, 2015 8:34:23 PM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5 Jun 27, 2015 8:34:23 PM INFO: parquet.hadoop.SplitStrategy: Using Client Side Metadata Split Strategy Jun 27, 2015 8:34:24 PM INFO: parquet.hadoop.ClientSideMetadataSplitStrategy: There were no row groups that could be dropped due to filter predicates Jun 27, 2015 8:34:24 PM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 14<br/>
In addition, it appears that opening up parquet files through the newAPIHadoopFile creates a broadcast variable per file: SparkContext: Created broadcast 653 from newAPIHadoopFile at ADAMContext.scala:150<br/>
The solution would seem to be to both produce parquet files with larger/fewer parts, and to aggregate your parquet files together. Of course doing so means you might have to pay this penalty to simply run the job to aggregate to a single logical parquet file.<br/>
A _metadata files contains a copy of all the footers of all the files in same directory. To produce efficient splits and do projection push down we read the metadata on the client side. Consolidating the metadata per directory reduces the number of files to open and speeds up that process.If the _metadata file is absent we just fall back to the files footers, also if there are files are missing, we just ignore the information in the _metadata file so you can delete part files or move them around and it still works.</p>
</blockquote>

<ul>
<li><strong>Improve the speed of read in the metadata files</strong><br/>
Currently I have no good ways to generate the common metadata, the only option that I know is to increase the parquet.metadata.read.parallelism<br/></li>
</ul>

<p><a href="http://stackoverflow.com/questions/33726400/what-is-parquet-read-parallelism">stackoverflow</a></p>

<blockquote>
<p>increase ParquetFileReader parallelism from default 5 to 30 - with setting &quot;parquet.metadata.read.parallelism&quot;: &quot;30&quot; in newAPIHadoopFile config</p>
</blockquote>

<p>The way of setting it is like follow:</p>

<pre><code class="language-sql">%default BUCKET_SIZE 981

SET mapred.job.queue.name &#39;datascience&#39;;
SET parquet.metadata.read.parallelism 50;

REGISTER /home/adsymp/lib/dpp-pig-udf-0.0.1-SNAPSHOT.jar;
...
</code></pre>

<p><strong>reference</strong></p>

<ul>
<li><a href="https://groups.google.com/forum/#!topic/parquet-dev/qyqRrJ3bMew">A code snippet in a google-group&#39;s post</a></li>
</ul>

<pre><code class="language-sql">REGISTER /xxtech/scenarios/parquet-pig-bundle-1.2.4.jar
REGISTER $xxtechLocalRepo/xxtechPigExtension.jar
REGISTER $xxtechLocalRepo/lib/piggybank.jar
REGISTER $xxtechLocalRepo/lib/joda-time-1.6.jar

SET job.name parquet-store-$record-$parquet.pig;

dataImport = LOAD ‚/xxtech/$record/{%DATE%}/{%HOUR%}/{*}.avro&#39; USING com.xxtech.repo.pig.udf.storage.avro.ExtendedxxtechAvroStorage(&#39;$timestamp&#39;, &#39;-7200&#39;, &#39;missing_input_dir&#39;);

SET parquet.compression snappy
SET parquet.enable.dictionary true
SET parquet.block.size 536870912

store dataImport into &#39;/$parquet/xxtech/$record/20131020/10/$record.par&#39; using parquet.pig.ParquetStorer();

parquet.pig:

REGISTER /xxtech/scenarios/parquet-pig-bundle-1.2.4.jar

data = LOAD &#39;/parquet/xxtech/rawlogs/20131020/10/rawlogs.par/part-m-00000.snappy.parquet&#39; USING parquet.pig.ParquetLoader(&#39;SGSHeader: (VersionID: int,SequenceID: long,RecordType: int,TimeStamp: int,GMTOffset: int)&#39;);

a = GROUP data ALL;
describe a;


b = foreach a generate SUM(a.data.RecordType);
describe b;
dump b;
</code></pre>

<ul>
<li><a href="https://hadoopified.wordpress.com/2013/10/18/parquet-columnar-storage-hadoop/">A blog. By the way this is a good hadoop blog</a></li>
</ul>

<h1 id="toc_30">A side-by-side datatype comparison among avro, parquet, and pig</h1>

<p><a href="http://alvincjin.blogspot.com/2014/06/convert-same-pig-and-avro-output-schema.html">Not that useful, just a quick reference</a></p>

<h1 id="toc_31">More about the parquet</h1>

<p><strong>some useful command line tools</strong></p>

<pre><code class="language-bash"># showing schema
hadoop parquet.tools.Main schema /user/oozie/pairing-modeling-gpairing/2016-12-23/dev-cookie/dev-mob/output/part-r-28207-74d4715e-3418-4f39-9519-109c7364f939.snappy.parquet

# Check meta
hadoop parquet.tools.Main meta ...

# check head
 hadoop parquet.tools.Main head ...
</code></pre>

<p>More <a href="https://www.cloudera.com/documentation/enterprise/5-8-x/topics/cdh_ig_parquet.html#parquet_files">here</a></p>

<ul>
<li>A visual representation of parquet data strcuture</li>
</ul>

<p><img src="media/14742423250700/14827763637843.jpg" alt=""/></p>

<p><a href="http://grepalex.com/2014/05/13/parquet-file-format-and-object-model/">Original post</a></p>

<h1 id="toc_32">From one relation, put two fields into one bag by FLATTEN</h1>

<p>{A:(id1:chararray, id2:chararray)} → {B:(id:chararray)}, in which B::id is the collection of A::id1 and A::id2.</p>

<p>The following is the code that I tested and works (in hlab &gt; pig-book &gt; script &gt;  collect_one_bag.pig</p>

<pre><code>four_field = LOAD &#39;data_flatten&#39; AS (one:INT, two:INT, three:INT, four:INT);
one_field = FOREACH four_field GENERATE FLATTEN(TOBAG(one, two, three, four));
DESCRIBE one_field;
</code></pre>

<h1 id="toc_33">Learning from piggybank <code>HashFNV</code></h1>

<p>It used a technique to support different function implementations according to the input schema</p>

<p><code>HashFNV</code> has two subclasses <code>HashFNV1</code> and <code>HashFNV2</code>, in which, <code>HashFNV1</code> is used when input only has one argument, i.e. the string need be hashed, and <code>HashFNV2</code> is used when input has two arguments, i.e. the string need be hashed and the integer for the mod operation.</p>

<p>The key implementation method that is called in both the two subclasses <code>HashFNV1</code> and <code>HashFNV2</code> is actually inside <code>HashFNV</code>, and is called <code>hashFnv32(String s)</code></p>

<p>This technique of have different implementations is by overriding method <code>public List&lt;FuncSpec&gt; getArgToFuncMapping()</code>. Official document is at <a href="https://github.com/apache/pig/blob/branch-0.12/src/org/apache/pig/EvalFunc.java#L279">here</a>. And <code>HashFNV</code> is a good example of <a href="http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/string/HashFNV.java?view=markup#l77">using it</a> </p>

<p>For my case, in Spark, there is already an implementation I can use at <code>ge.drawbrid.dpp.spark2.util.Util#hashFnv32</code></p>

<p>Source code:<br/>
* <code>HashFNV</code> is at <a href="http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/string/HashFNV.java?view=markup">here</a><br/>
* <code>HashFNV1</code> is at <a href="http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/string/HashFNV1.java?view=markup">here</a><br/>
* <code>HashFNV2</code> is at <a href="http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/string/HashFNV2.java?view=markup">here</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">File Name Pattern Substitution</a>
</li>
<li>
<a href="#toc_1"><code>ToDate(milliseconds)</code> gives wrong date</a>
</li>
<li>
<a href="#toc_2">DateTime SimpleDateFormat</a>
</li>
<li>
<a href="#toc_3">Parameter substitution Shell Runner subtle bug</a>
</li>
<li>
<a href="#toc_4">Becareful of <code>DateTime</code> data type in pig</a>
</li>
<li>
<a href="#toc_5">More pitfalls for <code>DateTime</code> data type</a>
</li>
<li>
<a href="#toc_6">Setting of memory</a>
</li>
<li>
<a href="#toc_7">Pig Unit Test <code>override()</code> method</a>
</li>
<li>
<a href="#toc_8">Error message for wrong reference operator</a>
</li>
<li>
<a href="#toc_9">Pig Java UDF Unit Test</a>
</li>
<li>
<a href="#toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></a>
</li>
<li>
<a href="#toc_11">specify function output data type</a>
</li>
<li>
<a href="#toc_12">using <code>u0001</code> as the delimiter</a>
</li>
<li>
<a href="#toc_13">Error with SUBSTRING and SIZE function</a>
</li>
<li>
<a href="#toc_14">oneline to order two fields</a>
</li>
<li>
<a href="#toc_15">sort with duplicates</a>
</li>
<li>
<a href="#toc_16">input and output are reserved keyword in pig</a>
</li>
<li>
<a href="#toc_17">Mystery ERROR 2999</a>
</li>
<li>
<a href="#toc_18">Cumsum</a>
</li>
<li>
<a href="#toc_19">A pretty good graph cluster to pair Pig example code</a>
</li>
<li>
<a href="#toc_20">A pretty good Score bucketize and cumsum precision Pig example code</a>
</li>
<li>
<a href="#toc_21">Parameter Substitution in Pig</a>
</li>
<li>
<a href="#toc_22">SUBSTRING function</a>
</li>
<li>
<a href="#toc_23">Nested FOREACH</a>
</li>
<li>
<a href="#toc_24">Function are allowed inside FOREACH</a>
</li>
<li>
<a href="#toc_25">Use Star Expression and Project-Range Expression</a>
</li>
<li>
<a href="#toc_26">Strange bug ERROR 2116 related with compression</a>
</li>
<li>
<a href="#toc_27">Strange Bug with Split of two logic expressions</a>
</li>
<li>
<a href="#toc_28">Use <code>AvroStorage()</code> inside pig</a>
</li>
<li>
<a href="#toc_29">Load Parquet data to pig</a>
</li>
<li>
<a href="#toc_30">A side-by-side datatype comparison among avro, parquet, and pig</a>
</li>
<li>
<a href="#toc_31">More about the parquet</a>
</li>
<li>
<a href="#toc_32">From one relation, put two fields into one bag by FLATTEN</a>
</li>
<li>
<a href="#toc_33">Learning from piggybank <code>HashFNV</code></a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[git]]></title>
    <link href="www.echo-ohce.com/14742423250638.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250638.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Back to the unaltered HEAD</a>
</li>
<li>
<a href="#toc_1">Messed up local master, come back to the origin/master</a>
</li>
<li>
<a href="#toc_2">Reset one file</a>
</li>
<li>
<a href="#toc_3">Save change in one branch and apply it (partially apply it) to another branch</a>
</li>
<li>
<a href="#toc_4">Add only modified changes and ignore untracked (newly added) files</a>
</li>
<li>
<a href="#toc_5">Syntax of <code>.gitignore</code></a>
</li>
<li>
<a href="#toc_6">Untracked directory and file</a>
</li>
<li>
<a href="#toc_7">discard unstaged changes in Git</a>
</li>
<li>
<a href="#toc_8">Oh Shit, git</a>
</li>
<li>
<a href="#toc_9">Local unstaged change do not want to tracked and be committed</a>
</li>
<li>
<a href="#toc_10">git stash</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Back to the unaltered HEAD</h1>

<p>Run these two in order:  </p>

<pre><code class="language-bash">git reset --hard HEAD
git clean -df
</code></pre>

<p><font color='red'><strong>Be careful of using <code>git clean -df</code>, it will delete all the test cases</strong></font></p>

<h1 id="toc_1">Messed up local master, come back to the origin/master</h1>

<pre><code class="language-bash">git checkout -B master origin/master
</code></pre>

<p>If <code>-B</code> is given, the new branch is created if it doesn’t exist; otherwise, it is reset.  </p>

<h1 id="toc_2">Reset one file</h1>

<pre><code class="language-bash">git checkout HEAD -- my-file.txt
</code></pre>

<p><a href="http://stackoverflow.com/questions/7147270/hard-reset-of-a-single-file">Reference post1</a><br/><br/>
<a href="http://stackoverflow.com/questions/6561142/difference-between-git-checkout-filename-and-git-checkout-filename/6561160#6561160">Reference post2</a></p>

<h1 id="toc_3">Save change in one branch and apply it (partially apply it) to another branch</h1>

<pre><code class="language-bash">git stash
git checkout branch2
git stash list       # to check the various stash made in different branch
git stash pop        # pop out the last stash, and it will be deleted from the stash
git stash apply stash@{0}    # to select the stash and apply to branch2, the stash still saved in the list
git stash -u   # to stash currenlty untracked (newly added) files
</code></pre>

<h1 id="toc_4">Add only modified changes and ignore untracked (newly added) files</h1>

<pre><code class="language-bash">git add .   # stage all changed / untracked (newly added), but not includes deleted
git add -u  # stage all changed / deleted, but not untracked (newly added)
git add -A  # doing both of above
</code></pre>

<h1 id="toc_5">Syntax of <code>.gitignore</code></h1>

<pre><code class="language-bash">**/foo # matches file or directory &quot;foo&quot; anywhere
foo # the same as above  
**/foo/bar # matches file or   directory &quot;bar&quot; anywhere that is directly under directory &quot;foo&quot;  
abc/** # matches all files inside directory &quot;abc&quot;, infinite depth  
abc/* # matches all files inside directory &quot;abc&quot;, but not its sub-directories.  
a/**/b # match 0 or more directories in between  
a/**/b # matches &quot;a/b&quot;, &quot;a/x/b&quot;, &quot;a/x/y/b&quot; and so on.
</code></pre>

<h1 id="toc_6">Untracked directory and file</h1>

<ol>
<li><p>File previously tracked (in git repository) need remove it from the repo but keep it at local. <em>This is called as delete the cache</em><br/><br/>
<code>git rm -r -f --cached mydirectory/myfile</code><br/><br/>
the <code>-r</code> is recursive, the <code>-f</code> is force (in case, this tracked file has uncommitted change).  </p></li>
<li><p>Only change the local repo&#39;s <code>.gitignore</code> without change the remote repo&#39;s tracking behavior. Add ignore into <code>.git/info/exclude</code> instead of the <code>.gitignore</code> file at root.<br/><br/>
<a href="http://stackoverflow.com/questions/767147/ignore-the-gitignore-file-itself">reference post</a></p></li>
</ol>

<h1 id="toc_7">discard unstaged changes in Git</h1>

<pre><code class="language-bash">git checkout -- .
</code></pre>

<p>Make sure to include the last <code>.</code></p>

<h1 id="toc_8">Oh Shit, git</h1>

<p><a href="http://ohshitgit.com">An interesting blog describe git scenarios in plain english</a></p>

<h1 id="toc_9">Local unstaged change do not want to tracked and be committed</h1>

<p>This <a href="http://stackoverflow.com/questions/13630849/git-difference-between-assume-unchanged-and-skip-worktree">post</a> described my need:</p>

<blockquote>
<p>I have local changes to a file that I don&#39;t want to commit to my repository. It is a configuration file for building the application on a server, but I want to build locally with different settings. Naturally, the file always shows up when i do &#39;git status&#39; as something to be staged. I would like to hide this particular change and not commit it. I won&#39;t make any other changes to the file.</p>
</blockquote>

<p>In order to do that we should use <code>--skip-worktree</code> option like follow:</p>

<pre><code class="language-bash"># untrack this file: dpp/graph/proguard.gradle
git update-index --skip-worktree dpp/graph/proguard.gradle
</code></pre>

<p>The difference between <code>--assume-unchanged</code> and <code>--skip-worktree</code> is described well <a href="http://stackoverflow.com/a/13631525/4229125">here</a></p>

<h1 id="toc_10">git stash</h1>

<pre><code class="language-bash"># before checkout another branch
git stash
# checkcout to another branch and do other jobs
# after done, checkout back to this branch
git stash list
# stash@{0}: WIP on ali2: 0832570 Merge branch &#39;ds-163&#39; into ali2 
git stash apply --index stash@{0}
git stash drop --index stash@{0}

# directly use stash pop
git stash pop
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">Back to the unaltered HEAD</a>
</li>
<li>
<a href="#toc_1">Messed up local master, come back to the origin/master</a>
</li>
<li>
<a href="#toc_2">Reset one file</a>
</li>
<li>
<a href="#toc_3">Save change in one branch and apply it (partially apply it) to another branch</a>
</li>
<li>
<a href="#toc_4">Add only modified changes and ignore untracked (newly added) files</a>
</li>
<li>
<a href="#toc_5">Syntax of <code>.gitignore</code></a>
</li>
<li>
<a href="#toc_6">Untracked directory and file</a>
</li>
<li>
<a href="#toc_7">discard unstaged changes in Git</a>
</li>
<li>
<a href="#toc_8">Oh Shit, git</a>
</li>
<li>
<a href="#toc_9">Local unstaged change do not want to tracked and be committed</a>
</li>
<li>
<a href="#toc_10">git stash</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[bash]]></title>
    <link href="www.echo-ohce.com/14742423250478.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250478.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Using curly braces in Bash substitution</a>
</li>
<li>
<a href="#toc_1">nohup to a different file</a>
</li>
<li>
<a href="#toc_2">Append multiple lines to a file</a>
</li>
<li>
<a href="#toc_3">copy from bash shell to clipboard</a>
</li>
<li>
<a href="#toc_4">Hadoop hdfs get all except one folder (file)</a>
</li>
<li>
<a href="#toc_5">Hadoop hdfs / bash get matched folders</a>
</li>
<li>
<a href="#toc_6">check file encoding</a>
</li>
<li>
<a href="#toc_7">du without recursion</a>
</li>
<li>
<a href="#toc_8">checking running job and argument</a>
</li>
<li>
<a href="#toc_9">check the running job with complete arguments</a>
</li>
<li>
<a href="#toc_10">grep find negate</a>
</li>
<li>
<a href="#toc_11">grep find text in all files under a folder and all of its subfolders</a>
</li>
<li>
<a href="#toc_12">Keep the REPL across sessions</a>
</li>
<li>
<a href="#toc_13">Schedule Auto jobs on Mac using crontab</a>
</li>
<li>
<a href="#toc_14">Hadoop Yarn get the Aggregated Log for a job</a>
</li>
<li>
<a href="#toc_15">History search</a>
</li>
<li>
<a href="#toc_16">check what pid is listening to the port</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Using curly braces in Bash substitution</h1>

<p>It is always a good practice to use curly braces like <code>${foo}</code>, but there are following situations it is a must:</p>

<ul>
<li>confusing strings <code>${foo}bar</code> vs. <code>$foobar</code> in which <code>foobar</code> is a single parameter.</li>
<li>expanding arrays <code>${array[42]}</code></li>
<li>expanding positional parameters beyond 9 <code>$8 $9 ${10}</code></li>
</ul>

<p><code>{}</code> is called <em>brace expansion</em>, <code>${}</code> is called <em>variable expansion</em><br/>
<a href="http://stackoverflow.com/questions/8748831/when-do-we-need-curly-braces-in-variables-using-bash">Reference Post</a>  </p>

<h1 id="toc_1">nohup to a different file</h1>

<pre><code class="language-bash">nohup some_command &gt; nohup_file.out 2&gt;&amp;1 &amp;
</code></pre>

<p>in which <code>2&gt;&amp;1</code> means <font color='red'>redirect <code>stderr</code> to the same output as <code>stdout</code></font>.  </p>

<h1 id="toc_2">Append multiple lines to a file</h1>

<p>Using a <em>eoi</em> (end of input) symbol for multiple lines</p>

<pre><code class="language-bash">cat &lt;&lt;EOI &gt;&gt; file
multiple lines
input here
EOI
</code></pre>

<p>in which, the <code>&lt;&lt;EOI</code> is registering a special symbol marking the end of input, which should be in a line by itself; the <code>&gt;&gt; file</code> means it is <font color='red'>append</font> to the file.</p>

<h1 id="toc_3">copy from bash shell to clipboard</h1>

<p>On mac, it is super easy: <code>pbcopy</code> and <code>pbpaste</code>.  </p>

<pre><code class="language-bash">cat ~/.bashrc | pbcopy
</code></pre>

<p>After that command content of the <code>~/.bashrc</code> file will be available for pasting with <code>cmd+v</code> shortcut.</p>

<h1 id="toc_4">Hadoop hdfs get all except one folder (file)</h1>

<p>As noted before, Hadoop use its own <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html#globStatus%28org.apache.hadoop.fs.Path%29">glob</a>. to do the pattern matching for file name and paths.</p>

<pre><code class="language-bash">$ hls alibaba/ipstats/
Found 8 items
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:36 alibaba/ipstats/IP
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/acookie_day
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/avg_acookie_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/avg_umid_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_acookie_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_day
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_umid_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/umid_day  

$ hdfs dfs -get alibaba/ipstats/[^I]*
</code></pre>

<p>The command gets all folders except for <code>alibaba/ipstats/IP</code>  </p>

<h1 id="toc_5">Hadoop hdfs / bash get matched folders</h1>

<p>This works both on bash and hadoop hdfs</p>

<p>Below is the HDFS folder:</p>

<pre><code class="language-bash">Found 6 items
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/FP
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/TP
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/UNKNOWN
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:19 alibaba/graph/stats/cluster-size-histo
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:19 alibaba/graph/stats/coverage
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:45 alibaba/graph/stats/stats
</code></pre>

<p>Need get the folders of <code>cluster-size-histo</code>, <code>coverage</code> and <code>stats</code></p>

<pre><code class="language-bash">hdfs dfs -get alibaba/graph/stats/[cs]*
</code></pre>

<p>Now local folder:</p>

<pre><code class="language-bash">total 8
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 cluster-size-histo
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 coverage
-rw-rw-r-- 1 yu yu  9 Sep 30 19:28 FP_cnt
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 stats
-rw-rw-r-- 1 yu yu  7 Sep 30 19:29 TP_cnt
</code></pre>

<p>Need delete folders of <code>cluster-size-histo</code>, <code>coverage</code> and <code>stats</code>:</p>

<pre><code class="language-bash">rm -rf [cs]*
</code></pre>

<h1 id="toc_6">check file encoding</h1>

<pre><code class="language-bash">file -I filename
</code></pre>

<p>The output is in format of: <code>/Path/To/Filename: fileformat/filetype; charset=encoding</code>. For example:  </p>

<pre><code class="language-bash">file -I Base.pig
Base.pig: application/octet-stream; charset=binary
</code></pre>

<h1 id="toc_7">du without recursion</h1>

<p>only want to see the whole folder&#39;s disk usage, not the details of its sub-folders.  </p>

<pre><code class="language-bash">du -s -h *
</code></pre>

<p><code>-s</code>: no recursion<br/>
<code>-h</code>: human readable</p>

<h1 id="toc_8">checking running job and argument</h1>

<p>the following commands works well:</p>

<pre><code class="language-bash">top -U [username]
ps -fu [username] # My most used one
ps -efl | grep &lt;username&gt; # get all the arguments
ps -efl | grep &lt;command name&gt;  

ps -efl | egrep &#39;\s+yu\s+&#39;
</code></pre>

<p>This is another good example, used to check <code>mongo</code> instance running in the system:</p>

<pre><code class="language-bash">ps -ef | grep mongod | grep -v grep | wc -l | tr -d &#39; &#39;
</code></pre>

<p>Step-by-Step:</p>

<ul>
<li><p>The <strong><code>ps -ef | grep mongod</code></strong> part return all the running processes, that have any relation to the supplied string, i.e. <code>mongod</code>, e.g. have the string in the executable path, have the string in the username, etc.</p></li>
<li><p>When you run the previous command, the <code>grep mongod</code> also becomes a process containing the string <code>mongod</code> in the <code>COMMAND</code> column of <code>ps</code> output, so it will also appear in the output. For that reason you need to eliminate it by piping <strong><code>grep -v grep</code></strong>, which filters all the lines from the input that contain the string <code>grep</code>.</p></li>
<li><p>So now you have all possible lines that contain string <code>mongod</code> and are not the instances of <code>grep</code>. What to do? Count them, and do that with <strong><code>wc -l</code></strong>.</p></li>
<li><p><code>wc -l</code> output contains additional formatting, i.e. spaces, so just for the sake of the beauty, run <strong><code>tr -d &#39; &#39;</code></strong> to remove the redundant spaces.</p></li>
</ul>

<p>As a result you will get a single number, representing the number of processes you <code>grep</code>&#39;ed for.</p>

<h1 id="toc_9">check the running job with complete arguments</h1>

<ol>
<li><code>ps -fu username</code> \(\rightarrow\) get the pid for the running job that would like to find the arguments.</li>
<li><code>ps -fu username | grep #pid</code> using the pid obtained in last step to get the complete arguments in nice format</li>
</ol>

<h1 id="toc_10">grep find negate</h1>

<p>negative matching, i.e. match lines that do not contain pattern</p>

<pre><code class="language-bash">grep -v [pattern] 
# -v, --invert-match select non-matching lines
</code></pre>

<h1 id="toc_11">grep find text in all files under a folder and all of its subfolders</h1>

<pre><code class="language-bash">grep -r -l -F -n -i -I &quot;string&quot; /path
</code></pre>

<p>in which:<br/>
<code>-r</code> is for recursive<br/>
<code>-l</code> is for showing the file name only (stop reading the file as soon as the string is find)<br/>
<code>-F</code> is searching for literal &quot;fixed string&quot;, not regexp<br/>
<code>-n</code> is printing the line number<br/>
<code>-i</code> is case-insensitive<br/>
<code>-I</code> is ignore binary files<br/>
Also there are:<br/>
<code>--exclude-dir=dir</code> is useful for excluding directories like .svn and .git</p>

<h1 id="toc_12">Keep the REPL across sessions</h1>

<p>This is very useful when running a Spark REPL on a remote ssh cluster, after logging out and close the terminal, next time connect and logging in the ssh cluster, we can pick up the same REPL session!</p>

<pre><code class="language-bash"># before running the REPL
screen
# start the REPL
/home/xiaogu/repo/spark-1.6.1-bin-hadoop2.4/bin/spark-shell --queue datascience

# Do not leave the REPL but leave the screen
Ctrl-a d

# leave the ssh session

# re-log in the ssh session
screen -r
</code></pre>

<p><a href="http://www.tecmint.com/screen-command-examples-to-manage-linux-terminals/">Detailed post for the <code>screen</code> command</a></p>

<p><font color='salmon'><strong>Important Note</strong></font></p>

<p>After get into the screen, need source the <code>~/.bash_profile</code>, <code>~/.bashrc</code> to get the regular bash setting into effective. Also for iTerm2, need set the mouse to scroll up and down <a href="http://stackoverflow.com/questions/36594420/how-can-i-turn-off-scrolling-the-history-in-iterm2">Mouse setting for iTerm2</a> </p>

<h1 id="toc_13">Schedule Auto jobs on Mac using crontab</h1>

<p><a href="https://ole.michelsen.dk/blog/schedule-jobs-with-crontab-on-mac-osx.html">Schedule jobs with crontab on Mac OS X</a></p>

<p>This can be used to build and update personal stock repo.</p>

<h1 id="toc_14">Hadoop Yarn get the Aggregated Log for a job</h1>

<p>(Thanks Mingyang)</p>

<pre><code class="language-bash">yarn logs -applicationId application_1478897002704_67148 -appOwner yu  
</code></pre>

<h1 id="toc_15">History search</h1>

<p>I use very frequently <code>Ctrl-r</code> to get the command. However, not in the most efficient way. It should be:</p>

<ol>
<li><code>Ctrl-r</code></li>
<li>type in the start of the command</li>
<li>repeat pressing <code>Ctrl-r</code> to do the reverse search</li>
<li>use <code>Ctrl-s</code> to do the forward search with the partially typed in command</li>
</ol>

<p>In order to enable <code>Ctrl-s</code>, <a href="http://stackoverflow.com/a/36331088/4229125">the following setting need be configured in <code>~/.bashrc</code></a></p>

<pre><code class="language-bash"># Y.G. to enable Ctrl-S for the reverse command history search
stty -ixon
</code></pre>

<h1 id="toc_16">check what pid is listening to the port</h1>

<p><a href="http://unix.stackexchange.com/a/106572">from this post, and works for mac</a></p>

<pre><code class="language-bash">lsof -i :4444 # I used this to check whether my Selenium server has been shut down
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">Using curly braces in Bash substitution</a>
</li>
<li>
<a href="#toc_1">nohup to a different file</a>
</li>
<li>
<a href="#toc_2">Append multiple lines to a file</a>
</li>
<li>
<a href="#toc_3">copy from bash shell to clipboard</a>
</li>
<li>
<a href="#toc_4">Hadoop hdfs get all except one folder (file)</a>
</li>
<li>
<a href="#toc_5">Hadoop hdfs / bash get matched folders</a>
</li>
<li>
<a href="#toc_6">check file encoding</a>
</li>
<li>
<a href="#toc_7">du without recursion</a>
</li>
<li>
<a href="#toc_8">checking running job and argument</a>
</li>
<li>
<a href="#toc_9">check the running job with complete arguments</a>
</li>
<li>
<a href="#toc_10">grep find negate</a>
</li>
<li>
<a href="#toc_11">grep find text in all files under a folder and all of its subfolders</a>
</li>
<li>
<a href="#toc_12">Keep the REPL across sessions</a>
</li>
<li>
<a href="#toc_13">Schedule Auto jobs on Mac using crontab</a>
</li>
<li>
<a href="#toc_14">Hadoop Yarn get the Aggregated Log for a job</a>
</li>
<li>
<a href="#toc_15">History search</a>
</li>
<li>
<a href="#toc_16">check what pid is listening to the port</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala]]></title>
    <link href="www.echo-ohce.com/14767220898143.html"/>
    <updated>2016-10-17T09:34:49-07:00</updated>
    <id>www.echo-ohce.com/14767220898143.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Convert between regular collection and parallel collection</a>
</li>
<li>
<a href="#toc_1">function / method handler</a>
</li>
<li>
<a href="#toc_2">Get the type of variable in REPL</a>
</li>
<li>
<a href="#toc_3">Span method</a>
</li>
<li>
<a href="#toc_4">flatMap and Option</a>
</li>
<li>
<a href="#toc_5">Patter Matching with default</a>
</li>
<li>
<a href="#toc_6">Pattern matching in array assignment, variable name must start with lower case letter</a>
</li>
<li>
<a href="#toc_7">read and write to file</a>
</li>
<li>
<a href="#toc_8">string formatter</a>
</li>
<li>
<a href="#toc_9">regex option</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Convert between regular collection and parallel collection</h1>

<p>Using Seq as example. Use <code>.par</code> to convert from regular to parallel, and use <code>.seq</code> to convert back from parallel to regular.</p>

<p>Special note that <code>.toSeq</code>, <code>toMap</code> or <code>toSet</code> are converting between various collection but stay in the same parallel or regular domain.</p>

<p><a href="http://stackoverflow.com/a/12023096/4229125">stackoverflow post</a></p>

<h1 id="toc_1">function / method handler</h1>

<p>look at the following example:</p>

<pre><code class="language-scala">val test = Array(&quot;1.0&quot;, &quot;2.5&quot;, &quot;3.14159&quot;)

//Wrong
test map toDouble
// Because this is wrong
toDouble(test(2))
//correct
test map (_.toDouble)
// Because this is correct
test(2).toDouble

//Wrong
test map parseDouble
//correct
test map java.lang.Double.parseDouble
// Because this is correct
java.lang.Double.parseDouble(test(2))
</code></pre>

<h1 id="toc_2">Get the type of variable in REPL</h1>

<p><code>variableName.getClass.getSimpleName</code></p>

<h1 id="toc_3">Span method</h1>

<p><a href="https://www.garysieling.com/blog/scala-span-example">A good post</a><br/>
The span method lets you split a stream into two parts, by providing a function that detects the dividing line where you want the split to occur. What this doesn’t let you do is to sift through the stream and move each record into the a or b side – you can’t use this to separate even and odd numbers, for instance.</p>

<p>Example:</p>

<pre><code class="language-scala">var (a, b) = Stream.from(1).span(_ &lt; 10)
a: scala.collection.immutable.Stream[Int] = Stream(1, ?)
b: scala.collection.immutable.Stream[Int] = Stream(10, ?)
</code></pre>

<p>From this, it appears that it has actually run through 10 elements immediately, not waiting until you actually need one.</p>

<p>We can see it returns what we’d expect:</p>

<pre><code class="language-scala">scala&gt; a.take(10).toList
res26: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9)

scala&gt; b.take(10).toList
res27: List[Int] = List(10, 11, 12, 13, 14, 15, 16, 17, 18, 19)
</code></pre>

<p>To show that the even/odd filtering doesn’t work, see the example below. Since the first element is “1” (odd) it automatically goes to the “b” side, and you get an empty list for “a”.</p>

<pre><code class="language-scala">scala&gt; var (a, b) = Stream.from(1).span(_ % 2 == 0)
a: scala.collection.immutable.Stream[Int] = Stream()
b: scala.collection.immutable.Stream[Int] = Stream(1, ?)
</code></pre>

<h1 id="toc_4">flatMap and Option</h1>

<p><code>flatMap</code> on <code>Seq[Option]</code> effectively filter out the <code>None</code>s.<br/>
The keypoint of understanding Option is consider it as a Collection with 1 or 0 element. When using the <code>flatMap</code> it explode the inner collection and put all elements from the inner collection into the one big outer collection.</p>

<p><a href="http://danielwestheide.com/blog/2012/12/19/the-neophytes-guide-to-scala-part-5-the-option-type.html">A good reference post</a></p>

<h1 id="toc_5">Patter Matching with default</h1>

<pre><code class="language-scala">something match {
    case &quot;val&quot; =&gt; &quot;default&quot;
    case default =&gt; somefunction(default)
}
</code></pre>

<p>It is not a keyword, just an alias, so this will work as well:</p>

<pre><code class="language-scala">something match {
    case &quot;val&quot; =&gt; &quot;default&quot;
    case everythingElse =&gt; somefunction(everythingElse)
}
</code></pre>

<p>Or using <code>_</code></p>

<pre><code class="language-scala">something match {
    case &quot;val&quot; =&gt; &quot;default&quot;
    case _ =&gt; doSomething()
}
</code></pre>

<h1 id="toc_6">Pattern matching in array assignment, variable name must start with lower case letter</h1>

<p><a href="http://stackoverflow.com/a/8204231/4229125">stackoverflow post</a></p>

<p>If the variable name starts with capital letter, Scala treat it as a Constant</p>

<p>Wrong:</p>

<pre><code class="language-scala">val Array(trainData, CVData) = cleanedUserArtistData.randomSplit(Array(0.9, 0.1))
</code></pre>

<p>Correct:</p>

<pre><code class="language-scala">val Array(trainData, cvData) = cleanedUserArtistData.randomSplit(Array(0.9, 0.1))
</code></pre>

<h1 id="toc_7">read and write to file</h1>

<p>Scala doesn’t offer any special file writing capability, so fall back and use the Java <code>PrintWriter</code> or <code>FileWriter</code> approaches</p>

<pre><code class="language-scala">// PrintWriter
import java.io._
val pw = new PrintWriter(new File(&quot;hello.txt&quot; ))
pw.write(&quot;Hello, world&quot;)
pw.close

// FileWriter
val file = new File(canonicalFilename)
val bw = new BufferedWriter(new FileWriter(file))
bw.write(text)
bw.close()
</code></pre>

<p><a href="https://www.safaribooksonline.com/library/view/scala-cookbook/9781449340292/ch12s03.html">reference</a></p>

<p>Read in utilities can be found in package <code>scala.io.Source</code> (<a href="http://www.scala-lang.org/api/current/scala/io/Source.html">both the class and its companion object</a></p>

<h1 id="toc_8">string formatter</h1>

<p><a href="http://docs.scala-lang.org/overviews/core/string-interpolation.html">reference here</a></p>

<p>This is very useful feature, make the mixing of string and variables much easier, and also handling the escape characters well.</p>

<ul>
<li><code>s</code></li>
<li><code>f</code></li>
<li><code>raw</code></li>
</ul>

<h1 id="toc_9">regex option</h1>

<p><code>i</code> - case insensitive<br/>
<code>d</code> - only unix lines are recognized as end of line<br/>
<code>m</code> - enable multiline mode<br/>
<code>s</code> - <code>.</code> matches any characters including line end<br/>
<code>u</code> - Enables Unicode-aware case folding<br/>
<code>x</code> - Permits whitespace and comments in pattern</p>

<p>The way of specifying it like follow:<br/>
<code>&quot;&quot;&quot;(?i)the \w+?(?=\W)&quot;&quot;&quot;.r</code><br/>
The <code>&quot;&quot;&quot;</code> means raw string. No need to double the <code>\</code><br/>
and the <code>(?i)</code> at the beginning gives the option of case insensitive</p>

<p><a href="http://daily-scala.blogspot.com/2010/01/regular-expression-2-rest-regex-class.html">reference here</a><br/>
<a href="http://www.scala-lang.org/api/rc2/scala/util/matching/Regex.html">official document</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Convert between regular collection and parallel collection</a>
</li>
<li>
<a href="#toc_1">function / method handler</a>
</li>
<li>
<a href="#toc_2">Get the type of variable in REPL</a>
</li>
<li>
<a href="#toc_3">Span method</a>
</li>
<li>
<a href="#toc_4">flatMap and Option</a>
</li>
<li>
<a href="#toc_5">Patter Matching with default</a>
</li>
<li>
<a href="#toc_6">Pattern matching in array assignment, variable name must start with lower case letter</a>
</li>
<li>
<a href="#toc_7">read and write to file</a>
</li>
<li>
<a href="#toc_8">string formatter</a>
</li>
<li>
<a href="#toc_9">regex option</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[aws]]></title>
    <link href="www.echo-ohce.com/14836486252242.html"/>
    <updated>2017-01-05T12:37:05-08:00</updated>
    <id>www.echo-ohce.com/14836486252242.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">setting the credential and connect to ec2</a>
</li>
<li>
<a href="#toc_1">Deploy a jupyter notebook server on aws</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">setting the credential and connect to ec2</h1>

<ul>
<li>Need install <code>awscli</code> by:</li>
</ul>

<pre><code class="language-bash">conda install -c conda-forge awscli
</code></pre>

<ul>
<li>Then run:</li>
</ul>

<pre><code class="language-bash">aws configure
</code></pre>

<ul>
<li><p>Log in the management account of aws, and check the Access Key ID and Secret Access Key for the developer account for the configure setting</p></li>
<li><p>After setting up, <code>boto3</code> can connect to the aws ec2</p></li>
</ul>

<pre><code class="language-python">import boto3
region = &#39;us-west-2&#39;
instance_id = &#39;i-0fa00c2c21a729e6e&#39;
ec2 = boto3.resource(&#39;ec2&#39;, region_name = region)
i = ec2.Instance(id=instance_id)
i.start()
print i.state
print &#39;{} or by DNS name: {}&#39;.format(i.public_ip_address, i.public_dns_name)
i.stop()
</code></pre>

<h1 id="toc_1">Deploy a jupyter notebook server on aws</h1>

<ul>
<li><a href="http://yangjie.me/2015/08/26/Run-Jupyter-Notebook-Server-on-AWS-EC2/">reference post 1</a></li>
<li><a href="http://navoshta.com/aws-tensorflow/">set up jupyter and tensorflow on aws</a></li>
<li><a href="https://gist.github.com/dengemann/3245552bcdbf7f8f2293abb96c2348f4">Here is a security group script for ipython on aws</a></li>
</ul>

<hr/>

<ul>
<li>
<a href="#toc_0">setting the credential and connect to ec2</a>
</li>
<li>
<a href="#toc_1">Deploy a jupyter notebook server on aws</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[vim]]></title>
    <link href="www.echo-ohce.com/14742423250782.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250782.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">move (jump) cursor to previous location or previous files</a>
</li>
<li>
<a href="#toc_1">Quick move 101</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">move (jump) cursor to previous location or previous files</h1>

<table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>ma</code></td>
<td>set mark <font color='red'>a</font> at current cursor location</td>
</tr>
<tr>
<td><code>&#39;a</code> - quotemark</td>
<td>jump to line of mark <font color='red'>a</font> (first non-blank character in line)</td>
</tr>
<tr>
<td><code>&#39;a</code> - backtick</td>
<td>jump to position (line and column) of mark <font color='red'>a</font></td>
</tr>
<tr>
<td><code>&#39;&#39;</code> - 2 quotemark</td>
<td>jump to the head of the line of previous location</td>
</tr>
<tr>
<td><code>``</code> -2 backtick</td>
<td>jump to the previous location</td>
</tr>
<tr>
<td><code>:marks</code></td>
<td>list all current marks (including marks of <strong>other files</strong>)</td>
</tr>
<tr>
<td><code>:help mark-motions</code></td>
<td>help for mark related stuff</td>
</tr>
<tr>
<td><code>:help jump-motions</code></td>
<td>help for jump related stuff</td>
</tr>
</tbody>
</table>

<p>reference <a href="http://vim.wikia.com/wiki/Using_marks">using marks</a><br/>
reference <a href="http://stackoverflow.com/questions/5052079/move-cursor-to-its-last-position">stackoverflow</a></p>

<h1 id="toc_1">Quick move 101</h1>

<p><a href="http://usevim.com/2012/03/09/quick-movement/">Very good post</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">move (jump) cursor to previous location or previous files</a>
</li>
<li>
<a href="#toc_1">Quick move 101</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Analytics with Spark Reading Notes]]></title>
    <link href="www.echo-ohce.com/14770727250961.html"/>
    <updated>2016-10-21T10:58:45-07:00</updated>
    <id>www.echo-ohce.com/14770727250961.html</id>
    <content type="html"><![CDATA[
<p>This documents the reading notes of this book. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Souce Code Link:</a>
</li>
<li>
<a href="#toc_1">Chapter 1 and 2</a>
<ul>
<li>
<a href="#toc_2">Start REPL</a>
</li>
<li>
<a href="#toc_3">Quick tricks to use</a>
</li>
<li>
<a href="#toc_4">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_5">Note about the <code>getOrElse()</code> method in the above map:</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Chapter 3</a>
<ul>
<li>
<a href="#toc_7">Download dataset and put on HDFS and start REPL</a>
</li>
<li>
<a href="#toc_8">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_9">The use of mllib ALS</a>
</li>
<li>
<a href="#toc_10">Rating</a>
</li>
<li>
<a href="#toc_11">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_12">split into Training set and CV set and calculate the AUC</a>
</li>
</ul>
</li>
</ul>


<hr/>

<h1 id="toc_0">Souce Code Link:</h1>

<p><a href="https://github.com/sryza/aas/tree/1st-edition">github</a></p>

<h1 id="toc_1">Chapter 1 and 2</h1>

<h2 id="toc_2">Start REPL</h2>

<pre><code class="language-bash"># this works on hlab
spark-shell --master yarn-client

# this works on dblab (this one is already outdated)
/home/xiaogu/repo/spark-1.6.1-bin-hadoop2.4/bin/spark-shell --queue datascience

# this works on dblab for spark 2.1.0 (Scala 2.11.8, Java 1.7.0_45)
# the spark-shell is a shell command with necessary parameters for db
cd ~/spark_pg
spark-shell
</code></pre>

<h2 id="toc_3">Quick tricks to use</h2>

<ol>
<li>help: <code>:help</code></li>
<li>history / find the variable or function name: <code>:history</code> or <code>:h?</code></li>
<li>paste code: <code>:paste</code></li>
</ol>

<h2 id="toc_4">Inside REPL Follow along</h2>

<pre><code class="language-scala">val rawblocks = sc.textFile(&quot;linkage/*.csv&quot;)
rawblocks.first
val head = rawblocks.take(10)
head.length
head foreach println

def isHeader(line:String):Boolean =
    line.contains(&quot;id_1&quot;)

head filterNot isHeader foreach println
head filter (ele =&gt; ! isHeader(ele)) foreach println
head filter (!isHeader(_)) foreach println

// Note that spark RDD does not have filterNot method
val noheader = rawblocks filter (!isHeader(_))

def toDouble(token:String):Double = {
    if (token.equals(&quot;?&quot;)) Double.NaN;
    else token.toDouble;}
    
val line = head(5)

def parse(line:String):(Int, Int, Array[Double], Boolean) = {
    val tokens = line split (&#39;,&#39;);
    val id1 = tokens(0).toInt;
    val id2 = tokens(1).toInt;
    val scores:Array[Double] = tokens slice (2, 11) map toDouble;
    val matched = tokens(11).toBoolean;
    (id1, id2, scores, matched)}

val tup = parse(line)

case class MatchData(id1:Int, id2:Int, scores:Array[Double], matched:Boolean);

def parse(line:String):MatchData = {
    val tokens = line split (&#39;,&#39;);
    val id1 = tokens(0).toInt;
    val id2 = tokens(1).toInt;
    val scores:Array[Double] = tokens slice (2, 11) map toDouble;
    val matched = tokens(11).toBoolean;
    MatchData(id1, id2, scores, matched)}
    

val parsed = rawblocks filter (!isHeader(_)) map parse;
parsed.cache()

val mds = head filterNot isHeader map parse;

val grouped = mds.groupBy(_.matched);

val matchCounts = parsed.map(_.matched).countByValue()
val matchCountsSeq = matchCounts.toSeq

import java.lang.Double.isNaN

// Using the stats Action on RDD[Double]
val stats = for (i &lt;- 0 until 9) yield (parsed map 
                (_.scores(i)) filter (!isNaN(_)) stats)
                
// write and load the NAStatCounter.scala
:load spark_book/NAStatCounter.scala

// use foldLeft, zip, map to build the stat with missing values

val samp_m0 = rawblocks take 10
val samp_m1 = samp_m0 filterNot isHeader map parse

val sample = samp_m1.foldLeft(samp_m1.head.scores map (x =&gt; new NAStatCounter())) ((statCounterArry, record) =&gt; statCounterArry zip record.scores map {case (preStatCounter, newScore) =&gt; preStatCounter.add(newScore)})
// Couple things remember:
// 1. samp_m1.foldLeft()() is correct, do not use infix here, samp_m1 foldLeft () () is wrong!
// 2. map {case () =&gt;} is correct, do not use map(case ()=&gt;)
// 3. Using zip to zip up two arrays, that&#39;s how we get the corresponding foldings

// Note foldLeft is in Scala. In spark use aggregate. Need And the aggregate API is different than scala’s foldLeft. here is a good explaination:
http://cuipengfei.me/blog/2014/10/31/spark-fold-aggregate-why-not-foldleft/

val statsm = statsWithMissing(parsed filter (_.matched) map (_.scores))
val statsn = statsWithMissing(parsed filter (!_.matched) map (_.scores))

case class Scored(md : MatchData, score : Double)
val ct = parsed map (row =&gt; Scored(row, (Seq(2,5,6,7,8) map (idx =&gt; if (row.scores(idx).isNaN) 0 else row.scores(idx))).sum))
ct.cache()

val allTrue = (ct map (_.md.matched)).countByValue.getOrElse(true, 0).asInstanceOf[Number].doubleValue

val allFalse = (ct map (_.md.matched)).countByValue.getOrElse(false, 0).asInstanceOf[Number].doubleValue

// using different score cut threshold to divide true/false matches
import breeze.linalg._
val scoreRange = breeze.linalg.linspace( (ct map (_.score)).min, (ct map (_.score)).max, 5)

val truePositive = scoreRange map (cut =&gt; (ct filter (_.score&gt;cut) map (_.md.matched)).countByValue().getOrElse(true, 0).asInstanceOf[Number].doubleValue)

</code></pre>

<h2 id="toc_5">Note about the <code>getOrElse()</code> method in the above map:</h2>

<p>Because what&#39;s in the map (given by <code>countByValue()</code>) is of type <code>Long</code>, and the default value I gave is of type <code>Int</code>, the <code>getOrElse()</code> method returns the super type for both that is <code>AnyVal</code>.</p>

<p><code>AnyVal</code> has no easy way to cast, so far what I found seems the easiest way is to first convert to type <code>Number</code> then cast to <code>Int</code>. <font color='red'> It can not be convert to type <code>Int</code> directly, otherwise exception will be thrown. </font> </p>

<p>put it all together, for a given map:</p>

<pre><code class="language-scala">val value = map.getOrElse(key, 0).asInstanceOf[Number].doubleValue
</code></pre>

<h1 id="toc_6">Chapter 3</h1>

<h2 id="toc_7">Download dataset and put on HDFS and start REPL</h2>

<p><code>http://bit.ly/1KiJdOR</code></p>

<p>The default spark-shell on hlab has driver memory and executor memory of 512MB, which are not enough for the job that we are doing for this chapter. Use the following setting for setting the memory</p>

<p><a href="http://stackoverflow.com/questions/31463382/increase-java-heap-size-in-spark-on-yarn">stackoverflow post</a></p>

<p><a href="https://spark.apache.org/docs/1.2.0/configuration.html">official spark option list</a></p>

<pre><code class="language-bash"># this works on hlab
spark-shell --master yarn-client --driver-memory 2g --executor-memory 2g --conf spark.rdd.compress=true
</code></pre>

<h2 id="toc_8">Inside REPL Follow along</h2>

<pre><code class="language-scala">val rawUserArtistData = sc.textFile(&quot;spark_book/user_artist_data.txt&quot;, 20)

val overLimitCnt = (rawUserArtistData map (_.split(&quot;\\s+&quot;) map (_.toLong)) 
                    filter (arr =&gt; arr(0) &gt; java.lang.Integer.MAX_VALUE 
                                || arr(1)  &gt; java.lang.Integer.MAX_VALUE)).count()
//Note that in scala, we use .size, but for spark RDD, we need use .count(), because it is parallel partitioned.

val rawArtistData =sc.textFile(&quot;spark_book/artist_data.txt&quot;, 20)

//note the following is use char of single quote
val test = rawArtistData take 10
val line0 = test(0) span (_ != &#39;\t&#39;) match {case (id, name) =&gt; (id.toInt, name.trim)}

//Using the flatMap and Option and pattern match with default to handle the dirty data
val artistByID = rawArtistData flatMap (_ span (_ != &#39;\t&#39;) 
                                        match { 
                                            case(id, name) =&gt; {
                                                try {
                                                    Some((id.toInt, name.trim))
                                                } catch {
                                                    case e:NumberFormatException =&gt; None
                                                }}
                                            case anythingElse =&gt; None})
                                        
val rawArtistAlias = sc.textFile(&quot;spark_book/artist_alias.txt&quot;, 20)

//Just as before, using Option to catch exceptions
val artistAliasOption = (rawArtistAlias map (_ span (_ != &#39;\t&#39;) 
                                             match {case (id1, id2) 
                                                      =&gt; try { 
                                                          (Some(id1.trim.toInt, id2.trim.toInt)) } 
                                                         catch { 
                                                          case e: NumberFormatException=&gt;None};
                                                    case _ =&gt; None}))

//remove Options
val artistAliasRDD = artistAliasOption flatMap (x=&gt;x)

//collect as Map
val artistAlias = artistAliasRDD collectAsMap

// pay special attention to the pattern matching of Array
val formatedUserArtistData =(rawUserArtistData map (_ split(&quot;\\s+&quot;) match 
    { case Array(id1Str, id2Str, cntStr) =&gt; try {
                                                Some(Array(id1Str.trim.toInt,
                                                           id2Str.trim.toInt,
                                                           cntStr.trim.toInt))
                                                }
                                            catch {
                                                case e:Throwable =&gt; None
                                                };
      case _ =&gt; None })) flatMap (x=&gt;x)
// build the Rating class used in MLLib
import org.apache.spark.mllib.recommendation._
// broadcast to save resource and speed up, then clean the alias
val bArtistAlias = sc.broadcast(artistAlias)

// special note:
// 1. use the broadcasted thing, need refer to its .value.
// 2. pay attention to how to use the pattern matching inside map anonymous func
val cleanedUserArtistData = (formatedUserArtistData map 
     ({case Array(userId, artistId, cnt) =&gt; 
       Rating(userId, bArtistAlias.value.getOrElse(artistId, artistId), cnt)
       })
    ).cache()

val model = ALS.trainImplicit(cleanedUserArtistData, 10, 5, 0.01, 1.0)                                                 

// see what model looks like
model.userFeatures take 10 map ({case (userId, scoresArr) =&gt; userId.toString + &quot; : &quot; + scoresArr.mkString(&quot; , &quot;)}) foreach println

model.productFeatures take 10 map ({case (productId, scoresArr) =&gt; productId.toString + &quot; : &quot; + scoresArr.mkString(&quot; , &quot;)}) foreach println
                                 
</code></pre>

<h2 id="toc_9">The use of mllib ALS</h2>

<p><a href="http://spark.apache.org/docs/latest/ml-collaborative-filtering.html">Official documents</a></p>

<p>A very good page for explaining the &quot;explicit preference&quot; and &quot;implicit preference&quot; at <a href="http://predictionio.incubator.apache.org/templates/recommendation/training-with-implicit-preference/">PredicctionIO</a></p>

<h2 id="toc_10">Rating</h2>

<p>A more compact class to represent a rating than Tuple3[Int, Int, Double].<br/>
It is kind of like case class <code>Rating(user: Int, product: Int, rating: Double)</code><br/>
<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.recommendation.Rating">reference</a></p>

<h2 id="toc_11">Inside REPL Follow along</h2>

<pre><code class="language-scala">
// find the artist list for userID = 2093760
val targetUserID = 2093760
val joined = cleanedUserArtistData filter (_.user == targetUserID) map (rateEntry =&gt; (rateEntry.product, rateEntry.rating)) join (artistByID)

// Note that these are RDDs, do not forget the collect!
(joined map ({case (artistID, (cnt, name)) =&gt; name + &quot; : &quot; + cnt.toInt.toString}) collect) foreach println


// make recommendation
// Note that it is NOT RDD, but regular scala Array
val recommendations = model.recommendProducts(targetUserID, 5)

val recommendSet = (recommendations map (_.product)).toSet

val relatedArtistNameMap = (artistByID filter (record =&gt; recommendSet.contains(record._1))).collectAsMap

recommendations map (rateEntry =&gt; relatedArtistNameMap.getOrElse(rateEntry.product, &quot;NoName&quot;) + &quot; : &quot; + rateEntry.rating.toString) foreach println

</code></pre>

<h2 id="toc_12">split into Training set and CV set and calculate the AUC</h2>

<pre><code class="language-scala">val Array(trainData, cvData) = cleanedUserArtistData.randomSplit(Array(0.9, 0.1))
trainData.cache()
cvData.cache()
val model = ALS.trainImplicit(trainData, 10, 5, 0.01, 1.0)

// calculate the mean AUC for cvData
// 1. get the prediction on cvData using the model
val cvDataOfUserProduct = cvData map (record =&gt; (record.user, record.product))
// predict take RDD[tuple2]
val predictRating = model.predict(cvDataOfUserProduct)

// construct (user, wrongproduct)
val allProduct = (cleanedUserArtistData map (_.product) distinct).collect()
val allProductSize = allProduct.size
val bAllProduct = sc.broadcast(allProduct)
// using mapPartitions and broadcast to save memory, groupBy shuffles the RDD partitions, the shuffled RDD will have the same groupBy key records together inside one partition.

cvData groupBy (_.user) mapPartitions 
    {case (user, ratings) =&gt; {
        val correctProductSet = (ratings map (_.product)).collect().toSet
        var counter = 0
        val randomGen = new scala.util.Random()
        val wrongProductArr = new ArrayBuffer[Int]()
        while (counter &lt; correctProductSet.size) {
            val productCandidate = bAllProduct.value(randomGen.nextInt(allProductSize))
            if (!correctProductSet.contains(productCandidate)) {
                counter += 1
                wrongProductArr += productCandidate
            }
        }
    }
    wrongProductArr
}

        
            
            
                        



</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">Souce Code Link:</a>
</li>
<li>
<a href="#toc_1">Chapter 1 and 2</a>
<ul>
<li>
<a href="#toc_2">Start REPL</a>
</li>
<li>
<a href="#toc_3">Quick tricks to use</a>
</li>
<li>
<a href="#toc_4">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_5">Note about the <code>getOrElse()</code> method in the above map:</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Chapter 3</a>
<ul>
<li>
<a href="#toc_7">Download dataset and put on HDFS and start REPL</a>
</li>
<li>
<a href="#toc_8">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_9">The use of mllib ALS</a>
</li>
<li>
<a href="#toc_10">Rating</a>
</li>
<li>
<a href="#toc_11">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_12">split into Training set and CV set and calculate the AUC</a>
</li>
</ul>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning]]></title>
    <link href="www.echo-ohce.com/14858223421956.html"/>
    <updated>2017-01-30T16:25:42-08:00</updated>
    <id>www.echo-ohce.com/14858223421956.html</id>
    <content type="html"><![CDATA[
<p>This documents my personal understandings about bayesian, keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">An interesting illustration</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">An interesting illustration</h1>

<p><img src="media/14858223421956/14858223844745.jpg" alt=""/></p>

<p><a href="http://radimrehurek.com/data_science_python/">From this post</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[technical analysis]]></title>
    <link href="www.echo-ohce.com/14742423250761.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250761.html</id>
    <content type="html"><![CDATA[
<p>This documents my learning of stock technical analysis, keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">python packages to use</a>
</li>
<li>
<a href="#toc_1">python notebook extra setting</a>
</li>
<li>
<a href="#toc_2">Some good blogs</a>
</li>
<li>
<a href="#toc_3">Using tushare to get historical data</a>
</li>
<li>
<a href="#toc_4">Using pandas and yahoo is a better choice of getting daily data</a>
</li>
<li>
<a href="#toc_5">Build personal stock data repo</a>
</li>
<li>
<a href="#toc_6">这里有一篇用JAVA建立自己股票数据系统的文章</a>
</li>
<li>
<a href="#toc_7">使用通达信等本地股票软件的数据</a>
</li>
<li>
<a href="#toc_8">通达信的本地数据</a>
</li>
<li>
<a href="#toc_9">知乎上的一个关于获取数据的合集</a>
</li>
<li>
<a href="#toc_10">各种股票软件当地数据的数据结构</a>
</li>
<li>
<a href="#toc_11">实时行情数据接口</a>
</li>
<li>
<a href="#toc_12">前后，不复权</a>
</li>
<li>
<a href="#toc_13">Very useful Yahoo API to obtain data</a>
</li>
<li>
<a href="#toc_14">The schema of my MongoDB</a>
<ul>
<li>
<a href="#toc_15">collection per stock</a>
</li>
</ul>
</li>
</ul>


<hr/>

<h1 id="toc_0">python packages to use</h1>

<ol>
<li>zipline - for testing framework</li>
<li>tushare - for chinese stock market data</li>
<li><a href="https://github.com/mrjbq7/ta-lib">talib</a> - for indicators and functions. The official document is <a href="https://mrjbq7.github.io/ta-lib/doc_index.html">here</a></li>
</ol>

<h1 id="toc_1">python notebook extra setting</h1>

<pre><code class="language-python">import zipline as zp
import tushare as ts
ts.set_token(&#39;ef0f57f6c63416271c140912cb1e2bf58e1580f1aa5147efa19538085b21cf60&#39;) # use your own free api key
</code></pre>

<h1 id="toc_2">Some good blogs</h1>

<ol>
<li><a href="http://pythontrader.blogspot.com">使用Python語言開發「程式交易」系統</a></li>
<li><a href="http://tradingwithpython.blogspot.com">Trading with Python</a></li>
</ol>

<h1 id="toc_3">Using tushare to get historical data</h1>

<ul>
<li>A good <a href="http://www.360doc.com/content/16/0213/20/7249274_534357168.shtml">post</a> to quick check.</li>
<li>The stock codename in ts may be different than normal stock software.</li>
<li>to get the shanghai and shenzhen index use the following code example</li>
</ul>

<pre><code class="language-python">sh_index=ts.get_hist_data(&#39;sh&#39;)
sz_index=ts.get_hist_data(&#39;sz&#39;)
</code></pre>

<h1 id="toc_4">Using pandas and yahoo is a better choice of getting daily data</h1>

<p>tushare is not very stable, it is slow. It also has 3 years limitation.<br/><br/>
For using pandas and yahoo, these are resources:</p>

<ul>
<li><a href="https://yongle.gitbooks.io/data-science/content/note/4pandas/yahoo.html">A good post</a></li>
<li><a href="http://pandas-datareader.readthedocs.io/en/latest/">official docs</a></li>
<li><a href="http://jxyyjm.lofter.com/post/1d11b81d_7bbbc16">股票数据接口的小结</a></li>
</ul>

<p>如果要获取沪深股市信息股票名称格式如下：<br/>
上海：股票代码.SS<br/>
深圳：股票代码.SZ</p>

<pre><code class="language-python">import pandas_datareader.data as web

daily = web.DataReader(&#39;000001.SS&#39;, &#39;yahoo&#39;, &#39;2000-01-01&#39;, &#39;2016-01-01&#39;)
</code></pre>

<h1 id="toc_5">Build personal stock data repo</h1>

<p><a href="http://figurebelow.com/2013/10/18/creating-a-personal-stock-database-using-mongo-and-openshift/">A good post of building MongoDB stock data repo</a></p>

<pre><code>* One key component [YfinanceMongo](https://github.com/figurebelow/yfinanceMongo)
* The key wrapper [YFinanceFetcher](https://github.com/figurebelow/yfinancefetcher)
</code></pre>

<p>Using <code>crontab</code> to update this repo automatically.</p>

<h1 id="toc_6">这里有一篇用JAVA建立自己股票数据系统的文章</h1>

<p><a href="http://qkxue.net/info/126246/Python">写得不错，有时间应该看看</a></p>

<h1 id="toc_7">使用通达信等本地股票软件的数据</h1>

<p><a href="http://studygolang.com/topics/922">使用的go语言</a></p>

<h1 id="toc_8">通达信的本地数据</h1>

<ul>
<li>最全面的方式是在通达信中键入34打开数据导出，然后导出数据为CSV的格式。

<ul>
<li>这种方式可以选择前后复权或者不复权。</li>
<li>必须在windows才有这项功能</li>
<li>VMShareFolder的同步</li>
<li><a href="https://github.com/pywinauto/pywinauto">pywinauto</a>来实现自动操作</li>
</ul></li>
<li><p>可以在<code>安装目录\vipdoc\{sh,sz}\lday</code>下直接读取binary的日线数据文件，读取的格式是：<br/>
<code>int date; int open; int high; int low; int close; int amount; int vol; int reservation;</code><br/><br/>
<a href="http://www.doczj.com/doc/aa3ebcb827284b73f24250d8.html">参考文档 - 目录位置</a><br/>
<a href="http://www.doczj.com/doc/d1217c1ec281e53a5802ff71-3.html">参考文档 - 数据结构</a></p></li>
<li><p>通达信的复权有一个精确复权的选项，在这个<a href="https://read01.com/yzaQ04.html">帖子</a>里面有比较</p></li>
<li><p>这个blog有一套很全面的通达信目录文件结构的说明<br/>
<a href="http://www.programgo.com/article/80971445678/;jsessionid=1C3A149026552B218952C855376F14C6">目录文件结构说明</a></p></li>
<li><p>C code for read in the daily data</p></li>
</ul>

<pre><code class="language-c++">//  stock.cpp : Defines the entry point for the console application.
 //
 
#include  &quot; stdafx.h &quot; 
#include  &lt; stdio.h &gt; 
#include  &lt; conio.h &gt; 
#include  &lt; stdlib.h &gt; 

typedef  struct 
 {
    int date;
    int open;
    int high;
    int low;
    int close;
    int amount;
    int vol;
    int reservation;
}  StockData; 

StockData stockData;
 int  read_data(FILE  * );

 void  main()
 {
    FILE *fp;
    if((fp = fopen(&quot;E:/通达信/Vipdoc/sh/lday/sh600036.day&quot;,&quot;rb&quot;)) == NULL) // 打开招商银行日线
    { printf(&quot;Error: Can^t open sh600036.DAY ! &quot;);
    exit(0); }
    read_data(fp);
    fclose(fp);
    if(getch()==0) getch();
    exit(0);
} 

 int  read_data(FILE  * fp)
 {
    float fn;
    while (! feof(fp)) {
        fread(&amp;stockData,sizeof(StockData),1,fp);
        printf(&quot;%10lu &quot;,stockData.date);
        fn=float(stockData.open)/100;
        printf(&quot;%8.2f &quot;,fn);
        fn=float(stockData.high)/100;
        printf(&quot;%8.2f &quot;,fn);
        fn=float(stockData.low)/100;
        printf(&quot;%8.2f &quot;,fn);
        fn=float(stockData.close)/100;
        printf(&quot;%8.2f &quot;,fn);
        printf(&quot;%8lu &quot;,stockData.amount);
        fn=float(stockData.vol)/100;
        printf(&quot;%8.2f &quot;,fn);
    }
    printf(&quot; &quot;);
    return 0;
} 
</code></pre>

<p><a href="http://www.cnblogs.com/same/archive/2007/04/13/711466.html">另一个数据读取的源码</a></p>

<h1 id="toc_9">知乎上的一个关于获取数据的合集</h1>

<p><a href="https://www.zhihu.com/question/22145919">不错的一个总结</a></p>

<p>其中我准备实现的是:    </p>

<ul>
<li>yahoo</li>
<li>Sina</li>
<li>雪球</li>
<li>sohu <a href="http://q.stock.sohu.com/hisHq?code=zs_000001&amp;start=20000504&amp;end=20151215&amp;stat=1&amp;order=D&amp;period=d&amp;callback=historySearchHandler&amp;rt=jsonp&amp;r=0.8391495715053367&amp;0.9677250558488026">一个范例e</a></li>
<li>tushare</li>
</ul>

<h1 id="toc_10">各种股票软件当地数据的数据结构</h1>

<p><a href="http://so.anggang.com/?a=url&amp;k=c3732a2c&amp;u=aHR0cDovL3d3dy4zNjBkb2MuY29tL2NvbnRlbnQvMTAvMTIwMy8xNS8xMzcyNDA5Xzc0Njk1MDEzLnNodG1s&amp;t=5Yi45ZWG572R57uc5o6l5Y!j5YiG5p6Q5pa55rOVKOW3sue7j!WPr!S7peS4jumAmui!vuS@oeacjeWKoeWZqOihjOaDheaVsOaNrui@nuaOpSEpKOmhtSAuLi4=&amp;s=6YCa6L6!5L!h6IKh56Wo5pWw5o2u5o6l5Y!j">汇总的帖子</a></p>

<h1 id="toc_11">实时行情数据接口</h1>

<p><a href="http://so.anggang.com/?a=url&amp;k=c3732a2c&amp;u=aHR0cDovL3d3dy4zNjBkb2MuY29tL2NvbnRlbnQvMTAvMTIwMy8xNS8xMzcyNDA5Xzc0Njk1MDEzLnNodG1s&amp;t=5Yi45ZWG572R57uc5o6l5Y!j5YiG5p6Q5pa55rOVKOW3sue7j!WPr!S7peS4jumAmui!vuS@oeacjeWKoeWZqOihjOaDheaVsOaNrui@nuaOpSEpKOmhtSAuLi4=&amp;s=6YCa6L6!5L!h6IKh56Wo5pWw5o2u5o6l5Y!j">带附件下载的</a></p>

<h1 id="toc_12">前后，不复权</h1>

<p><a href="https://xueqiu.com/3488649239/62074848">这个帖子说得很详细</a><br/>
<img src="media/14742423250761/14828098074435.jpg" alt=""/></p>

<h1 id="toc_13">Very useful Yahoo API to obtain data</h1>

<p>Spent more time to dig into the Yahoo API. They actually have all data including dividend and split.</p>

<p>There are several different APIs:</p>

<hr/>

<ul>
<li>For getting the <font color='red'><strong>historical</strong></font> information: <code>http://ichart.yahoo.com/</code>

<ol>
<li><code>http://ichart.yahoo.com/x?</code></li>
<li><code>http://ichart.yahoo.com/table.csv?</code></li>
</ol></li>
</ul>

<p>The <code>x?</code> API can get access to dividend and split information, which is very important!<br/>
There is no complete API documentation that I can find. but a <a href="http://stackoverflow.com/a/6123886/4229125">sample code looks like this</a>:   </p>

<pre><code class="language-html">http://ichart.finance.yahoo.com/x?s=IBM&amp;a=00&amp;b=2&amp;c=1962&amp;d=04&amp;e=25&amp;f=2011&amp;g=v&amp;y=0&amp;z=30000
</code></pre>

<p>As far as I can tell, <code>g=v</code> means get the dividend and split, <code>y</code> and <code>z</code> seems not been used, and the starting date / ending date of <code>a~f</code> can be ignored if we need all information of the dividend and split. My example:</p>

<pre><code class="language-html">http://ichart.finance.yahoo.com/x?s=600859.SS&amp;g=v
</code></pre>

<p>specifying the date can cut the data&#39;s from/to:</p>

<pre><code class="language-html">http://ichart.finance.yahoo.com/x?s=600859.SS&amp;a=0&amp;b=1&amp;c=1998&amp;d=0&amp;e=1&amp;f=2013&amp;g=v
STARTDATE, 19980101
ENDDATE, 20130101
</code></pre>

<p>The <code>table.csv?</code> API is very similar, only difference is the lack of the <font color='red'><strong>split</strong></font> information.</p>

<p><a href="http://community.jaspersoft.com/wiki/building-custom-datasource-yahoo-finance-data">A good post for the meaning of the parameters and a Java implementation</a></p>

<p>A exmaple:</p>

<pre><code class="language-html">http://ichart.finance.yahoo.com/table.csv?s=YHOO&amp;d=0&amp;e=28&amp;f=2010&amp;g=d&amp;a=3&amp;b=12&amp;c=2009&amp;ignore=.csv
</code></pre>

<table>
<thead>
<tr>
<th style="text-align: center">symbol</th>
<th style="text-align: center">meaning</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: center">s</td>
<td style="text-align: center">Ticker symbol</td>
</tr>
<tr>
<td style="text-align: center">a</td>
<td style="text-align: center">The &quot;from month&quot; - 1</td>
</tr>
<tr>
<td style="text-align: center">b</td>
<td style="text-align: center">The &quot;from day&quot; (two digits)</td>
</tr>
<tr>
<td style="text-align: center">c</td>
<td style="text-align: center">The &quot;from year&quot;</td>
</tr>
<tr>
<td style="text-align: center">d</td>
<td style="text-align: center">The &quot;to month&quot; - 1</td>
</tr>
<tr>
<td style="text-align: center">e</td>
<td style="text-align: center">The &quot;to day&quot; (two digits)</td>
</tr>
<tr>
<td style="text-align: center">f</td>
<td style="text-align: center">The &quot;to year&quot;</td>
</tr>
<tr>
<td style="text-align: center">g</td>
<td style="text-align: center">d for day, m for month, y for yearly, v for dividend</td>
</tr>
</tbody>
</table>

<p>Javascript built-in function can be used in the url too, <a href="https://support.klipfolio.com/hc/en-us/articles/215546368-Use-Yahoo-Finance-as-a-data-source-">example</a></p>

<pre><code class="language-html">http://ichart.yahoo.com/x?s=600839.SS&amp;a={date.addMonths(-2).format(&#39;MM&#39;)}&amp;b={date.today.format(&#39;dd&#39;)}&amp;c={date.today.format(&#39;yyyy&#39;)}&amp;d={date.addMonths(-1).format(&#39;MM&#39;)}&amp;e={date.today.format(&#39;dd&#39;)}&amp;f={date.today.format(&#39;yyyy&#39;)}&amp;g=v&amp;ignore=.csv
</code></pre>

<hr/>

<ul>
<li>For getting the <font color='red'><strong>latest</strong></font> information: <code>http://finance.yahoo.com/d/</code><br/>

<ol>
<li><code>http://download.finance.yahoo.com/d/quotes.csv?</code></li>
<li><code>http://finance.yahoo.com/d/quotes.csv?</code></li>
</ol></li>
</ul>

<p><code>http://download.finance.yahoo.com/d/quotes.csv?s={SYMBOLS}&amp;f={DATA THAT WE WANT}</code><br/>
<code>f</code> is for the data specification characters. </p>

<ul>
<li><a href="http://wern-ancheta.com/blog/2015/04/05/getting-started-with-the-yahoo-finance-api/">Here is a good table for all the symbols</a> </li>
<li><a href="http://www.jarloo.com/yahoo_finance/">Another good one</a></li>
<li><a href="https://greenido.wordpress.com/2009/12/22/work-like-a-pro-with-yahoo-finance-hidden-api/">Another yet good one</a></li>
<li><a href="http://www.financialwisdomforum.org/gummy-stuff/Yahoo-data.htm">gummy-stuff probably is the first person put everything together</a></li>
</ul>

<hr/>

<p>There is another yahoo <code>YQL</code> API.   </p>

<p><a href="http://thesimplesynthesis.com/article/finance-apis#yahoo-yql-finance-api">This post</a> give some good information about this and a summary of other yahoo APIs</p>

<h1 id="toc_14">The schema of my MongoDB</h1>

<h2 id="toc_15">collection per stock</h2>

<p>collection name: &#39;600033&#39;</p>

<pre><code class="language-json">{&#39;Date&#39;: 2010-01-15, // sorted
 &#39;Open&#39;: [{&#39;value&#39;: 11.25,
           &#39;data_source&#39;: [1, 3, 4], // set
           &#39;vote&#39;: 3}, // sorted on &#39;vote&#39;
          {&#39;value&#39;: 13.24,
           &#39;data_source&#39;: [2],
           &#39;vote&#39;: 1},
          {&#39;value&#39;: 24,
           &#39;data_source&#39;: [5],
           &#39;vote&#39;: 1}]
 &#39;Close&#39;: [...]
 &#39;High&#39;: [...]
 &#39;Low&#39;: [...]
 &#39;Volume&#39;: [...]
 &#39;Amount&#39;: [...]
                
    
    
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">python packages to use</a>
</li>
<li>
<a href="#toc_1">python notebook extra setting</a>
</li>
<li>
<a href="#toc_2">Some good blogs</a>
</li>
<li>
<a href="#toc_3">Using tushare to get historical data</a>
</li>
<li>
<a href="#toc_4">Using pandas and yahoo is a better choice of getting daily data</a>
</li>
<li>
<a href="#toc_5">Build personal stock data repo</a>
</li>
<li>
<a href="#toc_6">这里有一篇用JAVA建立自己股票数据系统的文章</a>
</li>
<li>
<a href="#toc_7">使用通达信等本地股票软件的数据</a>
</li>
<li>
<a href="#toc_8">通达信的本地数据</a>
</li>
<li>
<a href="#toc_9">知乎上的一个关于获取数据的合集</a>
</li>
<li>
<a href="#toc_10">各种股票软件当地数据的数据结构</a>
</li>
<li>
<a href="#toc_11">实时行情数据接口</a>
</li>
<li>
<a href="#toc_12">前后，不复权</a>
</li>
<li>
<a href="#toc_13">Very useful Yahoo API to obtain data</a>
</li>
<li>
<a href="#toc_14">The schema of my MongoDB</a>
<ul>
<li>
<a href="#toc_15">collection per stock</a>
</li>
</ul>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mongodb]]></title>
    <link href="www.echo-ohce.com/14820465795454.html"/>
    <updated>2016-12-17T23:36:19-08:00</updated>
    <id>www.echo-ohce.com/14820465795454.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Pandas and Mongodb</a>
</li>
<li>
<a href="#toc_1">Auto Start MongoDB daemon (mongod) on the background</a>
</li>
<li>
<a href="#toc_2">Hitting the ulimit</a>
</li>
<li>
<a href="#toc_3">sort argument inside find_one method</a>
</li>
<li>
<a href="#toc_4">Indexing and sort in both directions</a>
</li>
<li>
<a href="#toc_5">mongodb locations</a>
</li>
<li>
<a href="#toc_6">check mongodb is running or not</a>
</li>
<li>
<a href="#toc_7">embedded documents</a>
</li>
<li>
<a href="#toc_8">delete a field</a>
</li>
<li>
<a href="#toc_9">sort</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Pandas and Mongodb</h1>

<ul>
<li>From pandas dataframe to mongodb: use <code>to_dict(&#39;records&#39;)</code>. <font color='red'>Must have the records parameters to get a list of dict, not a nested dict </font>. <a href="http://stackoverflow.com/a/33984472/4229125">Stackoverflow post</a></li>
</ul>

<pre><code class="language-python">db.timeline.insert_many(yahoo_df[::-1]      #yahoo_df is reversed order
                        .drop(&#39;Adj Close&#39;, axis=1)      #don&#39;t need the adj close
                        .to_dict(&#39;records&#39;))            #to mongodb
</code></pre>

<ul>
<li>From mongodb to pandas dataframe: use <code>list</code> of the pymonogo&#39;s <code>find</code>&#39;s cursor.  <a href="http://stackoverflow.com/a/17805626/4229125">Stackoverflow post</a></li>
</ul>

<pre><code class="language-python">from dateutil import parser as dateparser

start_date = dateparser.parse(&#39;2015-01-01&#39;)
end_date = dateparser.parse(&#39;2016-01-01&#39;)
mongo_df = pd.DataFrame(list(db.timeline.find({&#39;Date&#39;:{&#39;$lt&#39;:end_date, &#39;$gt&#39;:start_date}})))
</code></pre>

<ul>
<li>If it is pymongo&#39;s <code>find_one</code> output, it should convert to <code>pd.Series</code> instead of dataframe</li>
</ul>

<pre><code class="language-python">import pymongo
import pandas as pd
client = pymongo.MongoClient(host=&#39;localhost&#39;, port=27017)
db = client[&#39;personalTrader&#39;]
symbol_coll = db[&#39;symbol&#39;]
stock_info = pd.Series(symbol_coll.find_one({&#39;code&#39;:&#39;600000&#39;}))
</code></pre>

<h1 id="toc_1">Auto Start MongoDB daemon (mongod) on the background</h1>

<p><a href="http://serverfault.com/a/398366">Stackoverflow post</a></p>

<pre><code class="language-bash">mkdir -p ~/Library/LaunchAgents
cp /usr/local/Cellar/mongodb/3.2.10/homebrew.mxcl.mongodb.plist ~/Library/LaunchAgents
launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.mongodb.plist
</code></pre>

<p>This will launch mongod in the background now and every time you restart your computer.</p>

<p>The best way to get a program to run at startup on OS X is to create a LaunchDaemon (see <a href="http://developer.apple.com/mac/library/documentation/MacOSX/Conceptual/BPSystemStartup/Articles/LaunchOnDemandDaemons.html">Apple&#39;s docs</a>, and take a look at some of the Apple-supplied daemons in /System/Library/LaunchDaemons) and install it in /Library/LaunchDaemons. <a href="https://sourceforge.net/projects/lingon/files/">Lingon</a> can help you create the .plist file.<br/>
Note by Y.G <code>Lingon</code> is dead now.</p>

<p><font color='red'><strong>UPDATE</strong></font><br/>
A easier way is to use the app <code>MongoDB.prefPane</code> <a href="https://www.mongodb.com/blog/post/macosx-preferences-pane-for-mongodb">Official link</a><br/><br/>
After install this app, the <code>~/Library/LaunchAgents/homebrew.mxcl.mongodb.plist</code> will be replaced by <code>~/Library/LaunchAgents/com.remysaissy.mongodbprefspane.plist</code>. However the content is exactly the same.</p>

<h1 id="toc_2">Hitting the ulimit</h1>

<p><strong><a href="https://github.com/basho/basho_docs/issues/1402">A very good summary post about this</a></strong></p>

<p>mongod silently die, check the log at <code>/usr/local/var/log/mongodb/mongo.log</code> find the following error:</p>

<blockquote>
<p>2016-12-18T21:44:30.153-0800 W FTDC     [ftdc] Uncaught exception in &#39;FileNotOpen: Failed to open interim file /usr/local/var/mongodb/diagnostic.data/metrics.interim.temp&#39; <br/>
2016-12-18T21:44:34.930-0800 E STORAGE  [thread1] WiredTiger (24) [1482126274:929985][960:0x700000393000], file:WiredTiger.wt, WT_SESSION.checkpoint: /usr/local/var/mongodb/WiredTiger.turtle: handle-open: open: Too many open files<br/>
2016-12-18T21:44:34.934-0800 I -        [thread1] Fatal Assertion 28558</p>
</blockquote>

<p>This is because I have too many collections in the db and hitting the limit of open files.</p>

<ul>
<li><a href="https://docs.mongodb.com/manual/reference/ulimit/#ulimit">check the current limit</a><br/></li>
</ul>

<pre><code class="language-bash">ulimit -a  
</code></pre>

<ul>
<li><a href="https://docs.mongodb.com/manual/reference/ulimit/#recommended-ulimit-settings">recommended limit</a></li>
<li>temporary set a higher limit (which will expire when restart)</li>
</ul>

<pre><code class="language-bash">sudo launchctl limit maxfiles 65536 65536
sudo launchctl limit maxproc 2048 2048
ulimit -n 65536
sudo ulimit -u 2048
</code></pre>

<p>What I did in the end that works are exactly following the <a href="https://github.com/basho/basho_docs/issues/1402">aforementioned post</a></p>

<ol>
<li>Changed the limit from the <code>plist</code> file.</li>
<li>created the two file: <code>/Library/LaunchDaemons/limit.maxfiles.plist</code> and <code>/Library/LaunchDaemons/limit.maxproc.plis</code></li>
<li>Updated my <code>~/.bashrc</code></li>
<li>restart</li>
</ol>

<h1 id="toc_3">sort argument inside find_one method</h1>

<p>In <code>pymongo</code>, it is easy to make bug at the the sort syntax. The correct way of sort and take the first one. (The <code>[]</code> of the sort argument is a must, i.e. sort must be a <strong>list</strong> of <strong>tuple</strong>)</p>

<pre><code class="language-python">first_date = db[ncode].find_one(sort=[(&#39;Date_d&#39;, pymongo.ASCENDING)])[&#39;Date_d&#39;]
last_date = db[ncode].find_one(sort=[(&#39;Date_d&#39;, pymongo.DESCENDING)])[&#39;Date_d&#39;]
</code></pre>

<h1 id="toc_4">Indexing and sort in both directions</h1>

<p>MongoDB can scan an index from both directions. MongoDB may also traverse the index in either directions. As a result, for single-field indexes, ascending and descending indexes are interchangeable. This is not the case for compound indexes: in compound indexes, the direction of the sort order can have a greater impact on the results</p>

<h1 id="toc_5">mongodb locations</h1>

<p>After installing MongoDB with <code>Homebrew</code>:</p>

<ul>
<li>The databases are stored in the <code>/usr/local/var/mongodb/</code> directory</li>
<li>The <code>mongod.conf</code> file is here: <code>/usr/local/etc/mongod.conf</code></li>
<li>The mongo logs can be found at <code>/usr/local/var/log/mongodb/</code></li>
<li>The mongo binaries are here: <code>/usr/local/Cellar/mongodb/[version]/bin</code></li>
</ul>

<h1 id="toc_6">check mongodb is running or not</h1>

<pre><code class="language-bash">ps -ef | grep mongod | grep -v grep | wc -l | tr -d &#39; &#39;
</code></pre>

<h1 id="toc_7">embedded documents</h1>

<p>Filter or no filter,there is no difference on performance. See this <a href="http://stackoverflow.com/a/2140125/4229125">post</a>:  </p>

<blockquote>
<p>There&#39;s currently no way to filter on embedded docs in the way you&#39;re describing. Using the dot notation allows you to match on an embedded doc, but the entire document, parent and all, will still be returned. It&#39;s also possible to select which fields will be returned, but that doesn&#39;t really help your case, either.</p>
</blockquote>

<p>It doesn&#39;t make difference to filter it inside pandas DataFrame or filter it from the mongodb side.</p>

<p>However, I have some good working examples for the advanced usage of mongodb</p>

<ul>
<li>put a dataframe into a document as an array of embedded sub-documents</li>
</ul>

<pre><code class="language-python">code = &#39;600033&#39;
symbol_coll.update_one({&#39;code&#39;:code}, 
                       {&#39;$set&#39;: {&#39;fhpg&#39;: [dict(v.dropna()) for k, v in df.iterrows()]}}, 
                       upsert=False)
</code></pre>

<p>In the above code, <code>[dict(v.dropna()) for k, v in df.iterrows()]</code> is equivalent to <code>df.to_dict(&#39;records&#39;)</code>, but also skip the field with <code>nan</code> values when insert into the mongodb, i.e. make the database a little bit cleaner.</p>

<ul>
<li>get the dataframe out from an array of embedded sub-documents inside a document. Need use pipeline (listed two methods below)</li>
</ul>

<pre><code class="language-python">p_match = {&#39;$match&#39;:{&#39;code&#39;:code}}
p_project = {&#39;$project&#39;: {&#39;fhps&#39;:1, &#39;_id&#39;:0}}
pipeline=[p_match, p_project]

# note the use of [0], because the aggregate result is a list with one element
df = pd.DataFrame(list(symbol_coll.aggregate(pipeline))[0][&#39;fhps&#39;])

# use $unwind, do not need [0] anymore.
p_unwind = {&#39;$unwind&#39;: {&#39;path&#39;:&#39;$fhps&#39;}}
pipeline=[p_match, p_project, p_unwind]
df = pd.DataFrame([record[&#39;fhps&#39;] for record in symbol_coll.aggregate(pipeline)])
</code></pre>

<ul>
<li>get only one field (one column) out from an array of embedded sub-documents inside a document.</li>
</ul>

<p>The trick is inside the <code>$project</code>: using the <a href="https://docs.mongodb.com/manual/reference/glossary/#term-field-path">field path</a> syntax</p>

<pre><code class="language-python">p_match = {&#39;$match&#39;:{&#39;code&#39;:code}}
p_unwind = {&#39;$unwind&#39;: {&#39;path&#39;:&#39;$fhps&#39;}}
# here used field path syntax
p_project = {&#39;$project&#39;: {&#39;Date&#39;: &#39;$fhps.Date&#39;, &#39;_id&#39;:0}}
pipeline=[p_match, p_unwind, p_project]
df = pd.DataFrame(list(symbol_coll.aggregate(pipeline)))
</code></pre>

<ul>
<li>get some rows out from an array of embedded sub-documents inside a document.</li>
</ul>

<p>This is much easier to do once we have pandas dataframe. To do it in side mongo actually has no benefit. But document there as a future reference</p>

<p><strong>A pitfall : <code>$elemMatch</code> used with <code>$project</code> only return the <font color='red'>first</font> matching element.</strong> <a href="https://docs.mongodb.com/v3.2/reference/operator/projection/elemMatch/#definition">See reference here</a><br/>
<strong>The same thing holds true for <code>$</code>, i.e. the positional operator</strong> <a href="https://docs.mongodb.com/v3.2/reference/operator/projection/positional/#proj._S_">reference here</a></p>

<pre><code class="language-python"># only return the first match
symbol_coll.find_one({&#39;code&#39;:code, 
                      &#39;fhps.Date&#39;:{&#39;$lt&#39;:date_parser.parse(&#39;2012-06-25&#39;)}},
                     {&#39;fhps.$&#39;:1, &#39;_id&#39;:0})
# still only return the first match
symbol_coll.find_one({&#39;code&#39;: code}, {&#39;fhps&#39;: {&#39;$elemMatch&#39;: {&#39;Fh&#39;: {&#39;$gt&#39;: 0.01}}}, &#39;_id&#39;:0})    
</code></pre>

<p>Need use <code>$filter</code> method inside the <code>$project</code> pipeline stage (<font color='red'> <code>$filter</code> itself is not a pipeline stage</font>)</p>

<pre><code class="language-python">p_match = {&#39;$match&#39;:{&#39;code&#39;:code}}
p_project = {&#39;$project&#39;: {&#39;fhps&#39;:{&#39;$filter&#39;:{&#39;input&#39;:&#39;$fhps&#39;, 
                                             &#39;as&#39;:&#39;fhps_f&#39;,
                                             &#39;cond&#39;:{&#39;$gt&#39;: [&#39;$$fhps_f.Date&#39;, date_parser.parse(&#39;2012-06-25&#39;)]}}},
                          &#39;_id&#39;:0}}
pipeline=[p_match, p_project]
df = pd.DataFrame(list(symbol_coll.aggregate(pipeline))[0][&#39;fhps&#39;])
</code></pre>

<p><code>$$</code> in the above code is the syntax to refer a variable, as explained <a href="https://docs.mongodb.com/master/meta/aggregation-quick-reference/#field-path-and-system-variables">in the official document</a></p>

<h1 id="toc_8">delete a field</h1>

<p>I need remove <code>fhps</code> which is an array of embedded sub-documents:</p>

<pre><code class="language-python"># note the empty &#39;&#39; inside the unset
symbol_coll.update_one({&#39;code&#39;:code}, {&#39;$unset&#39;:{&#39;fhps&#39;:&#39;&#39;}})
</code></pre>

<h1 id="toc_9">sort</h1>

<hr/>

<ul>
<li>
<a href="#toc_0">Pandas and Mongodb</a>
</li>
<li>
<a href="#toc_1">Auto Start MongoDB daemon (mongod) on the background</a>
</li>
<li>
<a href="#toc_2">Hitting the ulimit</a>
</li>
<li>
<a href="#toc_3">sort argument inside find_one method</a>
</li>
<li>
<a href="#toc_4">Indexing and sort in both directions</a>
</li>
<li>
<a href="#toc_5">mongodb locations</a>
</li>
<li>
<a href="#toc_6">check mongodb is running or not</a>
</li>
<li>
<a href="#toc_7">embedded documents</a>
</li>
<li>
<a href="#toc_8">delete a field</a>
</li>
<li>
<a href="#toc_9">sort</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[intellij]]></title>
    <link href="www.echo-ohce.com/14765768181414.html"/>
    <updated>2016-10-15T17:13:38-07:00</updated>
    <id>www.echo-ohce.com/14765768181414.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, shortcuts I learned. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">List of most frequently used shortcut:</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">List of most frequently used shortcut:</h1>

<ul>
<li>Jump to previous location (navigate back):

<ul>
<li>cmd + [</li>
<li>cmd + ]</li>
</ul></li>
<li>Go to the previous/next occurrence of the word. This feature is called <code>Find Word at Caret</code> <a href="http://stackoverflow.com/a/18404014/4229125">stackoverflow post</a>. I set it as <code>Ctrl+,</code></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[setting]]></title>
    <link href="www.echo-ohce.com/14742423250740.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250740.html</id>
    <content type="html"><![CDATA[
<p>This documents various settings that I normally use. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">python notebook imports and setting</a>
</li>
<li>
<a href="#toc_1">Mac multiple desktop screen spaces setting.</a>
</li>
<li>
<a href="#toc_2">python conda virtual environment and jupyter notebook kernel</a>
</li>
<li>
<a href="#toc_3">Mac OS gets very busy on background process</a>
</li>
<li>
<a href="#toc_4">DNS and VPN connection issue</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">python notebook imports and setting</h1>

<pre><code class="language-python">import os
import pandas as pd
import numpy as np
import math
import plotly.plotly as py
import plotly.offline as po
import plotly.graph_objs as go
import plotly.tools as tls
import colorlover as cl
import cufflinks as cf
import subprocess
from ipywidgets import interact
import glob # for shell expansion
from IPython.display import display, HTML, Image
import matplotlib.pyplot as plt

%matplotlib inline

po.init_notebook_mode()
cf.set_config_file(theme=&#39;white&#39;)
pd.set_option(&#39;precision&#39;, 3) # printing precision
np.set_printoptions(precision=3) # printing precision 
cf.go_offline() # using cufflinks at offline mode
</code></pre>

<h1 id="toc_1">Mac multiple desktop screen spaces setting.</h1>

<p><a href="http://www.tweaking4all.com/os-tips-and-tricks/macosx-tips-and-tricks/mac-multiple-desktops-spaces/">Very good post</a><br/><br/>
Using <code>SteerMouse</code> to set the mouse shortcut.</p>

<h1 id="toc_2">python conda virtual environment and jupyter notebook kernel</h1>

<p>Need install a plugin <code>nb_conda_kernels</code></p>

<ol>
<li>switch to the desired virtual environment</li>
<li>install plugin</li>
<li>run jupyter notebook then pick the desired kernel from notebook</li>
</ol>

<pre><code class="language-bash">source activate environment_name
conda install -c conda-forge nb_conda_kernels
jupyter notebook

source deactive
</code></pre>

<p><a href="https://github.com/Anaconda-Platform/nb_conda_kernels">plugin&#39;s main page</a><br/>
<a href="http://stackoverflow.com/a/38880722">stackoverflow post</a></p>

<h1 id="toc_3">Mac OS gets very busy on background process</h1>

<ol>
<li>It is because of <code>optimal Layout</code></li>
<li>The app to check the system performance is <code>Activity Monitor</code></li>
<li>There is another process named <code>distnoted</code> taking a lot of CPU resources. This is Apple&#39;s kernel process, but it is safely to be removed. Using the script (saved as <code>install_checkdistnoted.sh</code> add a cron job to kill this process when it takes too much resources.<br/>
See the <a href="http://apple.stackexchange.com/a/234478">original post for this scirpt</a><br/></li>
</ol>

<h1 id="toc_4">DNS and VPN connection issue</h1>

<pre><code class="language-bash">alias reload=&quot;sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist;sudo launchctl load -w /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist&quot;
</code></pre>

<p>From obuli: this refreshes DNS entries somehow i have to do this every time I connect to VPN from my home</p>

<hr/>

<ul>
<li>
<a href="#toc_0">python notebook imports and setting</a>
</li>
<li>
<a href="#toc_1">Mac multiple desktop screen spaces setting.</a>
</li>
<li>
<a href="#toc_2">python conda virtual environment and jupyter notebook kernel</a>
</li>
<li>
<a href="#toc_3">Mac OS gets very busy on background process</a>
</li>
<li>
<a href="#toc_4">DNS and VPN connection issue</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[bayesian statistics]]></title>
    <link href="www.echo-ohce.com/14742423250523.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250523.html</id>
    <content type="html"><![CDATA[
<p>This documents my personal understandings about bayesian, keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Likelihood function is not a pdf</a>
</li>
<li>
<a href="#toc_1">MLE maximum Likelihood Estimation</a>
</li>
<li>
<a href="#toc_2">Gamma distribution in <code>scipy</code></a>
</li>
<li>
<a href="#toc_3">Exponential distribution</a>
</li>
<li>
<a href="#toc_4">Normal distribution</a>
<ul>
<li>
<a href="#toc_5">Known std of $sigma<sup>2$</sup></a>
</li>
<li>
<a href="#toc_6">Unknown std of $sigma<sup>2$</sup></a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">pdf, pmf, cdf, ppf explained</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Likelihood function is not a pdf</h1>

<p>(We are here only talking about continuous distribution case since I used pdf here.)<br/><br/>
Let the observation as \(O\) and the parameters as \(\theta\), the probability of seeing \(O\) given \(\theta\) is \(p(O|\theta)\). This is a pdf. It is a function of \(O\), and \(\int_{O}{p(O|\theta)\ dO}=1\), which means integrate through all possible observation \(O\), the sum of probability should be 1.<br/><br/>
The likelihood is the other way around as: given the observation \(O\) that we have seen, the likelihood that the parameter is value \(\theta\).<br/>
\[L(O|\theta)=p(O|\theta)\]<br/>
If we do an integration over all possible \(\theta\), \(\int_{\theta}{L(O|\theta)\ d\theta}\neq1\)<br/><br/>
<a href="http://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability">post on stackoverflow about the difference between likelihood and probability</a><br/>
<a href="http://stats.stackexchange.com/questions/31238/what-is-the-reason-that-a-likelihood-function-is-not-a-pdf">post on stackoverflow about likelihood and pdf</a>  </p>

<h1 id="toc_1">MLE maximum Likelihood Estimation</h1>

<p><a href="https://onlinecourses.science.psu.edu/stat504/node/28">Here</a> is a clear explained MLE course page. It gives Binomial distribution example and Poisson example</p>

<h1 id="toc_2">Gamma distribution in <code>scipy</code></h1>

<p><a href="https://en.wikipedia.org/wiki/Gamma_distribution">Wiki page</a></p>

<p>There are different ways to specify the gamma distribution, which all use two shape parameters:</p>

<ol>
<li><p>\(\alpha\) and \(\beta\) (\(\beta\) is called <code>rate</code>)<br/>
\[\Gamma(\alpha, \beta): pdf \rightarrow f(x; \alpha,\beta)=\frac{\beta^{\alpha}x^{\alpha-1}e^{-x\beta}}{\Gamma(\alpha)}\]<br/>
mean: \(\frac{\alpha}{\beta}\)<br/>
std: \(\frac{\sqrt{\alpha}}{\beta}\)</p></li>
<li><p>\(k\) and or \(\theta\) <br/>
\(\alpha=k\)<br/>
\(\theta=\frac{1}{\beta}\), and it is called <code>scale</code>.<br/>
\[\Gamma(k, \theta): pdf \rightarrow f(x; k,\theta)=\frac{1}{\theta}\frac{\frac{x}{\theta}^{k-1}e^{-\frac{x}{\theta}}}{\Gamma(k)}\]</p></li>
</ol>

<p>In <code>scipy.stats.gamma</code>, it uses the 2nd form, and the shape parameter <code>a</code> is \(k\), the <code>scale</code> is \(\theta\), for default value of <code>scale=1</code>, the pdf reduce to:<br/>
\[\Gamma(a, 1): pdf \rightarrow f(x; a,1)=\frac{x^{a-1}e^{-x}}{\Gamma(a)}\]<br/>
as shown on the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html#scipy.stats.gamma">scipy page</a></p>

<h1 id="toc_3">Exponential distribution</h1>

<p>like waiting for bus, earthquake, etc.  </p>

<p>We know it is exponential distribution: \( pdf(x) = \lambda e^{-\lambda x}\)<br/>
and we observer \(n\) observations: \(\tilde X ={x_1, x_2, ... x_n}\)<br/>
Now we want to get the estimate of \(\lambda\) (because its mean, i.e. expected value is \( \frac{1}{\lambda}\)).</p>

<ul>
<li><p>Likelihood function<br/>
\[ L(\tilde X | \lambda) = \Pi_{i=1}^{n}\lambda e^{-\lambda x_i} = \lambda^{n}e^{-\lambda \Sigma_{i=1}^{n}x_i}\] </p></li>
<li><p>Conjugate prior for \(\lambda\): </p>

<ul>
<li>Prior: \(pdf_{prior}(\lambda) = \Gamma(\alpha, \beta)\)</li>
<li>\(\alpha\) in the exponential-gamma model means the sample size (which is different than the poisson-gamma model, in which, \(\beta\) means the sample size)</li>
<li>mean and std are still the same for gamma distribution as before.</li>
<li>Posterior: \(pdf_{posterior}(\lambda) = \Gamma(\alpha+num_{newSample},  \beta+\Sigma(newSample))\)$</li>
</ul></li>
<li><p>If we have obtained the posterior and want to use the posterior to get the possibility density function pdf of random variable \(X\):<br/>
\[pdf_{posterior}(x) = \int pdf(x)* pdf_{posterior}(\lambda) d\lambda\]</p></li>
</ul>

<h1 id="toc_4">Normal distribution</h1>

<p>It mostly come from Central Limit Theorem.</p>

<h2 id="toc_5">Known std of \(\sigma^2\)</h2>

<p>We know it is normal distribution: \(pdf(x)=N(x|\mu,\sigma_0^2)\)<br/>
and we observer \(n\) observations: \(\tilde X ={x_1, x_2, ... x_n}\)<br/>
Now we want to get the estimate of \(\mu\)</p>

<ul>
<li><p>Likelihood function<br/>
\[ L(\tilde X | \mu ) = \Pi_{i=1}^{n}N(x_i|\mu,\sigma_0^2)\]</p></li>
<li><p>Conjugate prior for \(\mu\):</p>

<ul>
<li>Prior: \(pdf_{prior}(\mu)=N(\mu|m_0, s_0^2)\)</li>
<li>effective sample size of the prior: \(\frac{\sigma_0^2}{s_0^2}\)</li>
<li>Posterior: \(pdf_{posterior}(\mu)=N(\mu|(\frac{n}{n+\frac{\sigma_0^2}{s_0^2}}\bar x + \frac{\frac{\sigma_0^2}{s_0^2}}{n+\frac{\sigma_0^2}{s_0^2}}m_0 ), (\frac{1}{\frac{n}{\sigma_0^2}+\frac{1}{s_0^2}}))\)</li>
</ul></li>
<li><p>If we have obtained the posterior and want to use the posterior to get the possibility density function pdf of random variable \(X\):<br/>
\[pdf_{posterior}(x) = \int pdf(x)* pdf_{posterior}(\mu) d\mu\]<br/>
<strong>A quick way of calculation:</strong><br/>
\[\int N(x|\mu,\sigma_0^2)* N(\mu|m_0, s_0^2) d\mu=N(x|m_0, (s_0^2+\sigma_0^2))\]</p></li>
</ul>

<h2 id="toc_6">Unknown std of \(\sigma^2\)</h2>

<h1 id="toc_7">pdf, pmf, cdf, ppf explained</h1>

<ul>
<li>continuous distribution does not have point probabilities. pdf is the density function for a continuous distribution, and it is not defined for discrete distribution.
\[ P(x_1&lt;=X &lt;= x_2) = \int_{x_1}^{x_2} pdf(x)dx\]</li>
<li>pmf is the point probabilities of discrete distribution, and it is not defined for continuous distribution.
\[ P(X=x_1) = pmf(x_1)\]</li>
<li>cdf gives the accumulated probabilities, and it is defined for both the continuous and discrete distributions
\[ P(X&lt;=x_1) = cdf(x_1)\]</li>
<li>ppf is the inverse of cdf, and is mostly used to calculate the confidence interval. The result of ppf is <strong>not</strong> probability but the actual value of the random variable \(X\).
\[ P(X&lt;=ppf(\beta)) = \beta\]</li>
</ul>

<hr/>

<ul>
<li>
<a href="#toc_0">Likelihood function is not a pdf</a>
</li>
<li>
<a href="#toc_1">MLE maximum Likelihood Estimation</a>
</li>
<li>
<a href="#toc_2">Gamma distribution in <code>scipy</code></a>
</li>
<li>
<a href="#toc_3">Exponential distribution</a>
</li>
<li>
<a href="#toc_4">Normal distribution</a>
<ul>
<li>
<a href="#toc_5">Known std of $sigma<sup>2$</sup></a>
</li>
<li>
<a href="#toc_6">Unknown std of $sigma<sup>2$</sup></a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">pdf, pmf, cdf, ppf explained</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Graphviz Dot]]></title>
    <link href="www.echo-ohce.com/14773416332741.html"/>
    <updated>2016-10-24T13:40:33-07:00</updated>
    <id>www.echo-ohce.com/14773416332741.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Good Resources</a>
</li>
<li>
<a href="#toc_1">Fix the node locations after adding new edges</a>
</li>
<li>
<a href="#toc_2">Layout</a>
</li>
<li>
<a href="#toc_3">command line of dot</a>
</li>
<li>
<a href="#toc_4">Using of color</a>
</li>
<li>
<a href="#toc_5">Maybe better alternative</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Good Resources</h1>

<ol>
<li><a href="http://www.graphviz.org/doc/info/attrs.html">Node, Edge and Graph Attributes - API doc to find parameters to use</a></li>
<li><a href="http://www.tonyballantyne.com/graphs.html#sec-5-2">A good tutorial, very simple and easy</a></li>
<li><a href="http://graphs.grevian.org/reference">Another very quick reference site</a> Some graph showing on the website are not using the default <code>-Kdot</code> engine.</li>
</ol>

<h1 id="toc_1">Fix the node locations after adding new edges</h1>

<p>For example, I made a graph G1, and I want to show that after some steps new edges are added to the graph G2.</p>

<p>Use neato layout engine as following steps </p>

<ol>
<li>Using normal way to generate the graph G1 (normal .dot file)</li>
<li>Output the G1 in <code>gv</code> format<br/>
<code>dot -Tgv -Kdot tinygraphOriginal.dot -o original.gv</code> </li>
<li>Open the <code>gv</code> file and adding edges</li>
<li>Generate the new graph G2 by<br/>
<code>dot -Tpng -Kneato -n original.gv -o C3.png</code></li>
</ol>

<h1 id="toc_2">Layout</h1>

<p>Couple tricks to use: </p>

<ol>
<li>Using subgrap:
<code>clusterrank=local</code>
<code>subgraph cluster_***</code><br/></li>
<li>Using <code>[style=invis]</code> for invisible nodes and edges to align</li>
<li>Using <code>rank=same</code>, <code>group</code> etc</li>
<li>Using <code>weight</code> properties of edges</li>
</ol>

<h1 id="toc_3">command line of dot</h1>

<p>The <a href="http://graphviz.org/content/command-line-invocation">complete arguments</a> are listed here. But the most frequently used one is:</p>

<pre><code class="language-bash">dot -Tpng -Kdot tinygraphOriginal.dot -o original.png
</code></pre>

<h1 id="toc_4">Using of color</h1>

<ul>
<li>In the graph top level specify the color scheme</li>
</ul>

<pre><code class="language-dot">strict graph {
    label=&quot;Original C1 graph&quot;;
    nodesep=2.0
    splines=line;
    colorscheme=&quot;Brewer&quot;;
    ...
    
</code></pre>

<ul>
<li>use color and fontcolor, also use the form <code>/xxx/#</code> to specify the detailed palette and color.</li>
</ul>

<pre><code class="language-dot">1 -- 2 [ label=&quot;0.1&quot;, dir=both, color=&quot;/set312/1&quot;, fontcolor=&quot;/set312/1&quot;];
</code></pre>

<ul>
<li><a href="http://www.graphviz.org/doc/info/colors.html">List of available colors are here</a></li>
</ul>

<h1 id="toc_5">Maybe better alternative</h1>

<p>For small one time use graph, maybe use <code>Omnigraffle</code> is a much better choice.</p>

]]></content>
  </entry>
  
</feed>
