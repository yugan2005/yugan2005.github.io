<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Echo-ohcE]]></title>
  <link href="www.echo-ohce.com/atom.xml" rel="self"/>
  <link href="www.echo-ohce.com/"/>
  <updated>2016-12-19T00:22:10-08:00</updated>
  <id>www.echo-ohce.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.coderforart.com/">CoderForArt</generator>

  
  <entry>
    <title type="html"><![CDATA[python]]></title>
    <link href="www.echo-ohce.com/14742423250719.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250719.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">shell expansion insdie python with  <code>subprocess.call</code></a>
</li>
<li>
<a href="#toc_1">plotly using LaTex</a>
</li>
<li>
<a href="#toc_2">plotly cufflinks pandas dataframe customize the plot</a>
</li>
<li>
<a href="#toc_3">timezone</a>
</li>
<li>
<a href="#toc_4">From String to timestamp to epoch unix time</a>
</li>
<li>
<a href="#toc_5">datetime(python), pd.Timestamp(pandas), np.datetime64(numpy)</a>
</li>
<li>
<a href="#toc_6">To make a linspace of pd.Timestamp</a>
</li>
<li>
<a href="#toc_7">plotly tick setting: string format, number of ticks and location of ticks.</a>
</li>
<li>
<a href="#toc_8">plotly datetime type tick setting</a>
</li>
<li>
<a href="#toc_9">plotly change the padding size between subplots</a>
</li>
<li>
<a href="#toc_10">read in gz / gzip file</a>
</li>
<li>
<a href="#toc_11">plotly bar chart, use whether value is negative or positive to determine its color</a>
</li>
<li>
<a href="#toc_12">plotly subplot shared axis setting</a>
</li>
<li>
<a href="#toc_13">plotly remove missed dates from time series plot</a>
</li>
<li>
<a href="#toc_14">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</a>
</li>
<li>
<a href="#toc_15">Use <code>:</code> or <code>slice(None)</code></a>
</li>
<li>
<a href="#toc_16">String for pandas datetime period and frequency</a>
</li>
<li>
<a href="#toc_17">Python Library for Probabilistic Graphical Models</a>
</li>
<li>
<a href="#toc_18">Python blocking and non-blocking subprocess calls</a>
</li>
<li>
<a href="#toc_19">nohup python that includes a nohup shell script</a>
</li>
<li>
<a href="#toc_20">ipython jupyter notebook pandas showing options</a>
</li>
<li>
<a href="#toc_21">matplotlib style and colors</a>
</li>
<li>
<a href="#toc_22">matplotlib backend choices</a>
</li>
<li>
<a href="#toc_23">matplotlib with mpld3 <code>twinx</code> two side axes issue</a>
</li>
<li>
<a href="#toc_24">Conda install python package CondaHTTPError Error</a>
</li>
<li>
<a href="#toc_25">utf8 unicode in string formatter &amp; file read/write</a>
</li>
<li>
<a href="#toc_26">Use a relative path in a python module</a>
</li>
<li>
<a href="#toc_27">Mysterious Error pandas groupby error</a>
</li>
<li>
<a href="#toc_28">matplotlib add axes (subplot) on a made figure and sharex</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">shell expansion insdie python with  <code>subprocess.call</code></h1>

<p>For python older than 2.7 (like the default on on DB cluster, but we can change the shebang to specify 2.7), we can only use <code>subprocess.call()</code>.<br/><br/>
when using shell expansion with this command, need have <code>shell=True</code>. Better way to have shell expansion inside python is to use module <code>glob</code><br/><br/>
But the following works, because it is not the shell expansion but the <code>imagemagick</code> does the expansion.  </p>

<pre><code class="language-python">subprocess.call([&#39;convert&#39;, &#39;-delay&#39;, &#39;10&#39;, &#39;-loop&#39;, &#39;10&#39;, &#39;temp_1*.png&#39;, &#39;cluster_size_anim.gif&#39;])
</code></pre>

<h1 id="toc_1">plotly using LaTex</h1>

<p>It is not possible to use LaTex in offline mode. (don&#39;t waste time to figure out a way to do that anymore, maybe wait for newer update)<br/><br/>
It works when using the online mode.</p>

<h1 id="toc_2">plotly cufflinks pandas dataframe customize the plot</h1>

<ol>
<li>using the <code>asFigure=True</code> option to get the handle of the figure</li>
<li>using <code>update</code> method to update the layout python dictionary</li>
<li>using <code>po.iplot()</code> to plot the figure</li>
</ol>

<p>Example:  </p>

<pre><code class="language-python">figure = data.iplot(kind=&#39;scatter&#39;, fill=True, asFigure=True)
figure.layout.update(xaxis=dict(nticks=len(data.asfreq(&#39;M&#39;).index)))
po.iplot(figure)
</code></pre>

<h1 id="toc_3">timezone</h1>

<p>A classical scenario is get timestamp in UTC, want to convert to local timezone. Panda can do it for time series</p>

<pre><code class="language-python">ts2 = ts1.
        tz_localize(&#39;UTC&#39;).
        tz_convert(&#39;US/Pacific&#39;)
</code></pre>

<p>the <code>tz_localize(&#39;UTC&#39;)</code> make it timezone aware, the <code>tz_convert(&#39;US/Pacific&#39;)</code> convert it to local timezone.</p>

<h1 id="toc_4">From String to timestamp to epoch unix time</h1>

<p>Use <code>pd.Timestamp()</code> to convert string into pandas timestamp, then use <code>value</code> to epoch unix time in nanosecond</p>

<pre><code class="language-python">pd.Timestamp(&#39;2002-01-01&#39;).value/1e6
# this convert to epoch microseconds
</code></pre>

<h1 id="toc_5">datetime(python), pd.Timestamp(pandas), np.datetime64(numpy)</h1>

<p>At least between python and pandas is easy:</p>

<pre><code class="language-python">from dateutil import parser as dateparser
from datetime import datetime

dt = dateparser.parse(&#39;2015-01-01&#39;) # python datetime
dt2 = datetime(2015, 1, 1)          # Also python datetime
ts = pd.to_datetime(dt)             # pandas Timestamp
dt = ts.to_pydatetime()             # back to python datetime
</code></pre>

<p>A very good graph to show the converting relationship:</p>

<p><img src="media/14742423250719/14757939209171.jpg" alt=""/></p>

<h1 id="toc_6">To make a linspace of pd.Timestamp</h1>

<p>The <code>pd.date_range(start=None, end=None, periods=None, freq=&#39;D&#39;)</code> method will not work. This method is for different purpose.</p>

<pre><code class="language-python">rng = pd.date_range(&#39;1/1/2011&#39;, periods=72, freq=&#39;H&#39;)

# the same as pd.date_range(start, end, freq=&#39;D&#39;)
rng = pd.date_range(start, end)
</code></pre>

<p>To make the linespace of pd.Timestamp:<br/>
1. convert pd.Timestamp to long type<br/>
2. use np.linspace to make the array of long<br/>
3. convert back to pd.Timestamp array<br/>
4. Special note: If the original timestamp has timezone info, the step 3 need use different method to restore the timezone info</p>

<pre><code class="language-python"># the index of memory_df dataframe is of type pd.Timestamp
# the ticks_array is what we want linspace array of pd.Timestamp
# this is for cases without timezone information
ticks_array = pd.to_datetime(np.linspace(start=memory_df.index[0].value, 
                                         stop=memory_df.index[-1].value, 
                                         num=n_ticks, 
                                         endpoint=True))
                                         
# if need keep timezone info, to_datetime() method is not good anymore.
# for example the index of memory_df dataframe has timezone info
memory_df.index = memory_df.index.tz_localize(&#39;UTC&#39;).tz_convert(&#39;US/Pacific&#39;)

ticks_array = [pd.Timestamp(timetick, tz=memory_df.index.tz) for timetick in 
               (np.linspace(start=memory_df.index[0].value,
                            stop=memory_df.index[-1].value,
                            num=n_ticks, endpoint=True))]
</code></pre>

<h1 id="toc_7">plotly tick setting: string format, number of ticks and location of ticks.</h1>

<ul>
<li>The easiest way to set <code>nticks</code> and <code>tickformat</code>. Like the following example:</li>
</ul>

<pre><code class="language-python">plot_layout = go.Layout(xaxis=dict(nticks=5,
                                   tickformat=&#39;.2f&#39;))
</code></pre>

<ul>
<li>The hard way is to set <code>tickmode=&#39;array&#39;, ticktext=[...], tickvals=[...]</code>. </li>
</ul>

<h1 id="toc_8">plotly datetime type tick setting</h1>

<p>directly use <code>datetime</code> datatype and the plotly tick setting options have subtle bugs, especially when dealing with timezone awared data.</p>

<p>The workaround:<br/>
1. convert the index to string using <code>strftime</code> (these string need to have high enough resolution like <code>&#39;%H:%M:%S:%f&#39;</code>, otherwise, the plot will loose resolution because of having the same xaxis values)<br/>
2. generating <code>ticktext</code> and <code>tickvals</code>,specifing any format you like for <code>ticktext</code>.  </p>

<p>Look the below example:  </p>

<pre><code class="language-python">def visualization(job_id, basepath=&#39;.&#39;):
    memory_file = os.sep.join([basepath, &#39;memory_usage_&#39;+job_id+&#39;.csv&#39;])
    
    memory_df = pd.read_csv(memory_file)
    timestamp = pd.to_datetime(memory_df.timestamp, infer_datetime_format=True)
    memory_df = memory_df.set_index(timestamp).drop(&#39;timestamp&#39;, axis=1)
    memory_df=memory_df/float(1024*1024*1024*1024)
    memory_df.index = memory_df.index.tz_localize(&#39;UTC&#39;).tz_convert(&#39;US/Pacific&#39;)
    
    # change the index to string for plotting purpose
    memory_plot_df = memory_df.copy()
    memory_plot_df.index = pd.Series(data=[ts.strftime(&#39;%H:%M:%S:%f&#39;) for ts in memory_plot_df.index],name=&#39;time&#39;)
    mem_fig = memory_plot_df.iplot(asFigure=True)
    
    # set the xaxis ticks
    n_ticks = 10
    picked_tick_index = ((np.linspace(start=0, stop=len(memory_plot_df.index)-1, num=n_ticks, endpoint=True))
                        .astype(int))
    ticktext=[&#39;:&#39;.join(ts.split(&#39;:&#39;)[:2]) for ts in memory_plot_df.index[picked_tick_index]] # only take hour and mins
    tickvals=memory_plot_df.index[picked_tick_index]
    mem_fig.layout.update(title=&#39;Memory usage for &#39;+job_id,
                          yaxis=dict(title=&#39;TB&#39;),
                          xaxis=dict(title=&#39;time&#39;,
                                     tickmode=&#39;array&#39;,
                                     ticktext=ticktext,
                                     tickvals=tickvals))

    po.iplot(mem_fig, show_link=False)
</code></pre>

<h1 id="toc_9">plotly change the padding size between subplots</h1>

<p>Use <code>specs</code> and <code>vertical_spacing</code> or <code>horizontal_spacing</code> inside <code>tls.make_subplots()</code></p>

<p>Example:  </p>

<pre><code class="language-python">import plotly.tools as tls

figure = tls.make_subplots(rows=2, cols=1, shared_xaxes=True,
                           specs=[[{}], [{}]],
                           vertical_spacing=0.01)
</code></pre>

<h1 id="toc_10">read in gz / gzip file</h1>

<pre><code class="language-python">import gzip

with gzip.open(file_path, &#39;rb&#39;) as f:
    for line in f:
</code></pre>

<h1 id="toc_11">plotly bar chart, use whether value is negative or positive to determine its color</h1>

<pre><code class="language-python"># here sh_MACD.ch is a time series, with values can be positive or negative.
# want to generate the bar chart that is red if its value is positive, or green if negative

colors=(sh_MACD.ch&gt;0).map({True:&#39;red&#39;, False:&#39;green&#39;}).values
trace_macd = go.Bar(x=sh_MACD.index, y=sh_MACD.ch, name = &#39;MACD&#39;,
                    marker=dict(color=colors))
</code></pre>

<p>Key points:<br/>
* use map to convert logical array to the color literals<br/>
* use values to get np.ndarray not pd.Series (because plotly need np.ndarray)<br/>
* use <code>marker</code> filed</p>

<h1 id="toc_12">plotly subplot shared axis setting</h1>

<p>For example, if the subplot is 2 row 1 col, with shared x axis, the axis&#39; names are: [(1,1) x1,y1 ], [(2,1) x1,y2 ]. To set the axis properties in the <code>layout</code> dictionary, using <code>xaxis1</code> instead of <code>xaxis</code>.</p>

<p>The following example is from <code>01-Project/01-Study/PersonalProject/EDA.ipynb</code></p>

<pre><code class="language-python">overlay_plot[&#39;layout&#39;].update(xaxis1=dict(type=&#39;category&#39;))
</code></pre>

<h1 id="toc_13">plotly remove missed dates from time series plot</h1>

<p>change the xaxis&#39; type to <code>category</code>. (refer to the example above)</p>

<h1 id="toc_14">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</h1>

<pre><code>In [65]: df
Out[65]: 
                     A         B         C
first second                              
bar   one     0.895717  0.410835 -1.413681
      two     0.805244  0.813850  1.607920
baz   one    -1.206412  0.132003  1.024180
      two     2.565646 -0.827317  0.569605
foo   one     1.431256 -0.076467  0.875906
      two     1.340309 -1.187678 -2.211372
qux   one    -1.170299  1.130127  0.974466
      two    -0.226169 -1.436737 -2.006747
</code></pre>

<p>select all <code>two</code> data. Two methods:</p>

<ul>
<li>MultiIndex is tuple, using <code>loc</code> and tuple to select, and use <code>slice(None)</code> inside tuple to represent <code>:</code> that we normally use (because <code>:</code> is not recognized inside tuple). <font color='red'> The dataframe MultiIndex must be sorted first.</font></li>
</ul>

<pre><code class="language-python">df=df.sort_index()
selection = df.loc[(slice(None), &#39;two&#39;), :]
</code></pre>

<ul>
<li>Use <code>xs</code> method and use <code>level</code> argument to specify the level. (note that <code>xs</code> by default works on rows, if want to use it on columns, use the <code>axis=1</code> argument)<br/></li>
</ul>

<pre><code class="language-python">selection = df.xs(&#39;two&#39;, level=&#39;second&#39;)
</code></pre>

<h1 id="toc_15">Use <code>:</code> or <code>slice(None)</code></h1>

<p>They mean the same thing, normally in <code>[ ]</code>, we just use <code>:</code>, like <code>[5, :]</code><br/>
However, inside tuple, <code>:</code> is not recognized, we have to use <code>slice(None)</code> instead.</p>

<h1 id="toc_16">String for pandas datetime period and frequency</h1>

<p>Useful official documents</p>

<ul>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases">frequency strings</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offsets">Anchored offset frequency strings</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offset-semantics">Rolling forward or backward to snap on the anchors</a></li>
</ul>

<h1 id="toc_17">Python Library for Probabilistic Graphical Models</h1>

<p><a href="https://github.com/pgmpy/pgmpy">github link</a></p>

<h1 id="toc_18">Python blocking and non-blocking subprocess calls</h1>

<p><code>Popen</code> is nonblocking; <code>call</code> and <code>check_call</code> are blocking.<br/><br/>
<a href="http://stackoverflow.com/a/21936682/4229125">A good stackoverflow post explaining this</a></p>

<h1 id="toc_19">nohup python that includes a nohup shell script</h1>

<ul>
<li>when using nohup with python, the output will be buffered, in order to see the output in real time using <code>python -u</code> option to turn off the output buffering</li>
</ul>

<pre><code class="language-bash"># calling the nohup python script like this:
nohup python -u subprocess_test.py 1&gt;py_nohup.out 2&gt;&amp;1 &amp;
</code></pre>

<ul>
<li>Do not use <code>nohup</code> inside the python script for calling a shell script. Instead using the <code>Popen</code> with <code>shell=True</code> option, and redirects the stdout to a file.</li>
</ul>

<p>The following example is located at <code>Dropbox/01-Project/01-Study/playground/py_subprocess_call</code></p>

<p>The python script <code>subprocess_test.py</code>:</p>

<pre><code class="language-python">#!/usr/local/bin/python

from subprocess import Popen;
from time import sleep, strftime, localtime;

Popen(&#39;./keep_logging.sh 1&gt;shell_logging_out 2&gt;&amp;1&#39;, shell=True)

for i in range(20):
    print &#39;python logging {:d} time&#39;.format(i)
    print strftime(&quot;%a %b %d %H:%M:%S PDT %Y&quot;, localtime())
    sleep(3)
</code></pre>

<p>The called shell script inside python is <code>keep_logging.sh</code>: </p>

<pre><code class="language-bash">for ((c=1;c&lt;=20 ;c++ ))
do
        echo &quot;shell logging $c times&quot;
        date
        sleep 3
done
</code></pre>

<p>The command of running the python script (in turn running the shell script) is:</p>

<pre><code class="language-bash">nohup python -u subprocess_test.py 1&gt;py_nohup.out 2&gt;&amp;1 &amp;
</code></pre>

<p>Using <code>tail -f</code> for both logging file <code>shell_logging_out</code> and <code>py_nohup.out</code> shows they are <strong>non-blocking</strong> and the output are in real time.</p>

<h1 id="toc_20">ipython jupyter notebook pandas showing options</h1>

<p><a href="http://pandas.pydata.org/pandas-docs/stable/options.html">official documents</a></p>

<ul>
<li>set jupyter notebook show float in dataframe with 2 digits precision (globally)<br/>
<code>pd.options.display.float_format = &#39;{:.2f}&#39;.format</code> <a href="http://stackoverflow.com/a/31671975/4229125">stackoverflow reference</a></li>
<li>get/set number of dataframe rows to show in notebook<br/>
<code>pd.get_option(&quot;display.max_rows&quot;)</code><br/>
<code>pd.set_option(&quot;display.max_rows&quot;,999)</code><br/></li>
</ul>

<h1 id="toc_21">matplotlib style and colors</h1>

<ul>
<li><a href="https://tonysyu.github.io/raw_content/matplotlib-style-gallery/gallery.html">Here</a> is a good post for showing how all the matplotlib styles look like.</li>
<li>The recommended way of choosing color is through <code>seaborn</code>. <a href="http://seaborn.pydata.org/tutorial/color_palettes.html">The office document</a> which is very useful.</li>
<li>To temporarily change the matplotlib&#39;s default <a href="http://seaborn.pydata.org/tutorial/color_palettes.html#palette-contexts">color palette</a> and <a href="http://matplotlib.org/users/style_sheets.html#temporary-styling">style</a>, use <code>with</code>:<br/></li>
</ul>

<pre><code class="language-python">
with sns.color_palette(&quot;PuBuGn_d&quot;):
    # generate some plots with this color_palette
    
with plt.style.context((&#39;ggplot&#39;)):
    # generate some plots using the ggplot style  
    
</code></pre>

<h1 id="toc_22">matplotlib backend choices</h1>

<p>If a separate figure window needed (not the jupyter embedded window) use <code>Qt5Agg</code> as:</p>

<pre><code class="language-python">from matplotlib import pyplot as plt

plt.switch_backend(&#39;Qt5Agg&#39;)
</code></pre>

<p>If want to see the figure embedded inside the jupyter notebook, use magic line will be good enough:</p>

<pre><code class="language-python">%matplotlib notebook
</code></pre>

<h1 id="toc_23">matplotlib with mpld3 <code>twinx</code> two side axes issue</h1>

<p><code>mpld3</code> can correctly convert matplotlib twinx figs, plotly cannot</p>

<pre><code class="language-python">import matplotlib as mpl
from matplotlib import pyplot as plt
import mpld3
import plotly.tools as tls
import plotly.offline as po

%matplotlib notebook

fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot([1,3,2], &#39;b-&#39;)
ax2.plot([16, 4, 1], &#39;r:&#39;)
ax.set_ylabel(&#39;y1&#39;)
ax2.set_ylabel(&#39;y2&#39;)
ax2.patch.set_alpha(0.0) # This is the main trick for showing two axes
ax.set_xlabel(&#39;x&#39;)
fig.show()

###
mpld3.display(fig)

###
po.init_notebook_mode()
plotly_fig = tls.mpl_to_plotly(fig)
po.iplot(plotly_fig, show_link=False)
</code></pre>

<h1 id="toc_24">Conda install python package CondaHTTPError Error</h1>

<p>Mysterious error like:</p>

<blockquote>
<p>CondaHTTPError: HTTP 404 None<br/>
for url &lt;None&gt;<br/>
The remote server could not find the channel you requested.</p>
</blockquote>

<p>The reason is that some package installed previous changed the <code>~/.condarc</code> setting and added a channel, which <a href="http://conda.pydata.org/docs/config.html?highlight=channel%20url#channel-locations-channels">&quot;Listing channel locations in the .condarc file will override conda defaults&quot;</a></p>

<p>the modified <code>~/.condarc</code> that cause problem:  </p>

<pre><code class="language-bash">channels:
  - https://repo.continuum.io/pkgs/
  - defaults
</code></pre>

<p>just need delete the extra line, make it look like</p>

<pre><code class="language-bash">channels:
  - defaults
</code></pre>

<p>Problem solved.</p>

<h1 id="toc_25">utf8 unicode in string formatter &amp; file read/write</h1>

<ul>
<li>Use the <code>io</code> package for utf8 file read/write</li>
</ul>

<pre><code class="language-python">import io
with io.open(filename,&#39;r&#39;,encoding=&#39;utf8&#39;) as f:
    text = f.read()
# process Unicode text
with io.open(filename,&#39;w&#39;,encoding=&#39;utf8&#39;) as f:
    f.write(text)
</code></pre>

<p><a href="http://stackoverflow.com/a/19591815/4229125">Stackoverflow post</a></p>

<ul>
<li>Use <code>u</code> for both strings in string formatter</li>
</ul>

<pre><code class="language-python">s = u&#39;\u2265&#39;

# UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\u2265&#39; in position 0...
print &#39;{0}&#39;.format(s)

# Correct
print u&#39;{0}&#39;.format(s)
</code></pre>

<ul>
<li>To make the code itself utf8 (such as including chinese characters inside the code), add this line at the beginning of the python file</li>
</ul>

<pre><code class="language-python"># -*- coding: utf-8 -*-
</code></pre>

<h1 id="toc_26">Use a relative path in a python module</h1>

<p>Use case: need refer to my personal key which is located in another folder <code>resources</code></p>

<p>Store the absolute path to the module directory at the very beginning of the module:</p>

<pre><code class="language-python">package_directory = os.path.dirname(os.path.abspath(__file__))
with open(os.path.join(package_directory, &#39;../resources/tk.key&#39;), &#39;r&#39;) as f:
    token = f.readline()
    ts.set_token(token)
</code></pre>

<p><a href="http://stackoverflow.com/a/4187345/4229125">stackoverflow post</a></p>

<p>The same trick can be used for import a relative path module. <a href="http://stackoverflow.com/a/7506029/4229125">This neat trick is from stackoverflow here</a></p>

<p>At the head of the python file:</p>

<pre><code class="language-python"># -*- coding: utf-8 -*-
import sys, os

sys.path.append(os.path.join(os.path.dirname(__file__), &#39;..&#39;))

import pymongo
from indicators import commonIndicator
from Const import Const
import fromTDX
</code></pre>

<h1 id="toc_27">Mysterious Error pandas groupby error</h1>

<p>For some version of Pandas actually this error will not occur. (pandas v.18.1 will have error, but v.19 has no problem)  </p>

<ul>
<li>Symptom:</li>
</ul>

<blockquote>
<p>lib/python2.7/site-packages/pandas/core/groupby.pyc in reset_identity(values)<br/>
AttributeError: &#39;NoneType&#39; object has no attribute &#39;_get_axis&#39;</p>
</blockquote>

<ul>
<li><p>Cause:<br/><br/>
The apply function return <code>None</code>. When groupby then apply this func, this will cause issue.<br/><br/>
<a href="http://stackoverflow.com/a/20225276/4229125">A similar discussion on stackoverflow</a></p></li>
<li><p>The fix:</p></li>
</ul>

<p>The apply func should return empty <code>pd.DataFrame()</code> or empty <code>pd.Series()</code>.   </p>

<p>Wrong:</p>

<pre><code class="language-python">def get_buy_in_grp(self, grp):
   cond_1_m0 = (grp[&#39;MA_2_&#39; + self.base_period] &gt; grp[&#39;MA_5_&#39; + self.base_period])
   cond_1 = cond_1_m0 &amp; (~cond_1_m0.shift().astype(bool))
   cond_2 = (grp[&#39;MA_5_&#39; + self.base_period] &gt;= self.MA5_p * grp[&#39;MA_5_&#39; + self.base_period].shift())
   cond = cond_1 &amp; cond_2

   if ~(cond.any()):
       return
   return grp[cond]
</code></pre>

<p>Correct:</p>

<pre><code class="language-python">def get_buy_in_grp(self, grp):
   cond_1_m0 = (grp[&#39;MA_2_&#39; + self.base_period] &gt; grp[&#39;MA_5_&#39; + self.base_period])
   cond_1 = cond_1_m0 &amp; (~cond_1_m0.shift().astype(bool))
   cond_2 = (grp[&#39;MA_5_&#39; + self.base_period] &gt;= self.MA5_p * grp[&#39;MA_5_&#39; + self.base_period].shift())
   cond = cond_1 &amp; cond_2

   if ~(cond.any()):
       return pd.DataFrame() # return None may cause issues for the groupby().apply() action
   return grp[cond]     
</code></pre>

<h1 id="toc_28">matplotlib add axes (subplot) on a made figure and sharex</h1>

<p>Use the <code>fig.add_axes()</code> method <a href="http://matplotlib.org/api/figure_api.html#matplotlib.figure.Figure.add_axes">official document</a></p>

<p>To sharex, just <code>sharex</code> parameter in the method. Note that if <code>adjustable</code> parameter is set, in order to use <code>sharex</code>, <code>adjustable = ‘datalim’</code></p>

<pre><code class="language-python">
# the fig and ax_close etc are returned by another method

ax_close_pos = ax_close.get_position()
ax_macd_pos = [ax_close_pos.x0, ax_close_pos.y0, ax_close_pos.width, ax_close_pos.height/5.0]
ax_macd = fig.add_axes(ax_macd_pos, frameon=True, sharex=ax_close)
macd_df = (test_record.df[[col_name for col_name in test_record.df.columns
                           if col_name[-1]==&#39;m&#39;]]
           .drop_duplicates()
           .set_index(&#39;Date_m&#39;))
ax_macd.bar(macd_df[macd_df.MACD_m&gt;0].index.values, 
            macd_df[macd_df.MACD_m&gt;0].MACD_m.values,
            width=15, color=&#39;salmon&#39;)
ax_macd.bar(macd_df[macd_df.MACD_m&lt;0].index.values, 
            macd_df[macd_df.MACD_m&lt;0].MACD_m.values,
            width=15, color=&#39;steelblue&#39;)
ax_macd.grid(True)
mpld3.display()
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">shell expansion insdie python with  <code>subprocess.call</code></a>
</li>
<li>
<a href="#toc_1">plotly using LaTex</a>
</li>
<li>
<a href="#toc_2">plotly cufflinks pandas dataframe customize the plot</a>
</li>
<li>
<a href="#toc_3">timezone</a>
</li>
<li>
<a href="#toc_4">From String to timestamp to epoch unix time</a>
</li>
<li>
<a href="#toc_5">datetime(python), pd.Timestamp(pandas), np.datetime64(numpy)</a>
</li>
<li>
<a href="#toc_6">To make a linspace of pd.Timestamp</a>
</li>
<li>
<a href="#toc_7">plotly tick setting: string format, number of ticks and location of ticks.</a>
</li>
<li>
<a href="#toc_8">plotly datetime type tick setting</a>
</li>
<li>
<a href="#toc_9">plotly change the padding size between subplots</a>
</li>
<li>
<a href="#toc_10">read in gz / gzip file</a>
</li>
<li>
<a href="#toc_11">plotly bar chart, use whether value is negative or positive to determine its color</a>
</li>
<li>
<a href="#toc_12">plotly subplot shared axis setting</a>
</li>
<li>
<a href="#toc_13">plotly remove missed dates from time series plot</a>
</li>
<li>
<a href="#toc_14">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</a>
</li>
<li>
<a href="#toc_15">Use <code>:</code> or <code>slice(None)</code></a>
</li>
<li>
<a href="#toc_16">String for pandas datetime period and frequency</a>
</li>
<li>
<a href="#toc_17">Python Library for Probabilistic Graphical Models</a>
</li>
<li>
<a href="#toc_18">Python blocking and non-blocking subprocess calls</a>
</li>
<li>
<a href="#toc_19">nohup python that includes a nohup shell script</a>
</li>
<li>
<a href="#toc_20">ipython jupyter notebook pandas showing options</a>
</li>
<li>
<a href="#toc_21">matplotlib style and colors</a>
</li>
<li>
<a href="#toc_22">matplotlib backend choices</a>
</li>
<li>
<a href="#toc_23">matplotlib with mpld3 <code>twinx</code> two side axes issue</a>
</li>
<li>
<a href="#toc_24">Conda install python package CondaHTTPError Error</a>
</li>
<li>
<a href="#toc_25">utf8 unicode in string formatter &amp; file read/write</a>
</li>
<li>
<a href="#toc_26">Use a relative path in a python module</a>
</li>
<li>
<a href="#toc_27">Mysterious Error pandas groupby error</a>
</li>
<li>
<a href="#toc_28">matplotlib add axes (subplot) on a made figure and sharex</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[git]]></title>
    <link href="www.echo-ohce.com/14742423250638.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250638.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Back to the unaltered HEAD</a>
</li>
<li>
<a href="#toc_1">Messed up local master, come back to the origin/master</a>
</li>
<li>
<a href="#toc_2">Reset one file</a>
</li>
<li>
<a href="#toc_3">Save change in one branch and apply it (partially apply it) to another branch</a>
</li>
<li>
<a href="#toc_4">Add only modified changes and ignore untracked (newly added) files</a>
</li>
<li>
<a href="#toc_5">Syntax of <code>.gitignore</code></a>
</li>
<li>
<a href="#toc_6">Untracked directory and file</a>
</li>
<li>
<a href="#toc_7">discard unstaged changes in Git</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Back to the unaltered HEAD</h1>

<p>Run these two in order:  </p>

<pre><code class="language-bash">git reset --hard HEAD
git clean -df
</code></pre>

<p><font color='red'><strong>Be careful of using <code>git clean -df</code>, it will delete all the test cases</strong></font></p>

<h1 id="toc_1">Messed up local master, come back to the origin/master</h1>

<pre><code class="language-bash">git checkout -B master origin/master
</code></pre>

<p>If <code>-B</code> is given, the new branch is created if it doesn’t exist; otherwise, it is reset.  </p>

<h1 id="toc_2">Reset one file</h1>

<pre><code class="language-bash">git checkout HEAD -- my-file.txt
</code></pre>

<p><a href="http://stackoverflow.com/questions/7147270/hard-reset-of-a-single-file">Reference post1</a><br/><br/>
<a href="http://stackoverflow.com/questions/6561142/difference-between-git-checkout-filename-and-git-checkout-filename/6561160#6561160">Reference post2</a></p>

<h1 id="toc_3">Save change in one branch and apply it (partially apply it) to another branch</h1>

<pre><code class="language-bash">git stash
git checkout branch2
git stash list       # to check the various stash made in different branch
git stash pop        # pop out the last stash, and it will be deleted from the stash
git stash apply stash@{0}    # to select the stash and apply to branch2, the stash still saved in the list
git stash -u   # to stash currenlty untracked (newly added) files
</code></pre>

<h1 id="toc_4">Add only modified changes and ignore untracked (newly added) files</h1>

<pre><code class="language-bash">git add .   # stage all changed / untracked (newly added), but not includes deleted
git add -u  # stage all changed / deleted, but not untracked (newly added)
git add -A  # doing both of above
</code></pre>

<h1 id="toc_5">Syntax of <code>.gitignore</code></h1>

<pre><code class="language-bash">**/foo # matches file or directory &quot;foo&quot; anywhere
foo # the same as above  
**/foo/bar # matches file or   directory &quot;bar&quot; anywhere that is directly under directory &quot;foo&quot;  
abc/** # matches all files inside directory &quot;abc&quot;, infinite depth  
abc/* # matches all files inside directory &quot;abc&quot;, but not its sub-directories.  
a/**/b # match 0 or more directories in between  
a/**/b # matches &quot;a/b&quot;, &quot;a/x/b&quot;, &quot;a/x/y/b&quot; and so on.
</code></pre>

<h1 id="toc_6">Untracked directory and file</h1>

<ol>
<li><p>File previously tracked (in git repository) need remove it from the repo but keep it at local. <em>This is called as delete the cache</em><br/><br/>
<code>git rm -r -f --cached mydirectory/myfile</code><br/><br/>
the <code>-r</code> is recursive, the <code>-f</code> is force (in case, this tracked file has uncommitted change).  </p></li>
<li><p>Only change the local repo&#39;s <code>.gitignore</code> without change the remote repo&#39;s tracking behavior. Add ignore into <code>.git/info/exclude</code> instead of the <code>.gitignore</code> file at root.<br/><br/>
<a href="http://stackoverflow.com/questions/767147/ignore-the-gitignore-file-itself">reference post</a></p></li>
</ol>

<h1 id="toc_7">discard unstaged changes in Git</h1>

<pre><code class="language-bash">git checkout -- .
</code></pre>

<p>Make sure to include the last <code>.</code></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Back to the unaltered HEAD</a>
</li>
<li>
<a href="#toc_1">Messed up local master, come back to the origin/master</a>
</li>
<li>
<a href="#toc_2">Reset one file</a>
</li>
<li>
<a href="#toc_3">Save change in one branch and apply it (partially apply it) to another branch</a>
</li>
<li>
<a href="#toc_4">Add only modified changes and ignore untracked (newly added) files</a>
</li>
<li>
<a href="#toc_5">Syntax of <code>.gitignore</code></a>
</li>
<li>
<a href="#toc_6">Untracked directory and file</a>
</li>
<li>
<a href="#toc_7">discard unstaged changes in Git</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[technical analysis]]></title>
    <link href="www.echo-ohce.com/14742423250761.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250761.html</id>
    <content type="html"><![CDATA[
<p>This documents my learning of stock technical analysis, keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">python packages to use</a>
</li>
<li>
<a href="#toc_1">python notebook extra setting</a>
</li>
<li>
<a href="#toc_2">Some good blogs</a>
</li>
<li>
<a href="#toc_3">Using tushare to get historical data</a>
</li>
<li>
<a href="#toc_4">Using pandas and yahoo is a better choice of getting daily data</a>
</li>
<li>
<a href="#toc_5">Build personal stock data repo</a>
</li>
<li>
<a href="#toc_6">这里有一篇用JAVA建立自己股票数据系统的文章</a>
</li>
<li>
<a href="#toc_7">使用通达信等本地股票软件的数据</a>
</li>
<li>
<a href="#toc_8">通达信的本地数据</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">python packages to use</h1>

<ol>
<li>zipline - for testing framework</li>
<li>tushare - for chinese stock market data</li>
<li><a href="https://github.com/mrjbq7/ta-lib">talib</a> - for indicators and functions. The official document is <a href="https://mrjbq7.github.io/ta-lib/doc_index.html">here</a></li>
</ol>

<h1 id="toc_1">python notebook extra setting</h1>

<pre><code class="language-python">import zipline as zp
import tushare as ts
ts.set_token(&#39;ef0f57f6c63416271c140912cb1e2bf58e1580f1aa5147efa19538085b21cf60&#39;) # use your own free api key
</code></pre>

<h1 id="toc_2">Some good blogs</h1>

<ol>
<li><a href="http://pythontrader.blogspot.com">使用Python語言開發「程式交易」系統</a></li>
<li><a href="http://tradingwithpython.blogspot.com">Trading with Python</a></li>
</ol>

<h1 id="toc_3">Using tushare to get historical data</h1>

<ul>
<li>A good <a href="http://www.360doc.com/content/16/0213/20/7249274_534357168.shtml">post</a> to quick check.</li>
<li>The stock codename in ts may be different than normal stock software.</li>
<li>to get the shanghai and shenzhen index use the following code example</li>
</ul>

<pre><code class="language-python">sh_index=ts.get_hist_data(&#39;sh&#39;)
sz_index=ts.get_hist_data(&#39;sz&#39;)
</code></pre>

<h1 id="toc_4">Using pandas and yahoo is a better choice of getting daily data</h1>

<p>tushare is not very stable, it is slow. It also has 3 years limitation.<br/><br/>
For using pandas and yahoo, these are resources:</p>

<ul>
<li><a href="https://yongle.gitbooks.io/data-science/content/note/4pandas/yahoo.html">A good post</a></li>
<li><a href="http://pandas-datareader.readthedocs.io/en/latest/">official docs</a></li>
<li><a href="http://jxyyjm.lofter.com/post/1d11b81d_7bbbc16">股票数据接口的小结</a></li>
</ul>

<p>如果要获取沪深股市信息股票名称格式如下：<br/>
上海：股票代码.SS<br/>
深圳：股票代码.SZ</p>

<pre><code class="language-python">import pandas_datareader.data as web

daily = web.DataReader(&#39;000001.SS&#39;, &#39;yahoo&#39;, &#39;2000-01-01&#39;, &#39;2016-01-01&#39;)
</code></pre>

<h1 id="toc_5">Build personal stock data repo</h1>

<p><a href="http://figurebelow.com/2013/10/18/creating-a-personal-stock-database-using-mongo-and-openshift/">A good post of building MongoDB stock data repo</a></p>

<pre><code>* One key component [YfinanceMongo](https://github.com/figurebelow/yfinanceMongo)
* The key wrapper [YFinanceFetcher](https://github.com/figurebelow/yfinancefetcher)
</code></pre>

<p>Using <code>crontab</code> to update this repo automatically.</p>

<h1 id="toc_6">这里有一篇用JAVA建立自己股票数据系统的文章</h1>

<p><a href="http://qkxue.net/info/126246/Python">写得不错，有时间应该看看</a></p>

<h1 id="toc_7">使用通达信等本地股票软件的数据</h1>

<p><a href="http://studygolang.com/topics/922">使用的go语言</a></p>

<h1 id="toc_8">通达信的本地数据</h1>

<ul>
<li>最全面的方式是在通达信中键入34打开数据导出，然后导出数据为CSV的格式。

<ul>
<li>这种方式可以选择前后复权或者不复权。</li>
<li>必须在windows才有这项功能</li>
<li>VMShareFolder的同步</li>
<li><a href="https://github.com/pywinauto/pywinauto">pywinauto</a>来实现自动操作</li>
</ul></li>
<li><p>可以在<code>安装目录\vipdoc\{sh,sz}\lday</code>下直接读取binary的日线数据文件，读取的格式是：<br/>
<code>int date; int open; int high; int low; int close; int amount; int vol; int reservation;</code><br/><br/>
<a href="http://www.doczj.com/doc/aa3ebcb827284b73f24250d8.html">参考文档 - 目录位置</a><br/>
<a href="http://www.doczj.com/doc/d1217c1ec281e53a5802ff71-3.html">参考文档 - 数据结构</a></p></li>
<li></li>
</ul>

<hr/>

<ul>
<li>
<a href="#toc_0">python packages to use</a>
</li>
<li>
<a href="#toc_1">python notebook extra setting</a>
</li>
<li>
<a href="#toc_2">Some good blogs</a>
</li>
<li>
<a href="#toc_3">Using tushare to get historical data</a>
</li>
<li>
<a href="#toc_4">Using pandas and yahoo is a better choice of getting daily data</a>
</li>
<li>
<a href="#toc_5">Build personal stock data repo</a>
</li>
<li>
<a href="#toc_6">这里有一篇用JAVA建立自己股票数据系统的文章</a>
</li>
<li>
<a href="#toc_7">使用通达信等本地股票软件的数据</a>
</li>
<li>
<a href="#toc_8">通达信的本地数据</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mongodb]]></title>
    <link href="www.echo-ohce.com/14820465795454.html"/>
    <updated>2016-12-17T23:36:19-08:00</updated>
    <id>www.echo-ohce.com/14820465795454.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Pandas and Mongodb</a>
</li>
<li>
<a href="#toc_1">Auto Start MongoDB daemon (mongod) on the background</a>
</li>
<li>
<a href="#toc_2">Hitting the ulimit</a>
</li>
<li>
<a href="#toc_3">sort argument inside find_one method</a>
</li>
<li>
<a href="#toc_4">Indexing and sort in both directions</a>
</li>
<li>
<a href="#toc_5">mongodb locations</a>
</li>
<li>
<a href="#toc_6">check mongodb is running or not</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Pandas and Mongodb</h1>

<ul>
<li>From pandas dataframe to mongodb: use <code>to_dict(&#39;records&#39;)</code>. <font color='red'>Must have the records parameters to get a list of dict, not a nested dict </font>. <a href="http://stackoverflow.com/a/33984472/4229125">Stackoverflow post</a></li>
</ul>

<pre><code class="language-python">db.timeline.insert_many(yahoo_df[::-1]      #yahoo_df is reversed order
                        .drop(&#39;Adj Close&#39;, axis=1)      #don&#39;t need the adj close
                        .to_dict(&#39;records&#39;))            #to mongodb
</code></pre>

<ul>
<li>From mongodb to pandas dataframe: use <code>list</code> of the pymonogo&#39;s <code>find</code>&#39;s cursor.  <a href="http://stackoverflow.com/a/17805626/4229125">Stackoverflow post</a></li>
</ul>

<pre><code class="language-python">from dateutil import parser as dateparser

start_date = dateparser.parse(&#39;2015-01-01&#39;)
end_date = dateparser.parse(&#39;2016-01-01&#39;)
mongo_df = pd.DataFrame(list(db.timeline.find({&#39;Date&#39;:{&#39;$lt&#39;:end_date, &#39;$gt&#39;:start_date}})))
</code></pre>

<h1 id="toc_1">Auto Start MongoDB daemon (mongod) on the background</h1>

<p><a href="http://serverfault.com/a/398366">Stackoverflow post</a></p>

<pre><code class="language-bash">mkdir -p ~/Library/LaunchAgents
cp /usr/local/Cellar/mongodb/3.2.10/homebrew.mxcl.mongodb.plist ~/Library/LaunchAgents
launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.mongodb.plist
</code></pre>

<p>This will launch mongod in the background now and every time you restart your computer.</p>

<p>The best way to get a program to run at startup on OS X is to create a LaunchDaemon (see <a href="http://developer.apple.com/mac/library/documentation/MacOSX/Conceptual/BPSystemStartup/Articles/LaunchOnDemandDaemons.html">Apple&#39;s docs</a>, and take a look at some of the Apple-supplied daemons in /System/Library/LaunchDaemons) and install it in /Library/LaunchDaemons. <a href="https://sourceforge.net/projects/lingon/files/">Lingon</a> can help you create the .plist file.<br/>
Note by Y.G <code>Lingon</code> is dead now.</p>

<p><font color='red'><strong>UPDATE</strong></font><br/>
A easier way is to use the app <code>MongoDB.prefPane</code> <a href="https://www.mongodb.com/blog/post/macosx-preferences-pane-for-mongodb">Official link</a><br/><br/>
After install this app, the <code>~/Library/LaunchAgents/homebrew.mxcl.mongodb.plist</code> will be replaced by <code>~/Library/LaunchAgents/com.remysaissy.mongodbprefspane.plist</code>. However the content is exactly the same.</p>

<h1 id="toc_2">Hitting the ulimit</h1>

<p><strong><a href="https://github.com/basho/basho_docs/issues/1402">A very good summary post about this</a></strong></p>

<p>mongod silently die, check the log at <code>/usr/local/var/log/mongodb/mongo.log</code> find the following error:</p>

<blockquote>
<p>2016-12-18T21:44:30.153-0800 W FTDC     [ftdc] Uncaught exception in &#39;FileNotOpen: Failed to open interim file /usr/local/var/mongodb/diagnostic.data/metrics.interim.temp&#39; <br/>
2016-12-18T21:44:34.930-0800 E STORAGE  [thread1] WiredTiger (24) [1482126274:929985][960:0x700000393000], file:WiredTiger.wt, WT_SESSION.checkpoint: /usr/local/var/mongodb/WiredTiger.turtle: handle-open: open: Too many open files<br/>
2016-12-18T21:44:34.934-0800 I -        [thread1] Fatal Assertion 28558</p>
</blockquote>

<p>This is because I have too many collections in the db and hitting the limit of open files.</p>

<ul>
<li><a href="https://docs.mongodb.com/manual/reference/ulimit/#ulimit">check the current limit</a><br/></li>
</ul>

<pre><code class="language-bash">ulimit -a  
</code></pre>

<ul>
<li><a href="https://docs.mongodb.com/manual/reference/ulimit/#recommended-ulimit-settings">recommended limit</a></li>
<li>temporary set a higher limit (which will expire when restart)</li>
</ul>

<pre><code class="language-bash">sudo launchctl limit maxfiles 65536 65536
sudo launchctl limit maxproc 2048 2048
ulimit -n 65536
sudo ulimit -u 2048
</code></pre>

<p>What I did in the end that works are exactly following the <a href="https://github.com/basho/basho_docs/issues/1402">aforementioned post</a></p>

<ol>
<li>Changed the limit from the <code>plist</code> file.</li>
<li>created the two file: <code>/Library/LaunchDaemons/limit.maxfiles.plist</code> and <code>/Library/LaunchDaemons/limit.maxproc.plis</code></li>
<li>Updated my <code>~/.bashrc</code></li>
<li>restart</li>
</ol>

<h1 id="toc_3">sort argument inside find_one method</h1>

<p>In <code>pymongo</code>, it is easy to make bug at the the sort syntax. The correct way of sort and take the first one. (The <code>[]</code> of the sort argument is a must, i.e. sort must be a <strong>list</strong> of <strong>tuple</strong>)</p>

<pre><code class="language-python">first_date = db[ncode].find_one(sort=[(&#39;Date_d&#39;, pymongo.ASCENDING)])[&#39;Date_d&#39;]
last_date = db[ncode].find_one(sort=[(&#39;Date_d&#39;, pymongo.DESCENDING)])[&#39;Date_d&#39;]
</code></pre>

<h1 id="toc_4">Indexing and sort in both directions</h1>

<p>MongoDB can scan an index from both directions. MongoDB may also traverse the index in either directions. As a result, for single-field indexes, ascending and descending indexes are interchangeable. This is not the case for compound indexes: in compound indexes, the direction of the sort order can have a greater impact on the results</p>

<h1 id="toc_5">mongodb locations</h1>

<p>After installing MongoDB with <code>Homebrew</code>:</p>

<ul>
<li>The databases are stored in the <code>/usr/local/var/mongodb/</code> directory</li>
<li>The <code>mongod.conf</code> file is here: <code>/usr/local/etc/mongod.conf</code></li>
<li>The mongo logs can be found at <code>/usr/local/var/log/mongodb/</code></li>
<li>The mongo binaries are here: <code>/usr/local/Cellar/mongodb/[version]/bin</code></li>
</ul>

<h1 id="toc_6">check mongodb is running or not</h1>

<pre><code class="language-bash">ps -ef | grep mongod | grep -v grep | wc -l | tr -d &#39; &#39;
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">Pandas and Mongodb</a>
</li>
<li>
<a href="#toc_1">Auto Start MongoDB daemon (mongod) on the background</a>
</li>
<li>
<a href="#toc_2">Hitting the ulimit</a>
</li>
<li>
<a href="#toc_3">sort argument inside find_one method</a>
</li>
<li>
<a href="#toc_4">Indexing and sort in both directions</a>
</li>
<li>
<a href="#toc_5">mongodb locations</a>
</li>
<li>
<a href="#toc_6">check mongodb is running or not</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[bash]]></title>
    <link href="www.echo-ohce.com/14742423250478.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250478.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Using curly braces in Bash substitution</a>
</li>
<li>
<a href="#toc_1">nohup to a different file</a>
</li>
<li>
<a href="#toc_2">Append multiple lines to a file</a>
</li>
<li>
<a href="#toc_3">copy from bash shell to clipboard</a>
</li>
<li>
<a href="#toc_4">Hadoop hdfs get all except one folder (file)</a>
</li>
<li>
<a href="#toc_5">Hadoop hdfs / bash get matched folders</a>
</li>
<li>
<a href="#toc_6">check file encoding</a>
</li>
<li>
<a href="#toc_7">du without recursion</a>
</li>
<li>
<a href="#toc_8">checking running job and argument</a>
</li>
<li>
<a href="#toc_9">grep find negate</a>
</li>
<li>
<a href="#toc_10">grep find text in all files under a folder and all of its subfolders</a>
</li>
<li>
<a href="#toc_11">Keep the REPL across sessions</a>
</li>
<li>
<a href="#toc_12">Schedule Auto jobs on Mac using crontab</a>
</li>
<li>
<a href="#toc_13">Hadoop Yarn get the Aggregated Log for a job</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Using curly braces in Bash substitution</h1>

<p>It is always a good practice to use curly braces like <code>${foo}</code>, but there are following situations it is a must:</p>

<ul>
<li>confusing strings <code>${foo}bar</code> vs. <code>$foobar</code> in which <code>foobar</code> is a single parameter.</li>
<li>expanding arrays <code>${array[42]}</code></li>
<li>expanding positional parameters beyond 9 <code>$8 $9 ${10}</code></li>
</ul>

<p><code>{}</code> is called <em>brace expansion</em>, <code>${}</code> is called <em>variable expansion</em><br/>
<a href="http://stackoverflow.com/questions/8748831/when-do-we-need-curly-braces-in-variables-using-bash">Reference Post</a>  </p>

<h1 id="toc_1">nohup to a different file</h1>

<pre><code class="language-bash">nohup some_command &gt; nohup_file.out 2&gt;&amp;1 &amp;
</code></pre>

<p>in which <code>2&gt;&amp;1</code> means <font color='red'>redirect <code>stderr</code> to the same output as <code>stdout</code></font>.  </p>

<h1 id="toc_2">Append multiple lines to a file</h1>

<p>Using a <em>eoi</em> (end of input) symbol for multiple lines</p>

<pre><code class="language-bash">cat &lt;&lt;EOI &gt;&gt; file
multiple lines
input here
EOI
</code></pre>

<p>in which, the <code>&lt;&lt;EOI</code> is registering a special symbol marking the end of input, which should be in a line by itself; the <code>&gt;&gt; file</code> means it is <font color='red'>append</font> to the file.</p>

<h1 id="toc_3">copy from bash shell to clipboard</h1>

<p>On mac, it is super easy: <code>pbcopy</code> and <code>pbpaste</code>.  </p>

<pre><code class="language-bash">cat ~/.bashrc | pbcopy
</code></pre>

<p>After that command content of the <code>~/.bashrc</code> file will be available for pasting with <code>cmd+v</code> shortcut.</p>

<h1 id="toc_4">Hadoop hdfs get all except one folder (file)</h1>

<p>As noted before, Hadoop use its own <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html#globStatus%28org.apache.hadoop.fs.Path%29">glob</a>. to do the pattern matching for file name and paths.</p>

<pre><code class="language-bash">$ hls alibaba/ipstats/
Found 8 items
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:36 alibaba/ipstats/IP
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/acookie_day
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/avg_acookie_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/avg_umid_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_acookie_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_day
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_umid_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/umid_day  

$ hdfs dfs -get alibaba/ipstats/[^I]*
</code></pre>

<p>The command gets all folders except for <code>alibaba/ipstats/IP</code>  </p>

<h1 id="toc_5">Hadoop hdfs / bash get matched folders</h1>

<p>This works both on bash and hadoop hdfs</p>

<p>Below is the HDFS folder:</p>

<pre><code class="language-bash">Found 6 items
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/FP
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/TP
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/UNKNOWN
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:19 alibaba/graph/stats/cluster-size-histo
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:19 alibaba/graph/stats/coverage
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:45 alibaba/graph/stats/stats
</code></pre>

<p>Need get the folders of <code>cluster-size-histo</code>, <code>coverage</code> and <code>stats</code></p>

<pre><code class="language-bash">hdfs dfs -get alibaba/graph/stats/[cs]*
</code></pre>

<p>Now local folder:</p>

<pre><code class="language-bash">total 8
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 cluster-size-histo
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 coverage
-rw-rw-r-- 1 yu yu  9 Sep 30 19:28 FP_cnt
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 stats
-rw-rw-r-- 1 yu yu  7 Sep 30 19:29 TP_cnt
</code></pre>

<p>Need delete folders of <code>cluster-size-histo</code>, <code>coverage</code> and <code>stats</code>:</p>

<pre><code class="language-bash">rm -rf [cs]*
</code></pre>

<h1 id="toc_6">check file encoding</h1>

<pre><code class="language-bash">file -I filename
</code></pre>

<p>The output is in format of: <code>/Path/To/Filename: fileformat/filetype; charset=encoding</code>. For example:  </p>

<pre><code class="language-bash">file -I Base.pig
Base.pig: application/octet-stream; charset=binary
</code></pre>

<h1 id="toc_7">du without recursion</h1>

<p>only want to see the whole folder&#39;s disk usage, not the details of its sub-folders.  </p>

<pre><code class="language-bash">du -s -h *
</code></pre>

<p><code>-s</code>: no recursion<br/>
<code>-h</code>: human readable</p>

<h1 id="toc_8">checking running job and argument</h1>

<p>the following commands works well:</p>

<pre><code class="language-bash">top -U [username]
ps -fu [username] # My most used one
ps -efl | grep &lt;username&gt; # get all the arguments
ps -efl | grep &lt;command name&gt;  

ps -efl | egrep &#39;\s+yu\s+&#39;
</code></pre>

<p>This is another good example, used to check <code>mongo</code> instance running in the system:</p>

<pre><code class="language-bash">ps -ef | grep mongod | grep -v grep | wc -l | tr -d &#39; &#39;
</code></pre>

<p>Step-by-Step:</p>

<ul>
<li><p>The <strong><code>ps -ef | grep mongod</code></strong> part return all the running processes, that have any relation to the supplied string, i.e. <code>mongod</code>, e.g. have the string in the executable path, have the string in the username, etc.</p></li>
<li><p>When you run the previous command, the <code>grep mongod</code> also becomes a process containing the string <code>mongod</code> in the <code>COMMAND</code> column of <code>ps</code> output, so it will also appear in the output. For that reason you need to eliminate it by piping <strong><code>grep -v grep</code></strong>, which filters all the lines from the input that contain the string <code>grep</code>.</p></li>
<li><p>So now you have all possible lines that contain string <code>mongod</code> and are not the instances of <code>grep</code>. What to do? Count them, and do that with <strong><code>wc -l</code></strong>.</p></li>
<li><p><code>wc -l</code> output contains additional formatting, i.e. spaces, so just for the sake of the beauty, run <strong><code>tr -d &#39; &#39;</code></strong> to remove the redundant spaces.</p></li>
</ul>

<p>As a result you will get a single number, representing the number of processes you <code>grep</code>&#39;ed for.</p>

<h1 id="toc_9">grep find negate</h1>

<p>negative matching, i.e. match lines that do not contain pattern</p>

<pre><code class="language-bash">grep -v [pattern] 
# -v, --invert-match select non-matching lines
</code></pre>

<h1 id="toc_10">grep find text in all files under a folder and all of its subfolders</h1>

<pre><code class="language-bash">grep -r -l -F -n -i -I &quot;string&quot; /path
</code></pre>

<p>in which:<br/>
<code>-r</code> is for recursive<br/>
<code>-l</code> is for showing the file name only (stop reading the file as soon as the string is find)<br/>
<code>-F</code> is searching for literal &quot;fixed string&quot;, not regexp<br/>
<code>-n</code> is printing the line number<br/>
<code>-i</code> is case-insensitive<br/>
<code>-I</code> is ignore binary files<br/>
Also there are:<br/>
<code>--exclude-dir=dir</code> is useful for excluding directories like .svn and .git</p>

<h1 id="toc_11">Keep the REPL across sessions</h1>

<p>This is very useful when running a Spark REPL on a remote ssh cluster, after logging out and close the terminal, next time connect and logging in the ssh cluster, we can pick up the same REPL session!</p>

<pre><code class="language-bash"># before running the REPL
screen
# start the REPL
/home/xiaogu/repo/spark-1.6.1-bin-hadoop2.4/bin/spark-shell --queue datascience

# Do not leave the REPL but leave the screen
Ctrl-a d

# leave the ssh session

# re-log in the ssh session
screen -r
</code></pre>

<p><a href="http://www.tecmint.com/screen-command-examples-to-manage-linux-terminals/">Detailed post for the <code>screen</code> command</a></p>

<h1 id="toc_12">Schedule Auto jobs on Mac using crontab</h1>

<p><a href="https://ole.michelsen.dk/blog/schedule-jobs-with-crontab-on-mac-osx.html">Schedule jobs with crontab on Mac OS X</a></p>

<p>This can be used to build and update personal stock repo.</p>

<h1 id="toc_13">Hadoop Yarn get the Aggregated Log for a job</h1>

<p>(Thanks Mingyang)</p>

<pre><code class="language-bash">yarn logs -applicationId application_1478897002704_67148 -appOwner yu  
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">Using curly braces in Bash substitution</a>
</li>
<li>
<a href="#toc_1">nohup to a different file</a>
</li>
<li>
<a href="#toc_2">Append multiple lines to a file</a>
</li>
<li>
<a href="#toc_3">copy from bash shell to clipboard</a>
</li>
<li>
<a href="#toc_4">Hadoop hdfs get all except one folder (file)</a>
</li>
<li>
<a href="#toc_5">Hadoop hdfs / bash get matched folders</a>
</li>
<li>
<a href="#toc_6">check file encoding</a>
</li>
<li>
<a href="#toc_7">du without recursion</a>
</li>
<li>
<a href="#toc_8">checking running job and argument</a>
</li>
<li>
<a href="#toc_9">grep find negate</a>
</li>
<li>
<a href="#toc_10">grep find text in all files under a folder and all of its subfolders</a>
</li>
<li>
<a href="#toc_11">Keep the REPL across sessions</a>
</li>
<li>
<a href="#toc_12">Schedule Auto jobs on Mac using crontab</a>
</li>
<li>
<a href="#toc_13">Hadoop Yarn get the Aggregated Log for a job</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[pig]]></title>
    <link href="www.echo-ohce.com/14742423250700.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250700.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">File Name Pattern Substitution</a>
</li>
<li>
<a href="#toc_1"><code>ToDate(milliseconds)</code> gives wrong date</a>
</li>
<li>
<a href="#toc_2">DateTime SimpleDateFormat</a>
</li>
<li>
<a href="#toc_3">Parameter substitution Shell Runner subtle bug</a>
</li>
<li>
<a href="#toc_4">Becareful of <code>DateTime</code> data type in pig</a>
</li>
<li>
<a href="#toc_5">More pitfalls for <code>DateTime</code> data type</a>
</li>
<li>
<a href="#toc_6">Setting of memory</a>
</li>
<li>
<a href="#toc_7">Pig Unit Test <code>override()</code> method</a>
</li>
<li>
<a href="#toc_8">Error message for wrong reference operator</a>
</li>
<li>
<a href="#toc_9">Pig Java UDF Unit Test</a>
</li>
<li>
<a href="#toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></a>
</li>
<li>
<a href="#toc_11">specify function output data type</a>
</li>
<li>
<a href="#toc_12">using <code>u0001</code> as the delimiter</a>
</li>
<li>
<a href="#toc_13">Error with SUBSTRING and SIZE function</a>
</li>
<li>
<a href="#toc_14">oneline to order two fields</a>
</li>
<li>
<a href="#toc_15">sort with duplicates</a>
</li>
<li>
<a href="#toc_16">input and output are reserved keyword in pig</a>
</li>
<li>
<a href="#toc_17">Mystery ERROR 2999</a>
</li>
<li>
<a href="#toc_18">Cumsum</a>
</li>
<li>
<a href="#toc_19">A pretty good graph cluster to pair Pig example code</a>
</li>
<li>
<a href="#toc_20">A pretty good Score bucketize and cumsum precision Pig example code</a>
</li>
<li>
<a href="#toc_21">Parameter Substitution in Pig</a>
</li>
<li>
<a href="#toc_22">SUBSTRING function</a>
</li>
<li>
<a href="#toc_23">Nested FOREACH</a>
</li>
<li>
<a href="#toc_24">Function are allowed inside FOREACH</a>
</li>
<li>
<a href="#toc_25">Use Star Expression and Project-Range Expression</a>
</li>
<li>
<a href="#toc_26">Strange bug ERROR 2116 related with compression</a>
</li>
<li>
<a href="#toc_27">Strange Bug with Split of two logic expressions</a>
</li>
<li>
<a href="#toc_28">Use AvroStorage() inside pig</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">File Name Pattern Substitution</h1>

<p>Pig is using hadoop file <code>glob</code> utilities to process the file name pattern. It is <font color=red>NOT</font> using shell&#39;s <code>glob</code>. Hadoop&#39;s <code>glob</code> are documented <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html#globStatus%28org.apache.hadoop.fs.Path%29">here</a>. Specially, it does not support <code>..</code> operator for a range.</p>

<p><a href="http://stackoverflow.com/questions/3515481/pig-latin-load-multiple-files-from-a-date-range-part-of-the-directory-structur">reference post</a></p>

<h1 id="toc_1"><code>ToDate(milliseconds)</code> gives wrong date</h1>

<p>Bug looks like always return date of some time at <code>1970-01-17</code>. This is because the input is 10 digits in seconds, and input should be in milliseconds. Need \( \times 1000\) on the input to get the correct DateTime.</p>

<h1 id="toc_2">DateTime SimpleDateFormat</h1>

<p>Pig&#39;s <code>DateTime</code> type conforms to the following format:</p>

<pre><code class="language-java">SimpleDateFormat dateFormat = 
    new SimpleDateFormat(&quot;yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSSXXX&quot;);
</code></pre>

<p>The string format is: <code>2001-07-04T12:08:56.235-07:00</code><br/>
<a href="https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html">Java official example table</a></p>

<p>[YGMARK] <code>-07:00</code> timezone format is only supported after JDK 7.</p>

<h1 id="toc_3">Parameter substitution Shell Runner subtle bug</h1>

<p>I like to define <code>_</code> as the output file separator. I usually have parameter <code>SEP= &#39;_&#39;</code> in my pig script and shell runner. However, if this parameter is defined inside shell runner, <em>it has to be escaped</em>, like: <code>SEP=&#39;\_&#39;</code>. Otherwise, this mislead error message will show:  </p>

<blockquote>
<p>ERROR org.apache.pig.Main - ERROR 1000: Error during parsing. Lexical error at line 1, column 6.  Encountered: <EOF> after : &quot;&quot;  </p>
</blockquote>

<p>It does not need be escaped if only defined inside pig script.<br/>
Another side note for seeing the above message. It is very likely caused by unbalanced bracket, quote, <font color='red'>extra space between the bash line separator <code>\</code> and the new line</font> etc.</p>

<h1 id="toc_4">Becareful of <code>DateTime</code> data type in pig</h1>

<p>It is convenient to get the time duration of difference by using the <code>DateTime</code> data type. However, in pig it is a know bug that <a href="https://issues.apache.org/jira/browse/PIG-3953">it can not be compared correctly.</a> Therefore ordering, grouping type of work should use either <code>ToUnixTime()</code> or <code>ToString()</code> before ordering.</p>

<blockquote>
<p>Error: org.joda.time.DateTime.compareTo ...</p>
</blockquote>

<h1 id="toc_5">More pitfalls for <code>DateTime</code> data type</h1>

<ul>
<li>The 2nd argument of <code>ToString(datetime [, format string])</code> is not optional, but a must for pig 0.12 (<a href="https://issues.apache.org/jira/browse/PIG-3805">Bug fixed on 0.13</a>)</li>
<li>For my case, <code>ToString(date, &#39;yyyyMMdd&#39;)</code> works.</li>
<li>All capital function name is not accepted, like <code>TOSTRING()</code> fails.</li>
<li>Also, there is no way to directly read in the <code>DateTime</code> type to pig. It still need be read in as <code>chararray</code>, otherwise it will be just null. (I tested it). <a href="http://stackoverflow.com/a/22052965">Stackoverflow post</a></li>
<li><code>GetMilliSecond(DateTime datetime)</code> is not the function to get the millisecond from epoch. It ... en, as the name indicates ... just get the millisecond of the time. To get the <strong>second</strong> use <code>ToUnixTime(DateTime datetime)</code>. To get the <strong>millisecond</strong> use <code>ToMilliSeconds(DateTime datetime)</code>.</li>
</ul>

<h1 id="toc_6">Setting of memory</h1>

<p>In side pig script or grunt:  </p>

<pre><code class="language-bash">SET mapreduce.map.memory.mb 4096;
SET mapreduce.reduce.memory.mb 8192;
</code></pre>

<h1 id="toc_7">Pig Unit Test <code>override()</code> method</h1>

<p>Code normally like:  </p>

<pre><code class="language-java">PigTest test = new PigTest(pigScript, parameters);
test.override(&quot;alias&quot;, &quot;alias = LOAD ...&quot;);
</code></pre>

<p>Inside the override, can not use variable substitution, otherwise will see error like:  </p>

<blockquote>
<p>org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias  </p>
</blockquote>

<p>Wrong:  </p>

<pre><code class="language-java">test.override(&quot;alias&quot;, &quot;alias = LOAD &#39;$INPUT&#39; USING PigStorage() AS (...);&quot;);
</code></pre>

<p>Should always use actual file path:</p>

<pre><code class="language-java">test.override(&quot;alias&quot;, &quot;alias = LOAD &#39;input.txt&#39; USING PigStorage() AS (...);&quot;);
</code></pre>

<h1 id="toc_8">Error message for wrong reference operator</h1>

<p>If receive following error message:  </p>

<blockquote>
<p>Error: Scalar has more than one row in the output.  </p>
</blockquote>

<p>It is very likely that I got a dot <code>Alias.field</code> where you need a double semi-colon <code>Alias::field</code>.<br/><br/>
<a href="http://stackoverflow.com/questions/22522155/pig-scalar-has-more-than-one-row-in-the-output">stackoverflow post</a>  </p>

<h1 id="toc_9">Pig Java UDF Unit Test</h1>

<p>Find a good <a href="http://www.crackinghadoop.com/unit-test-java-udfs/">post</a> specifically for Java UDF unit test.<br/><br/>
The main point is to use <code>DefaultTuple</code> to inject test cases into the UDF.  </p>

<pre><code class="language-java">String inputText = &quot;09/15/2014&quot;;
DefaultTuple input = new DefaultTuple();
input.append(inputText);
</code></pre>

<h1 id="toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></h1>

<ol>
<li><code>join</code> (more specifically, inner join) acts on more than two relations:<br/>
<code>X = JOIN A BY fieldA, B BY fieldB, C BY fieldC;</code> </li>
<li><p><code>join</code> on duplicates gives cross product on the two relations.  </p>

<pre><code class="language-sql">A = LOAD &#39;data1&#39; AS (a1:int,a2:int,a3:int);
DUMP A;   
(1,2,3)
(4,2,1)
(4,3,3)

B = LOAD &#39;data2&#39; AS (b1:int,b2:int);
DUMP B;
(4,6)
(4,9)

X = JOIN A BY a1, B BY b1;
DUMP X;
(4,2,1,4,6)
(4,3,3,4,6)
(4,2,1,4,9)
(4,3,3,4,9)  
</code></pre></li>
<li><p><code>join</code> on null disregards (filters out) null values. See the <a href="https://pig.apache.org/docs/r0.11.1/basic.html#nulls_join">official document</a>.  </p></li>
<li><p><code>cogroup</code> on duplicates gives nested set of tuples for the two relations (a bag for each side of the relation). If <code>flatten</code> is then used, will generate cross product.  </p></li>
<li><p><code>cogroup</code> or <code>group</code> for <code>null</code> key, will be grouped together per relation. <a href="https://pig.apache.org/docs/r0.11.1/basic.html#nulls_group">official document</a>  </p></li>
<li><p><code>cogroup</code> or <code>group</code> for <code>null</code> value, will just keep the empty bag, unless <code>inner</code> keywords is used.  </p></li>
<li><p><code>flatten</code> of an empty bag will <font color='red'><strong>remove</strong></font> the whole record (for the cross product case of <code>flatten</code> two bags) because <code>flatten</code> an empty bag produces no output. <a href="http://datafu.incubator.apache.org/docs/datafu/guide/more-tips-and-tricks.html">Refer to this post</a> to under this further and a trick (<em>generating a bag with a null tuple</em>) to avoid this behavior for getting &#39;outer join&#39; effect.</p></li>
</ol>

<h1 id="toc_11">specify function output data type</h1>

<p>No matter whether it is builtin function or UDF, it is always better to specify this function&#39;s output data type.  </p>

<p>This is wrong, because the CONCAT output type is <code>bytearray</code>:    </p>

<pre><code class="language-sql">ip_db = FOREACH ip_db_m3 GENERATE MD5(CONCAT(ip_raw, &#39;umpdmp&#39;)) AS ip;   
</code></pre>

<p>This is right by specifying clearly that the output data type is <code>chararray</code>:  </p>

<pre><code class="language-sql">ip_db_m3 = FOREACH ip_db_m2 GENERATE CONCAT(ip_raw, &#39;umpdmp&#39;) AS ip:chararray;
ip_db = FOREACH ip_db_m3 GENERATE MD5(ip) AS ip;   
</code></pre>

<h1 id="toc_12">using <code>\u0001</code> as the delimiter</h1>

<ul>
<li>If want to use <code>LzoPigStorage</code> need be careful about in pig, to specify the parameter of the function, you cannot do it at the code where calls this function, but need do it in the <code>DEFINE</code> statement at the beginning, like below:<br/></li>
</ul>

<pre><code class="language-sql">DEFINE LzoPigStorage com.twitter.elephantbird.pig.store.LzoPigStorage(&#39;\u0001&#39;);   

store OUT into &#39;$OUTPUT&#39; using LzoPigStorage();   
</code></pre>

<ul>
<li>Note that <code>&#39;^A&#39;</code> will <strong>NOT</strong> work, Only <code>&#39;\u0001&#39;</code> works.<br/></li>
<li>If use <code>PigStorage</code> just specify it inside the function like below:<br/></li>
</ul>

<pre><code class="language-sql">A = LOAD &#39;input.txt&#39; USING PigStorage(&#39;,&#39;);   
STORE A INTO &#39;out&#39; USING PigStorage(&#39;\u0001&#39;);   
</code></pre>

<h1 id="toc_13">Error with SUBSTRING and SIZE function</h1>

<p>I was trying to remove the leading <code>.</code> of a ip string, and used <code>SUBSTRING(ip, 1, SIZE(ip)) AS ip_fixed:chararray</code>. The error message reads:  </p>

<blockquote>
<p>ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.SUBSTRING as multiple or none of them fit. Please use an explicit cast  </p>
</blockquote>

<p>The reason is because <code>SIZE()</code> function returns <code>long</code> type, but <code>SUBSTRING()</code> function needs <code>int</code> type. The following code passes: <code>SUBSTRING(ip, 1, (int) SIZE(ip)) AS ip_fixed:chararray</code>  </p>

<h1 id="toc_14">oneline to order two fields</h1>

<pre><code class="language-sql">CC_raw = LOAD &#39;$INPUT_CC&#39; USING PigStorage()
                   AS (id1:chararray, id2:chararray, score:double);
CC_m0 = FOREACH CC_raw GENERATE 
            FLATTEN(((id1 &lt; id2) ? (id1, id2) : (id2, id1))) AS (smallID, largeID),
            score AS score;
</code></pre>

<p>order the two fields <code>id1</code> and <code>id2</code>.</p>

<h1 id="toc_15">sort with duplicates</h1>

<p>sort with heavily duplicated field sometimes generate highly skewed partitions (they are partitioned based on bins, records with the same value of the sorting field will be inside the same bin). This will reduce the performance dramatically.</p>

<p>Solution is to sort with combined fields like <code>BY (f1, f2)</code> in which, <code>f1</code> is the actual field that need be sorted but with a lot of duplicates, <code>f2</code> is another field that is much more evenly distributed to help reduce the skew.</p>

<p>Thanks for MingYan :)  </p>

<h1 id="toc_16">input and output are reserved keyword in pig</h1>

<p>Do not use <code>input</code> or <code>output</code> as relations name, because they are reserved keyword. Error message is like:  </p>

<blockquote>
<p>ERROR org.apache.pig.PigServer - exception during parsing: Error during parsing. <file clean_vertex_result.pig, line 3, column 0>  mismatched input &#39;input&#39; expecting EOF</p>
</blockquote>

<h1 id="toc_17">Mystery ERROR 2999</h1>

<blockquote>
<p>ERROR 2999: Unexpected internal error. null</p>
</blockquote>

<p>One case (I wasted one full day on this!) is I used the same name for both a relation and a field!</p>

<p>Solution: add <code>_r</code> for all relation names</p>

<h1 id="toc_18">Cumsum</h1>

<p>use <code>Over, Stitch, ORDER BY</code></p>

<pre><code class="language-sql">REGISTER /home/adsymp/lib/piggybank.jar;

DEFINE OVER org.apache.pig.piggybank.evaluation.Over(&#39;long&#39;);
DEFINE STITCH org.apache.pig.piggybank.evaluation.Stitch();

output_to_LP_m0 = FOREACH (GROUP pairs_bucketized ALL) {
                        sorted = ORDER pairs_bucketized BY score_bucket DESC;
                        GENERATE FLATTEN(STITCH(sorted, OVER(sorted.tp_inbucket, &#39;sum(long)&#39;), OVER(sorted.fp_inbucket, &#39;sum(long)&#39;)));};
</code></pre>

<ul>
<li><code>STITCH</code> take bags as input, and its output cannot be directly assigned to a relation, i.e. it has to be inside a <code>FOREACH</code> statement <a href="http://stackoverflow.com/a/27798007">Stackoverflow post</a> </li>
<li><code>ORDER</code> makes the rows sorted for the <code>cumsum</code></li>
<li><code>OVER</code> does the <code>cumsum</code> <a href="https://pig.apache.org/docs/r0.12.0/api/org/apache/pig/piggybank/evaluation/Over.html">Official document</a></li>
<li><code>STITCH</code> make the <code>cumsum</code> one-on-one appending with the correct row.</li>
<li>When I tried <code>OVER</code> through <code>grunt</code>, I checked the output schema is <code>null</code>. This is not a bug! In fact, if we didn&#39;t input the string of type in the <code>Over()</code> constructor, this <code>piggybank</code> function does not know what is the output schema.</li>
</ul>

<p>One lesson learned: If it is the logic planning stage error, checking the alias.</p>

<h1 id="toc_19">A pretty good graph cluster to pair Pig example code</h1>

<p><code>/Users/yugan/repo/drawbridge/dpp/workflow-new/miaozhen-scripts/graph/Stats.pig</code></p>

<p>In this code:</p>

<ul>
<li>use of <code>TOKENIZE</code>, <code>LAST_INDEX_OF</code></li>
<li>The way of exploding graph <code>cluster</code> into pairs and get the TP/FP with label data is pretty clean and neat.</li>
<li>use <code>HashString</code> to sample</li>
</ul>

<h1 id="toc_20">A pretty good Score bucketize and cumsum precision Pig example code</h1>

<p><code>/Users/yugan/repo/drawbridge/dpp/workflow-new/miaozhen-scripts/rank/ToPrec2.pig</code></p>

<p>In this code:</p>

<ul>
<li>use <code>$SCORE_FACTOR</code> converting double to long for bucketize.</li>
<li>use of three UDF <code>GetScoreBuckets</code>, <code>AnalyzeScoreBucket</code>, and <code>GetPrecision</code>.</li>
<li>use <code>HashString</code> to sample</li>
<li>use <code>replicated</code> mode for the JOIN</li>
</ul>

<p>But I have also pretty good code using <code>Over</code>, <code>Stitch</code>, <code>ORDER BY</code> to implement cumsum as shown above.</p>

<h1 id="toc_21">Parameter Substitution in Pig</h1>

<p>Official documentation <a href="http://wiki.apache.org/pig/ParameterSubstitution">here</a></p>

<h1 id="toc_22">SUBSTRING function</h1>

<p>It is does not include the second index position<br/>
<code>SUBSTRING(string, startIndex, stopIndex)</code>, <code>startIndex</code> starts from <code>0</code>, and <code>stopIndex</code> is not included.<br/><br/>
Given a field named <code>alpha</code> whose value is <code>ABCDEF</code>, to return substring <code>BCD</code> use this statement: <code>SUBSTRING(alpha,1,4)</code>. Note that <code>1</code> is the index of <code>B</code> (the first character of the substring) and <code>4</code> is the index of <code>E</code> (the character <strong>following</strong> the last character of the substring).</p>

<p>Note that the description is different in the official documents both for Pig 0.12:<br/>
* <a href="https://pig.apache.org/docs/r0.12.0/func.html#substring">Correct official document</a> <br/>
* <a href="https://pig.apache.org/docs/r0.12.0/api/org/apache/pig/builtin/SUBSTRING.html">Wrong official document</a></p>

<h1 id="toc_23">Nested FOREACH</h1>

<ol>
<li>Allowed operation are <code>CROSS</code>, <code>DISTINCT</code>, <code>FILTER</code>, <code>FOREACH</code>, <code>LIMIT</code>, <code>ORDER BY</code>, and Project operation. <a href="http://pig.apache.org/docs/r0.15.0/basic.html#foreach">Official Document</a></li>
<li><code>SPLIT</code> does not work inside nested FOREACH</li>
</ol>

<h1 id="toc_24">Function are allowed inside FOREACH</h1>

<p><a href="http://stackoverflow.com/a/26996221/4229125">an example of using <code>SUM</code> inside FOREACH from stackoverflow</a>  </p>

<pre><code class="language-sql">file = LOAD &#39;input.txt&#39; USING PigStorage() AS (type: chararray, year: chararray,
match_count: float, volume_count: float);
grouped = GROUP file BY type;
group_operat = FOREACH grouped {
                                 sum_m = SUM(file.match_count);
                                 sum_v = SUM(file.volume_count);
                                 GENERATE group,(float)(sum_m/sum_v) as sum_mv;
                                }
</code></pre>

<p>Lesson learned: Do not use 2 levels of <code>FOREACH</code> if it is not absolutely necessary.</p>

<p>Wrong </p>

<pre><code class="language-sql">acookie_log_input = LOAD &#39;$INPUT_ACOOKIE&#39; USING LzoPigStorage()
                       AS (acookie:chararray, ip:chararray, time_stamp:DateTime, useragent:chararray, browser:chararray,
                           os:chararray, url:chararray, province:chararray, acookie_daily_tag:chararray,
                           url_catelevel1:chararray, url_catelevel2:chararray, date:DateTime);
acookie_log_input_m0 = FOREACH acookie_log_input GENERATE acookie AS acookie, SUBSTRING(province, 0, 2) AS province:chararray;
acookie_log_input_m1 = FILTER acookie_log_input_m0 BY province IS NOT NULL;
acookie_log_input_m2 = FOREACH (GROUP acookie_log_input_m1 BY (acookie, province)) GENERATE FLATTEN(group) AS (acookie, province),
                        COUNT(acookie_log_input_m1) AS acookie_province_cnt;
-- Does not work from here
acookie_province = FOREACH (GROUP acookie_log_input_m2 BY acookie) {
                    total_cnt = SUM(acookie_log_input_m2.acookie_province_cnt);
                    normalized = FOREACH acookie_log_input_m2 GENERATE (int) (acookie_province_cnt * 100 / total_cnt) AS hist:int;
                    sorted = ORDER normalized BY hist DESC;
                    GENERATE group AS acookie, BagToString(sorted.hist) AS hist:chararray;};
</code></pre>

<p>Correct</p>

<pre><code class="language-sql">acookie_province_m0 = FOREACH (GROUP acookie_log_input_m2 BY acookie) {
                    total_cnt = SUM(acookie_log_input_m2.acookie_province_cnt);
                    GENERATE group AS acookie, FLATTEN(acookie_log_input_m2.acookie_province_cnt) AS (acookie_province_cnt),
                    total_cnt AS total_cnt;};
acookie_province_m1 = FOREACH acookie_province_m0 GENERATE acookie AS acookie, (int) (acookie_province_cnt*100/total_cnt) AS hist:int;
acookie_province = FOREACH (GROUP acookie_province_m1 BY acookie) {
                    sorted = ORDER acookie_province_m1 BY hist DESC;
                    GENERATE group AS acookie, BagToString(sorted.hist) AS hist:chararray;};
</code></pre>

<h1 id="toc_25">Use Star Expression and Project-Range Expression</h1>

<p>to avoid copy all the fields again and again.<br/>
<a href="https://pig.apache.org/docs/r0.12.0/basic.html#expressions">0.12 official documentation</a></p>

<p>Star Expression: <code>*</code><br/>
Project-Range Expression: <code>..</code></p>

<p>A good example:</p>

<pre><code class="language-sql">
lkp_input = LOAD &#39;$INPUT_PAIR&#39; USING PigStorage() AS (id1:long, id2:long, c2NumPath:int,
            c2SumTot:float, c2MinTot:float, c2MaxTot:float, c2SumS1:float, c2MinS1:float, 
            c2MaxS1:float, c2SumS2:float, c2MinS2:float, c2MaxS2:float, c3NumPath:int, 
            c3NumNoRedundantPath:int, c3NumDistinctID:int, c3SumTot:float, c3MinTot:float, 
            c3MaxTot:float, c3SumS1:float, c3MinS1:float, c3MaxS1:float, c3SumS2:float, 
            c3MinS2:float, c3MaxS2:float, c3SumS3:float, c3MinS3:float, c3MaxS3:float);

-- avoid using duplicated field names!
lkp_index_table = LOAD &#39;$INPUT_INDEX_TABLE&#39; USING LzoPigStorage() 
                    AS (id:long, str_id:chararray);

lkp_str_l = FOREACH (JOIN lkp_input BY id1, lkp_index_table BY id) GENERATE
            str_id AS strId1, id2..c3MaxS3;
lkp_str = FOREACH (JOIN lkp_str_l BY id2, lkp_index_table BY id) GENERATE
            strId1 AS strId1, str_id AS strId2, c2NumPath..c3MaxS3;

</code></pre>

<h1 id="toc_26">Strange bug ERROR 2116 related with compression</h1>

<p>Error code like:</p>

<blockquote>
<p>ERROR 2116:<br/>
<file lkpAddLabelNoLzo.pig, line 38, column 0> Output Location Validation Failed for: &#39;hdfs://sc2prod/user/yu/linkprediction/processed_full/0.05_0.08_0.005_11/pairsNoLzo</p>

<p>org.apache.pig.impl.plan.VisitorException: ERROR 2116:<br/>
<file lkpAddLabelNoLzo.pig, line 38, column 0> Output Location Validation Failed for: &#39;hdfs://sc2prod/user/yu/linkprediction/processed_full/0.05_0.08_0.005_11/pairsNoLzo<br/>
        at org.apache.pig.newplan.logical.rules.InputOutputFileValidator$InputOutputFileVisitor.visit(InputOutputFileValidator.java:75)</p>
</blockquote>

<p>This error has nothing to do with the output location, but because the wrong compression setting</p>

<p>The wrong one caused the bug</p>

<pre><code class="language-sql">SET output.compression.enabled true;
</code></pre>

<p>The correct one:</p>

<pre><code class="language-sql">SET mapreduce.output.fileoutputformat.compress true;
</code></pre>

<h1 id="toc_27">Strange Bug with Split of two logic expressions</h1>

<p>The error message looks like this:</p>

<blockquote>
<p>2016-11-10 03:38:16,010 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: Split - scope-1576 Operator Key: scope-1576): org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error while executing ForEach at [null[-1,-1]]<br/>
    at ....PhysicalOperator.processInput(PhysicalOperator.java:289)<br/>
    at ....physicalLayer.relationalOperators.POSplit.getNextTuple(POSplit.java:214)<br/>
    at ....relationalOperators.POSplit.runPipeline(POSplit.java:255)<br/>
    ...<br/>
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error while executing ForEach at [null[-1,-1]]<br/>
    at ....physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:306)<br/>
    at ....PhysicalOperator.processInput(PhysicalOperator.java:281)<br/>
    ... 19 more<br/>
Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Float<br/>
    at java.lang.Float.compareTo(Float.java:50)<br/>
    at ....expressionOperators.NotEqualToExpr.doComparison(NotEqualToExpr.java:113)<br/>
    at ....expressionOperators.NotEqualToExpr.getNextBoolean(NotEqualToExpr.java:84)<br/>
    at ....expressionOperators.POAnd.getNextBoolean(POAnd.java:90)</p>
</blockquote>

<p>This error message is <strong><font color='red'>misleading</font></strong>. There is not <code>ClassCastException</code> issue, it is caused by the <code>Split</code>. I don&#39;t understand how this is happened, but <code>split</code> is merely a syntax sugar, we can just use the <code>filter</code> with the same efficiency.</p>

<p>Problem code:</p>

<pre><code class="language-sql">lkp_input = LOAD &#39;$INPUT_PAIR&#39; USING LzoPigStorage() AS (strID1:chararray, strID2:chararray, c2NumPath:int, c2SumTot:float,
                    c2MinTot:float, c2MaxTot:float, c2SumS1:float, c2MinS1:float, c2MaxS1:float, c2SumS2:float,
                    c2MinS2:float, c2MaxS2:float, c3NumPath:int, c3NumNoRedundantPath:int, c3NumDistinctID:int,
                    c3SumTot:float, c3MinTot:float, c3MaxTot:float, c3SumS1:float, c3MinS1:float, c3MaxS1:float,
                    c3SumS2:float, c3MinS2:float, c3MaxS2:float, c3SumS3:float, c3MinS3:float, c3MaxS3:float, label:int);
SPLIT lkp_input INTO
    c2_only_pairs IF ((c2NumPath != -1) AND (c3NumPath == -1)),
    c3_only_pairs IF ((c2NumPath == -1) AND (c3NumPath != -1)),
    c2_c3_pairs OTHERWISE;

c2_all_pairs = FILTER lkp_input BY (c2NumPath != -1);
c3_all_pairs = FILTER lkp_input BY (c3NumPath != -1);
</code></pre>

<p>Correct code:</p>

<pre><code class="language-sql">lkp_input = LOAD &#39;$INPUT_PAIR&#39; USING LzoPigStorage() AS (strID1:chararray, strID2:chararray, c2NumPath:int, c2SumTot:float,
                    c2MinTot:float, c2MaxTot:float, c2SumS1:float, c2MinS1:float, c2MaxS1:float, c2SumS2:float,
                    c2MinS2:float, c2MaxS2:float, c3NumPath:int, c3NumNoRedundantPath:int, c3NumDistinctID:int,
                    c3SumTot:float, c3MinTot:float, c3MaxTot:float, c3SumS1:float, c3MinS1:float, c3MaxS1:float,
                    c3SumS2:float, c3MinS2:float, c3MaxS2:float, c3SumS3:float, c3MinS3:float, c3MaxS3:float, label:int);

c2_only_pairs = FILTER lkp_input BY ((c2NumPath != -1) AND (c3NumPath == -1));
c3_only_pairs = FILTER lkp_input BY ((c2NumPath == -1) AND (c3NumPath != -1));
c2_c3_pairs = FILTER lkp_input BY ((c2NumPath != -1) AND (c3NumPath != -1));;
    
c2_all_pairs = FILTER lkp_input BY (c2NumPath != -1);
c3_all_pairs = FILTER lkp_input BY (c3NumPath != -1);
</code></pre>

<h1 id="toc_28">Use AvroStorage() inside pig</h1>

<p>Need register the following jars</p>

<pre><code class="language-sql">REGISTER /home/adsymp/lib/piggybank.jar;
REGISTER /home/adsymp/lib/avro-1.7.6.jar;
REGISTER /home/adsymp/lib/jackson-core-asl-1.7.3.jar;
REGISTER /home/adsymp/lib/jackson-mapper-asl-1.7.3.jar;
REGISTER /home/adsymp/lib/json-simple-1.1.jar;

DEFINE AvroStorage org.apache.pig.piggybank.storage.avro.AvroStorage(&#39;no_schema_check&#39;);
</code></pre>

<p>Also, don&#39;t mess with the <code>float</code> and <code>double</code> types with the AvroStorage input.</p>

<hr/>

<ul>
<li>
<a href="#toc_0">File Name Pattern Substitution</a>
</li>
<li>
<a href="#toc_1"><code>ToDate(milliseconds)</code> gives wrong date</a>
</li>
<li>
<a href="#toc_2">DateTime SimpleDateFormat</a>
</li>
<li>
<a href="#toc_3">Parameter substitution Shell Runner subtle bug</a>
</li>
<li>
<a href="#toc_4">Becareful of <code>DateTime</code> data type in pig</a>
</li>
<li>
<a href="#toc_5">More pitfalls for <code>DateTime</code> data type</a>
</li>
<li>
<a href="#toc_6">Setting of memory</a>
</li>
<li>
<a href="#toc_7">Pig Unit Test <code>override()</code> method</a>
</li>
<li>
<a href="#toc_8">Error message for wrong reference operator</a>
</li>
<li>
<a href="#toc_9">Pig Java UDF Unit Test</a>
</li>
<li>
<a href="#toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></a>
</li>
<li>
<a href="#toc_11">specify function output data type</a>
</li>
<li>
<a href="#toc_12">using <code>u0001</code> as the delimiter</a>
</li>
<li>
<a href="#toc_13">Error with SUBSTRING and SIZE function</a>
</li>
<li>
<a href="#toc_14">oneline to order two fields</a>
</li>
<li>
<a href="#toc_15">sort with duplicates</a>
</li>
<li>
<a href="#toc_16">input and output are reserved keyword in pig</a>
</li>
<li>
<a href="#toc_17">Mystery ERROR 2999</a>
</li>
<li>
<a href="#toc_18">Cumsum</a>
</li>
<li>
<a href="#toc_19">A pretty good graph cluster to pair Pig example code</a>
</li>
<li>
<a href="#toc_20">A pretty good Score bucketize and cumsum precision Pig example code</a>
</li>
<li>
<a href="#toc_21">Parameter Substitution in Pig</a>
</li>
<li>
<a href="#toc_22">SUBSTRING function</a>
</li>
<li>
<a href="#toc_23">Nested FOREACH</a>
</li>
<li>
<a href="#toc_24">Function are allowed inside FOREACH</a>
</li>
<li>
<a href="#toc_25">Use Star Expression and Project-Range Expression</a>
</li>
<li>
<a href="#toc_26">Strange bug ERROR 2116 related with compression</a>
</li>
<li>
<a href="#toc_27">Strange Bug with Split of two logic expressions</a>
</li>
<li>
<a href="#toc_28">Use AvroStorage() inside pig</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[giraph]]></title>
    <link href="www.echo-ohce.com/14742423250614.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250614.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Kill the background job</a>
</li>
<li>
<a href="#toc_1">giraph Unit testing</a>
</li>
<li>
<a href="#toc_2">giraph edge value not Mutable in place</a>
</li>
<li>
<a href="#toc_3">giraph shell runner / command line</a>
</li>
<li>
<a href="#toc_4">Unsolved problem with the input string split in InputFormat class</a>
</li>
<li>
<a href="#toc_5">giraph log file memory info free/total/max</a>
</li>
<li>
<a href="#toc_6">Giraph Edge and Message memory optimization</a>
<ul>
<li>
<a href="#toc_7">Edge memory management</a>
</li>
<li>
<a href="#toc_8">Message memory management</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">Use MessageStore</a>
</li>
<li>
<a href="#toc_10">The memory cost for java classes</a>
</li>
<li>
<a href="#toc_11">Mutate Graph</a>
<ul>
<li>
<a href="#toc_12">When there is a conflict for the mutate request</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">The timeout parameter for waiting resources</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Kill the background job</h1>

<p>logging on sc2-hive1, run the following command to check the running job on background. (It works for both oozie and normal hadoop jobs)  </p>

<pre><code class="language-bash">ps -u yu
hadoop job -list | grep yu
</code></pre>

<p>When using the <code>ps</code> command and what to see the detailed command argument, get the <code>pid#</code> and run:  </p>

<pre><code class="language-bash">ps --pid pid#
</code></pre>

<p>Hadoop YARN command equivalent to <code>hadoop job -list</code>:  </p>

<pre><code class="language-bash">yarn application -appStates RUNNING -list
</code></pre>

<h1 id="toc_1">giraph Unit testing</h1>

<ul>
<li>When use <code>InternalVertexRunner</code>, comment out all giraph/hadoop counter logs</li>
<li>Most of the case, the unit test fails silently. Adding some <code>sout</code> maybe the only way to debug</li>
<li>the testing string input delimiter should always be <code>space</code> even in reality we can use all kinds of delimiter like <code>\t</code> etc.</li>
</ul>

<p><strong>wrong</strong></p>

<pre><code class="language-java">// tab &#39;\t&#39; delimited
String[] edges = new String[] { &quot;0  1&quot;, &quot;0  3&quot;, &quot;1  0&quot;, &quot;1  3&quot;, &quot;1  2&quot;};
</code></pre>

<p><strong>correct</strong></p>

<pre><code class="language-java">// space delimited
String[] edges = new String[] { &quot;0 1&quot;, &quot;0 3&quot;, &quot;1 0&quot;, &quot;1 3&quot;, &quot;1 2&quot;};
</code></pre>

<h1 id="toc_2">giraph edge value not Mutable in place</h1>

<p>edges are just a reference, it can not be changed in place.<br/>
After mutate the edge value, use <code>vertex.setEdge(id, edgevalue)</code> to change it.</p>

<h1 id="toc_3">giraph shell runner / command line</h1>

<p>This is one file for running giraph with MasterCompute, Aggregator, etc.  </p>

<pre><code class="language-bash">WORKER=177
MEM=5120
HEAP=4352
#zoo keeper
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

hadoop jar dpp-giraph-0.0.1-with-giraph-core.jar \
        com.adsymp.dpp.giraph.lp.LPRunner \
        -Dmapred.job.queue.name=datascience \
        -Dmapreduce.map.memory.mb=$MEM \
        -Dmapreduce.task.timeout=1800000 \
        -Dmapred.task.timeout=1800000 \
        -Dmapreduce.map.java.opts=-Xmx${HEAP}m \
        -Dgiraph.zkList=$ZKLIST \
        com.adsymp.dpp.giraph.lp.WeightedLPComputation \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        -mc com.adsymp.dpp.giraph.lp.ClusterHistogramMC \
        -aw org.apache.giraph.aggregators.TextAggregatorWriter \
        -ca giraph.textAggregatorWriter.frequency=1 \
        -vif com.adsymp.dpp.giraph.lp.LongScoredValueFloatAdjacencyListVertexInputFormat \
        -vip $ADJ \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op $OUT \
        -w $WORKER -ca giraph.SplitMasterWorker=true \
        -ca giraph.zkSessionMsecTimeout=360000 \
        -ca giraph.zkOpsMaxAttempts=20 \
        -ca giraph.zkOpsRetryWaitMsecs=10000 \
        -ca mapred.job.tracker=sc2-rm1:8088 \
        -ca mapreduce.job.tracker=sc2-rm1:8088 \
        -ca com.adsymp.giraph.max.cd=8 \
        -ca giraph.waitingRequestMsecs=300000 \
        -ca com.adsymp.giraph.max.cc=100 \
        -ca com.adsymp.giraph.max.dd=8 \
        -ca com.adsymp.giraph.max.degree=80 \
        -ca com.adsymp.giraph.ignore.dev.types=false \
        -ca com.adsymp.giraph.weight.threshold=0.4 \
        -ca giraph.jobRetryCheckerClass=com.adsymp.dpp.giraph.RetryThriceChecker \
        -ca mapred.job.queue.name=datascience \
        -ca mapred.output.compress=true \
        -ca mapred.task.timeout=1800004 \
        -ca mapreduce.map.memory.mb=$MEM \
        -ca mapreduce.map.java.opts=-Xmx${HEAP}m
</code></pre>

<p>Couple important points:<br/>
1. the line breaker <code>\</code>: must have one space before it and <strong>no</strong> space after it, and every broken line must have it.<br/>
2. Pay special attention to the sequence of the arguments, some of them matters.<br/>
3. The zookeeper list after the line of the Computation class is a must!  </p>

<p>This one file is for running giraph with jython:  </p>

<pre><code class="language-bash">hdfs dfs -rm -r giraph-jython/output
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

yarn jar giraph-jython-1.0-SNAPSHOT-jar-with-dependencies.jar \
        com.adsymp.dpp.giraph.jython.GiraphJythonRunner \
        -Dmapred.job.queue.name=datascience \
        -Dgiraph.zkList=$ZKLIST \
        graph-distance.py \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        --jythonClass GraphDistance \
        --typesHolder com.adsymp.dpp.giraph.jython.JythonTypes \
        -eif org.apache.giraph.io.formats.IntNullTextEdgeInputFormat \
        -eip giraph-jython/input/tiny_graph \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op giraph-jython/output \
        -w 1 \
        -ca giraph.SplitMasterWorker=false
</code></pre>

<h1 id="toc_4">Unsolved problem with the input string split in InputFormat class</h1>

<p>My input file is delimited by <code>^A</code> (i.e.<code>\u0001</code>). Tried to change the <code>LongNullTextEdgeInputFormat.LongNullTextEdgeReader#preprocessLine</code> for splitting by <code>\u0001</code>. Failed.</p>

<p>Temporary workaround is wrote another pig script to convert the input file schema.</p>

<h1 id="toc_5">giraph log file memory info free/total/max</h1>

<p>In the log file of each individual worker, we see the following lines about the real time memory usage: </p>

<blockquote>
<p>2016-10-21 21:35:44,615 INFO [compute-0] org.apache.giraph.graph.ComputeCallable: call: Completed 18 partitions, 159 remaining Memory (free/total/max) = 6154.37M / 16748.50M / 27079.00M.  </p>
</blockquote>

<p>This log is generated by the class: <code>org.apache.giraph.utils.MemoryUtils</code> <a href="https://github.com/apache/giraph/blob/release-1.1/giraph-core/src/main/java/org/apache/giraph/utils/MemoryUtils.java">source code here</a></p>

<p>this class calls <code>java.lang.Runtime</code> class and use the method <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#freeMemory()"><code>freeMemory()</code></a>, <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#maxMemory()"><code>maxMemory()</code></a>, <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#totalMemory()"><code>totalMemory()</code></a> to get the real time memory usage stats</p>

<ul>
<li>free<br/>
the amount of free memory in the Java Virtual Machine. Calling the <code>gc</code> method may result in increasing the value returned by freeMemory. <strong>an approximation to the total amount of memory currently available for future allocated objects</strong></li>
<li>max<br/>
the maximum amount of memory that the Java virtual machine will attempt to use. If there is no inherent limit then the value Long.MAX_VALUE will be returned. <strong>the maximum amount of memory that the virtual machine will attempt to use</strong> </li>
<li>total<br/>
the total amount of memory in the Java virtual machine. The value returned by this method may vary over time, depending on the host environment. (Note that the amount of memory required to hold an object of any given type may be implementation-dependent.) <strong>the total amount of memory currently available for current and future objects</strong></li>
</ul>

<h1 id="toc_6">Giraph Edge and Message memory optimization</h1>

<p>From the book chapter 11:</p>

<blockquote>
<p>Giraph does some memory management on its own. For instance, it stores some of the data, like edges, messages and vertex values, serialized inside of byte arrays that it allocates at the beginning of the computation. Still, it offers a pure Java object-oriented API. To achieve this abstraction, Giraph keeps a number of objects around, internally called representative objects, which it reinitializes with data coming from these binary arrays before passing them to the user, and which it serializes back to the arrays after the user is done with them. This mechanism is used, for example, for the Iterable containing the messages passed to the compute function, or the default implementation of the OutEdges interface where edges are stored for each vertex.</p>
</blockquote>

<p>With this said, it is different between edges and messages.</p>

<h2 id="toc_7">Edge memory management</h2>

<p>Edge in its full name should be <code>Edge&lt;I,E&gt;</code> in which <code>I</code> is the vertexID type (which is the vertexIDs that the edge is pointing to, i.e. <code>OutEdges</code>) and <code>E</code> is the edgeValue type. So we need have a data structure that builds the 1-1 corresponds between the pointing-to ID and the corresponding edgeValue.</p>

<p>The naive way of implementing it is just a <code>List&lt;I, E&gt;</code> or <code>Map&lt;I, E&gt;</code>,  the first one only provides iterator but is cheaper, and the second one provides random access efficiently but is more expensive.</p>

<p>Actually, the two different real implementations are: </p>

<ol>
<li>Provides only Iterable - <code>ByteArrayEdges</code>

<ol>
<li>Use a reusable <code>edge&lt;I,E&gt;</code> object for iterating all edges </li>
<li>The underlying data structure is byte array. Every time, it is deserialized (or serialized) to the byte array</li>
<li>It is already the most memory efficient implementation. The cons are: Not random access, CPU intensive<br/></li>
</ol></li>
<li>Provide random access - <code>OpenHashMapOutedges</code>

<ol>
<li>Can not use byte array anymore, because the edgeValue changes, not fix length of the byte array anymore.</li>
<li>For each vertex, all the edges are save in one Map, using pointing-to ID as the key, and the edgeValue as the Map value.</li>
<li>Use <code>fastutil</code> primitive map type.</li>
<li>Still using a reusable <code>edge&lt;I,E&gt;</code> object for iterating all edges.</li>
<li>Efficient <code>getEdgeValue</code> and <code>setEdgeValue</code> methods.</li>
</ol></li>
</ol>

<p><strong><font color='red'>Special Notes:</font></strong>  </p>

<ul>
<li>Can I use <code>ByteArrayEdges</code> but instructing the Map inside <code>compute</code> method for fast random access? It has pros and cons. 

<ul>
<li>Pros: the edges are saved in memory efficient byte arrays for all vertexes. Otherwise, for each vertex, it needs a map. Because in a giraph worker thread, the graph is processed partition after partition, and in a partition, the graph is processed vertex after vertex, so at a single time, for one worker thread, there will be only one map. </li>
<li>Cons: Stressed on CPU for creating and destroying a lot of maps. Also serialize and deserializes.<br/></li>
</ul></li>
<li>Hashmap actually is quite expensive w.r.t the memory usage, because of the load factor and avoid collision</li>
<li>When using the default <code>ByteArrayEdges</code>, and called <code>getEdgeValue</code> and <code>setEdgeValue</code> method, under the hood, for the <code>getEdgeValue</code>, it just scan through the immutable edges iterator, and for the  <code>setEdgeValue</code> it scan through the <code>MutableEdge</code> iterator.<br/></li>
</ul>

<blockquote>
<p>If the VertexEdges implementation has a specialized random-access method, we use that; otherwise, we scan the edges.</p>
</blockquote>

<h2 id="toc_8">Message memory management</h2>

<p>Different than the Edges, Message are always stored in a <strong>serialized format inside of byte arrays</strong>. We are using the same concept of re-usable iterator to pass the messages to the compute method.</p>

<p>Each giraph <strong>worker</strong> has one <strong>MessageStore</strong>, which is the message &#39;inbox&#39; for <strong>all</strong> the vertexes of this <strong>worker</strong>.</p>

<p>So, the message store, is organized by <strong>Partition index</strong> then further organized by <strong>Vertex index</strong>, essentially a nested map, with partitions as keys of the outer map, and vertex IDs as keys of the inner maps.</p>

<p>The default implementation of giraph uses <strong>original java HashMap</strong> and is not very efficient. There is some space of saving by using the <code>fastutil</code>. (comparing <code>long</code>: 8 bytes, and <code>Long</code>:24 bytes)</p>

<p>This optimization should be done always, because it only sacrifices generality of the partition ID and vertex ID type.</p>

<p><strong><font color='red'><a href="https://github.com/apache/giraph/blob/release-1.1/giraph-core/src/main/java/org/apache/giraph/comm/messages/primitives/long_id/LongByteArrayMessageStore.java">ALREADY HAVE A IMPLEMENTATION</a>: Just use <code>LongByteArrayMessageStore</code> </font></strong></p>

<h1 id="toc_9">Use MessageStore</h1>

<p><strong><font color=red>The book&#39;s content is outdated as of giraph v1.1</font></strong></p>

<p>According to the book, I must first write a class that implements <code>MessageStoreFactory</code> interface, and this factory generates the <code>MessageStore</code> implementation that I need.</p>

<p>Then specify the parameter of <code>giraph.messageStoreFactoryClass</code> to let giraph know.</p>

<p>But actually, the default giraph already implemented the <code>LongByteArrayMessageStore</code></p>

<ol>
<li><a href="http://giraph.apache.org/options.html">the default configuration</a> lists: 

<ol>
<li><code>messageEncodeAndStoreType</code> = <code>BYTEARRAY_PER_PARTITION</code></li>
<li><code>messageStoreFactoryClass</code> = <code>InMemoryMessageStoreFactory</code></li>
</ol></li>
<li>The <a href="https://github.com/apache/giraph/blob/release-1.1/giraph-core/src/main/java/org/apache/giraph/comm/messages/InMemoryMessageStoreFactory.java#L131">source code</a> of <code>InMemoryMessageStoreFactory</code> shows for vertexID <code>LongWritable</code>, <code>messageEncodeAndStoreType</code> = <code>BYTEARRAY_PER_PARTITION</code>, <code>messageStore = new LongByteArrayMessageStore()</code></li>
</ol>

<h1 id="toc_10">The memory cost for java classes</h1>

<p>Remember the load factors for Hash*** stuff, which make it expensive.</p>

<p>A good series posts here:<br/>
<a href="http://java-performance.info/overview-of-memory-saving-techniques-java/">An overview of memory saving techniques in Java</a><br/>
<a href="http://java-performance.info/memory-consumption-of-java-data-types-1/">Memory consumption of popular Java data types – part 1</a><br/>
<a href="http://java-performance.info/memory-consumption-of-java-data-types-2/">Memory consumption of popular Java data types – part 2</a></p>

<h1 id="toc_11">Mutate Graph</h1>

<p>Three different mechanism of mutating a graph</p>

<p>It is in the book of chapter 8</p>

<h2 id="toc_12">When there is a conflict for the mutate request</h2>

<p>It is resolved by <code>VertexResolver</code></p>

<blockquote>
<p>The default vertex resolver performs the following operations: </p>

<ol>
<li>If there were any edge removal requests, first apply these removals. </li>
<li>If there was a request to remove the vertex, then remove it. This is achieved by setting the return Vertex object to null . </li>
<li>If there was a request to add the vertex, and it does not exist, then create the vertex. </li>
<li>If the vertex has messages sent to it, and it does not exist, then create the vertex. </li>
<li>If there was a request to add edges to the vertex, if the vertex does not exist, first create and then add the edges; otherwise, simply add the edges. </li>
</ol>

<p>The order of this list is important because it defines exactly the way that Giraph resolves any conflicts by default. This means that if, for instance, there is request to remove a vertex and at the same time a request to add it, then because the default resolver checks the vertex creation after it does the deletion, it ends up creating the vertex.</p>
</blockquote>

<p><code>-ca giraph.vertexResolverClass=MyVertexResolver</code></p>

<h1 id="toc_13">The timeout parameter for waiting resources</h1>

<pre><code class="language-bash">-Dgiraph.maxMasterSuperstepWaitMsecs=18700000 \
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">Kill the background job</a>
</li>
<li>
<a href="#toc_1">giraph Unit testing</a>
</li>
<li>
<a href="#toc_2">giraph edge value not Mutable in place</a>
</li>
<li>
<a href="#toc_3">giraph shell runner / command line</a>
</li>
<li>
<a href="#toc_4">Unsolved problem with the input string split in InputFormat class</a>
</li>
<li>
<a href="#toc_5">giraph log file memory info free/total/max</a>
</li>
<li>
<a href="#toc_6">Giraph Edge and Message memory optimization</a>
<ul>
<li>
<a href="#toc_7">Edge memory management</a>
</li>
<li>
<a href="#toc_8">Message memory management</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">Use MessageStore</a>
</li>
<li>
<a href="#toc_10">The memory cost for java classes</a>
</li>
<li>
<a href="#toc_11">Mutate Graph</a>
<ul>
<li>
<a href="#toc_12">When there is a conflict for the mutate request</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">The timeout parameter for waiting resources</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[bayesian statistics]]></title>
    <link href="www.echo-ohce.com/14742423250523.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250523.html</id>
    <content type="html"><![CDATA[
<p>This documents my personal understandings about bayesian, keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Likelihood function is not a pdf</a>
</li>
<li>
<a href="#toc_1">MLE maximum Likelihood Estimation</a>
</li>
<li>
<a href="#toc_2">Gamma distribution in <code>scipy</code></a>
</li>
<li>
<a href="#toc_3">Exponential distribution</a>
</li>
<li>
<a href="#toc_4">Normal distribution</a>
<ul>
<li>
<a href="#toc_5">Known std of $sigma<sup>2$</sup></a>
</li>
<li>
<a href="#toc_6">Unknown std of $sigma<sup>2$</sup></a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">pdf, pmf, cdf, ppf explained</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Likelihood function is not a pdf</h1>

<p>(We are here only talking about continuous distribution case since I used pdf here.)<br/><br/>
Let the observation as \(O\) and the parameters as \(\theta\), the probability of seeing \(O\) given \(\theta\) is \(p(O|\theta)\). This is a pdf. It is a function of \(O\), and \(\int_{O}{p(O|\theta)\ dO}=1\), which means integrate through all possible observation \(O\), the sum of probability should be 1.<br/><br/>
The likelihood is the other way around as: given the observation \(O\) that we have seen, the likelihood that the parameter is value \(\theta\).<br/>
\[L(O|\theta)=p(O|\theta)\]<br/>
If we do an integration over all possible \(\theta\), \(\int_{\theta}{L(O|\theta)\ d\theta}\neq1\)<br/><br/>
<a href="http://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability">post on stackoverflow about the difference between likelihood and probability</a><br/>
<a href="http://stats.stackexchange.com/questions/31238/what-is-the-reason-that-a-likelihood-function-is-not-a-pdf">post on stackoverflow about likelihood and pdf</a>  </p>

<h1 id="toc_1">MLE maximum Likelihood Estimation</h1>

<p><a href="https://onlinecourses.science.psu.edu/stat504/node/28">Here</a> is a clear explained MLE course page. It gives Binomial distribution example and Poisson example</p>

<h1 id="toc_2">Gamma distribution in <code>scipy</code></h1>

<p><a href="https://en.wikipedia.org/wiki/Gamma_distribution">Wiki page</a></p>

<p>There are different ways to specify the gamma distribution, which all use two shape parameters:</p>

<ol>
<li><p>\(\alpha\) and \(\beta\) (\(\beta\) is called <code>rate</code>)<br/>
\[\Gamma(\alpha, \beta): pdf \rightarrow f(x; \alpha,\beta)=\frac{\beta^{\alpha}x^{\alpha-1}e^{-x\beta}}{\Gamma(\alpha)}\]<br/>
mean: \(\frac{\alpha}{\beta}\)<br/>
std: \(\frac{\sqrt{\alpha}}{\beta}\)</p></li>
<li><p>\(k\) and or \(\theta\) <br/>
\(\alpha=k\)<br/>
\(\theta=\frac{1}{\beta}\), and it is called <code>scale</code>.<br/>
\[\Gamma(k, \theta): pdf \rightarrow f(x; k,\theta)=\frac{1}{\theta}\frac{\frac{x}{\theta}^{k-1}e^{-\frac{x}{\theta}}}{\Gamma(k)}\]</p></li>
</ol>

<p>In <code>scipy.stats.gamma</code>, it uses the 2nd form, and the shape parameter <code>a</code> is \(k\), the <code>scale</code> is \(\theta\), for default value of <code>scale=1</code>, the pdf reduce to:<br/>
\[\Gamma(a, 1): pdf \rightarrow f(x; a,1)=\frac{x^{a-1}e^{-x}}{\Gamma(a)}\]<br/>
as shown on the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html#scipy.stats.gamma">scipy page</a></p>

<h1 id="toc_3">Exponential distribution</h1>

<p>like waiting for bus, earthquake, etc.  </p>

<p>We know it is exponential distribution: \( pdf(x) = \lambda e^{-\lambda x}\)<br/>
and we observer \(n\) observations: \(\tilde X ={x_1, x_2, ... x_n}\)<br/>
Now we want to get the estimate of \(\lambda\) (because its mean, i.e. expected value is \( \frac{1}{\lambda}\)).</p>

<ul>
<li><p>Likelihood function<br/>
\[ L(\tilde X | \lambda) = \Pi_{i=1}^{n}\lambda e^{-\lambda x_i} = \lambda^{n}e^{-\lambda \Sigma_{i=1}^{n}x_i}\] </p></li>
<li><p>Conjugate prior for \(\lambda\): </p>

<ul>
<li>Prior: \(pdf_{prior}(\lambda) = \Gamma(\alpha, \beta)\)</li>
<li>\(\alpha\) in the exponential-gamma model means the sample size (which is different than the poisson-gamma model, in which, \(\beta\) means the sample size)</li>
<li>mean and std are still the same for gamma distribution as before.</li>
<li>Posterior: \(pdf_{posterior}(\lambda) = \Gamma(\alpha+num_{newSample},  \beta+\Sigma(newSample))\)$</li>
</ul></li>
<li><p>If we have obtained the posterior and want to use the posterior to get the possibility density function pdf of random variable \(X\):<br/>
\[pdf_{posterior}(x) = \int pdf(x)* pdf_{posterior}(\lambda) d\lambda\]</p></li>
</ul>

<h1 id="toc_4">Normal distribution</h1>

<p>It mostly come from Central Limit Theorem.</p>

<h2 id="toc_5">Known std of \(\sigma^2\)</h2>

<p>We know it is normal distribution: \(pdf(x)=N(x|\mu,\sigma_0^2)\)<br/>
and we observer \(n\) observations: \(\tilde X ={x_1, x_2, ... x_n}\)<br/>
Now we want to get the estimate of \(\mu\)</p>

<ul>
<li><p>Likelihood function<br/>
\[ L(\tilde X | \mu ) = \Pi_{i=1}^{n}N(x_i|\mu,\sigma_0^2)\]</p></li>
<li><p>Conjugate prior for \(\mu\):</p>

<ul>
<li>Prior: \(pdf_{prior}(\mu)=N(\mu|m_0, s_0^2)\)</li>
<li>effective sample size of the prior: \(\frac{\sigma_0^2}{s_0^2}\)</li>
<li>Posterior: \(pdf_{posterior}(\mu)=N(\mu|(\frac{n}{n+\frac{\sigma_0^2}{s_0^2}}\bar x + \frac{\frac{\sigma_0^2}{s_0^2}}{n+\frac{\sigma_0^2}{s_0^2}}m_0 ), (\frac{1}{\frac{n}{\sigma_0^2}+\frac{1}{s_0^2}}))\)</li>
</ul></li>
<li><p>If we have obtained the posterior and want to use the posterior to get the possibility density function pdf of random variable \(X\):<br/>
\[pdf_{posterior}(x) = \int pdf(x)* pdf_{posterior}(\mu) d\mu\]<br/>
<strong>A quick way of calculation:</strong><br/>
\[\int N(x|\mu,\sigma_0^2)* N(\mu|m_0, s_0^2) d\mu=N(x|m_0, (s_0^2+\sigma_0^2))\]</p></li>
</ul>

<h2 id="toc_6">Unknown std of \(\sigma^2\)</h2>

<h1 id="toc_7">pdf, pmf, cdf, ppf explained</h1>

<ul>
<li>continuous distribution does not have point probabilities. pdf is the density function for a continuous distribution, and it is not defined for discrete distribution.
\[ P(x_1&lt;=X &lt;= x_2) = \int_{x_1}^{x_2} pdf(x)dx\]</li>
<li>pmf is the point probabilities of discrete distribution, and it is not defined for continuous distribution.
\[ P(X=x_1) = pmf(x_1)\]</li>
<li>cdf gives the accumulated probabilities, and it is defined for both the continuous and discrete distributions
\[ P(X&lt;=x_1) = cdf(x_1)\]</li>
<li>ppf is the inverse of cdf, and is mostly used to calculate the confidence interval. The result of ppf is <strong>not</strong> probability but the actual value of the random variable \(X\).
\[ P(X&lt;=ppf(\beta)) = \beta\]</li>
</ul>

<hr/>

<ul>
<li>
<a href="#toc_0">Likelihood function is not a pdf</a>
</li>
<li>
<a href="#toc_1">MLE maximum Likelihood Estimation</a>
</li>
<li>
<a href="#toc_2">Gamma distribution in <code>scipy</code></a>
</li>
<li>
<a href="#toc_3">Exponential distribution</a>
</li>
<li>
<a href="#toc_4">Normal distribution</a>
<ul>
<li>
<a href="#toc_5">Known std of $sigma<sup>2$</sup></a>
</li>
<li>
<a href="#toc_6">Unknown std of $sigma<sup>2$</sup></a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">pdf, pmf, cdf, ppf explained</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Analytics with Spark Reading Notes]]></title>
    <link href="www.echo-ohce.com/14770727250961.html"/>
    <updated>2016-10-21T10:58:45-07:00</updated>
    <id>www.echo-ohce.com/14770727250961.html</id>
    <content type="html"><![CDATA[
<p>This documents the reading notes of this book. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Souce Code Link:</a>
</li>
<li>
<a href="#toc_1">Chapter 1 and 2</a>
<ul>
<li>
<a href="#toc_2">Start REPL</a>
</li>
<li>
<a href="#toc_3">Quick tricks to use</a>
</li>
<li>
<a href="#toc_4">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_5">Note about the <code>getOrElse()</code> method in the above map:</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Chapter 3</a>
<ul>
<li>
<a href="#toc_7">Download dataset and put on HDFS and start REPL</a>
</li>
<li>
<a href="#toc_8">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_9">The use of mllib ALS</a>
</li>
<li>
<a href="#toc_10">Rating</a>
</li>
<li>
<a href="#toc_11">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_12">split into Training set and CV set and calculate the AUC</a>
</li>
</ul>
</li>
</ul>


<hr/>

<h1 id="toc_0">Souce Code Link:</h1>

<p><a href="https://github.com/sryza/aas/tree/1st-edition">github</a></p>

<h1 id="toc_1">Chapter 1 and 2</h1>

<h2 id="toc_2">Start REPL</h2>

<pre><code class="language-bash"># this works on hlab
spark-shell --master yarn-client

# this works on dblab
/home/xiaogu/repo/spark-1.6.1-bin-hadoop2.4/bin/spark-shell --queue datascience
</code></pre>

<h2 id="toc_3">Quick tricks to use</h2>

<ol>
<li>help: <code>:help</code></li>
<li>history / find the variable or function name: <code>:history</code> or <code>:h?</code></li>
<li>paste code: <code>:paste</code></li>
</ol>

<h2 id="toc_4">Inside REPL Follow along</h2>

<pre><code class="language-scala">val rawblocks = sc.textFile(&quot;linkage/*.csv&quot;)
rawblocks.first
val head = rawblocks.take(10)
head.length
head foreach println

def isHeader(line:String):Boolean =
    line.contains(&quot;id_1&quot;)

head filterNot isHeader foreach println
head filter (ele =&gt; ! isHeader(ele)) foreach println
head filter (!isHeader(_)) foreach println

// Note that spark RDD does not have filterNot method
val noheader = rawblocks filter (!isHeader(_))

def toDouble(token:String):Double = {
    if (token.equals(&quot;?&quot;)) Double.NaN;
    else token.toDouble;}
    
val line = head(5)

def parse(line:String):(Int, Int, Array[Double], Boolean) = {
    val tokens = line split (&#39;,&#39;);
    val id1 = tokens(0).toInt;
    val id2 = tokens(1).toInt;
    val scores:Array[Double] = tokens slice (2, 11) map toDouble;
    val matched = tokens(11).toBoolean;
    (id1, id2, scores, matched)}

val tup = parse(line)

case class MatchData(id1:Int, id2:Int, scores:Array[Double], matched:Boolean);

def parse(line:String):MatchData = {
    val tokens = line split (&#39;,&#39;);
    val id1 = tokens(0).toInt;
    val id2 = tokens(1).toInt;
    val scores:Array[Double] = tokens slice (2, 11) map toDouble;
    val matched = tokens(11).toBoolean;
    MatchData(id1, id2, scores, matched)}
    

val parsed = rawblocks filter (!isHeader(_)) map parse;
parsed.cache()

val mds = head filterNot isHeader map parse;

val grouped = mds.groupBy(_.matched);

val matchCounts = parsed.map(_.matched).countByValue()
val matchCountsSeq = matchCounts.toSeq

import java.lang.Double.isNaN

// Using the stats Action on RDD[Double]
val stats = for (i &lt;- 0 until 9) yield (parsed map 
                (_.scores(i)) filter (!isNaN(_)) stats)
                
// write and load the NAStatCounter.scala
:load spark_book/NAStatCounter.scala

// use foldLeft, zip, map to build the stat with missing values

val samp_m0 = rawblocks take 10
val samp_m1 = samp_m0 filterNot isHeader map parse

val sample = samp_m1.foldLeft(samp_m1.head.scores map (x =&gt; new NAStatCounter())) ((statCounterArry, record) =&gt; statCounterArry zip record.scores map {case (preStatCounter, newScore) =&gt; preStatCounter.add(newScore)})
// Couple things remember:
// 1. samp_m1.foldLeft()() is correct, do not use infix here, samp_m1 foldLeft () () is wrong!
// 2. map {case () =&gt;} is correct, do not use map(case ()=&gt;)
// 3. Using zip to zip up two arrays, that&#39;s how we get the corresponding foldings

// Note foldLeft is in Scala. In spark use aggregate. Need And the aggregate API is different than scala’s foldLeft. here is a good explaination:
http://cuipengfei.me/blog/2014/10/31/spark-fold-aggregate-why-not-foldleft/

val statsm = statsWithMissing(parsed filter (_.matched) map (_.scores))
val statsn = statsWithMissing(parsed filter (!_.matched) map (_.scores))

case class Scored(md : MatchData, score : Double)
val ct = parsed map (row =&gt; Scored(row, (Seq(2,5,6,7,8) map (idx =&gt; if (row.scores(idx).isNaN) 0 else row.scores(idx))).sum))
ct.cache()

val allTrue = (ct map (_.md.matched)).countByValue.getOrElse(true, 0).asInstanceOf[Number].doubleValue

val allFalse = (ct map (_.md.matched)).countByValue.getOrElse(false, 0).asInstanceOf[Number].doubleValue

// using different score cut threshold to divide true/false matches
import breeze.linalg._
val scoreRange = breeze.linalg.linspace( (ct map (_.score)).min, (ct map (_.score)).max, 5)

val truePositive = scoreRange map (cut =&gt; (ct filter (_.score&gt;cut) map (_.md.matched)).countByValue().getOrElse(true, 0).asInstanceOf[Number].doubleValue)

</code></pre>

<h2 id="toc_5">Note about the <code>getOrElse()</code> method in the above map:</h2>

<p>Because what&#39;s in the map (given by <code>countByValue()</code>) is of type <code>Long</code>, and the default value I gave is of type <code>Int</code>, the <code>getOrElse()</code> method returns the super type for both that is <code>AnyVal</code>.</p>

<p><code>AnyVal</code> has no easy way to cast, so far what I found seems the easiest way is to first convert to type <code>Number</code> then cast to <code>Int</code>. <font color='red'> It can not be convert to type <code>Int</code> directly, otherwise exception will be thrown. </font> </p>

<p>put it all together, for a given map:</p>

<pre><code class="language-scala">val value = map.getOrElse(key, 0).asInstanceOf[Number].doubleValue
</code></pre>

<h1 id="toc_6">Chapter 3</h1>

<h2 id="toc_7">Download dataset and put on HDFS and start REPL</h2>

<p><code>http://bit.ly/1KiJdOR</code></p>

<p>The default spark-shell on hlab has driver memory and executor memory of 512MB, which are not enough for the job that we are doing for this chapter. Use the following setting for setting the memory</p>

<p><a href="http://stackoverflow.com/questions/31463382/increase-java-heap-size-in-spark-on-yarn">stackoverflow post</a></p>

<p><a href="https://spark.apache.org/docs/1.2.0/configuration.html">official spark option list</a></p>

<pre><code class="language-bash"># this works on hlab
spark-shell --master yarn-client --driver-memory 2g --executor-memory 2g --conf spark.rdd.compress=true
</code></pre>

<h2 id="toc_8">Inside REPL Follow along</h2>

<pre><code class="language-scala">val rawUserArtistData = sc.textFile(&quot;spark_book/user_artist_data.txt&quot;, 20)

val overLimitCnt = (rawUserArtistData map (_.split(&quot;\\s+&quot;) map (_.toLong)) 
                    filter (arr =&gt; arr(0) &gt; java.lang.Integer.MAX_VALUE 
                                || arr(1)  &gt; java.lang.Integer.MAX_VALUE)).count()
//Note that in scala, we use .size, but for spark RDD, we need use .count(), because it is parallel partitioned.

val rawArtistData =sc.textFile(&quot;spark_book/artist_data.txt&quot;, 20)

//note the following is use char of single quote
val test = rawArtistData take 10
val line0 = test(0) span (_ != &#39;\t&#39;) match {case (id, name) =&gt; (id.toInt, name.trim)}

//Using the flatMap and Option and pattern match with default to handle the dirty data
val artistByID = rawArtistData flatMap (_ span (_ != &#39;\t&#39;) 
                                        match { 
                                            case(id, name) =&gt; {
                                                try {
                                                    Some((id.toInt, name.trim))
                                                } catch {
                                                    case e:NumberFormatException =&gt; None
                                                }}
                                            case anythingElse =&gt; None})
                                        
val rawArtistAlias = sc.textFile(&quot;spark_book/artist_alias.txt&quot;, 20)

//Just as before, using Option to catch exceptions
val artistAliasOption = (rawArtistAlias map (_ span (_ != &#39;\t&#39;) 
                                             match {case (id1, id2) 
                                                      =&gt; try { 
                                                          (Some(id1.trim.toInt, id2.trim.toInt)) } 
                                                         catch { 
                                                          case e: NumberFormatException=&gt;None};
                                                    case _ =&gt; None}))

//remove Options
val artistAliasRDD = artistAliasOption flatMap (x=&gt;x)

//collect as Map
val artistAlias = artistAliasRDD collectAsMap

// pay special attention to the pattern matching of Array
val formatedUserArtistData =(rawUserArtistData map (_ split(&quot;\\s+&quot;) match 
    { case Array(id1Str, id2Str, cntStr) =&gt; try {
                                                Some(Array(id1Str.trim.toInt,
                                                           id2Str.trim.toInt,
                                                           cntStr.trim.toInt))
                                                }
                                            catch {
                                                case e:Throwable =&gt; None
                                                };
      case _ =&gt; None })) flatMap (x=&gt;x)
// build the Rating class used in MLLib
import org.apache.spark.mllib.recommendation._
// broadcast to save resource and speed up, then clean the alias
val bArtistAlias = sc.broadcast(artistAlias)

// special note:
// 1. use the broadcasted thing, need refer to its .value.
// 2. pay attention to how to use the pattern matching inside map anonymous func
val cleanedUserArtistData = (formatedUserArtistData map 
     ({case Array(userId, artistId, cnt) =&gt; 
       Rating(userId, bArtistAlias.value.getOrElse(artistId, artistId), cnt)
       })
    ).cache()

val model = ALS.trainImplicit(cleanedUserArtistData, 10, 5, 0.01, 1.0)                                                 

// see what model looks like
model.userFeatures take 10 map ({case (userId, scoresArr) =&gt; userId.toString + &quot; : &quot; + scoresArr.mkString(&quot; , &quot;)}) foreach println

model.productFeatures take 10 map ({case (productId, scoresArr) =&gt; productId.toString + &quot; : &quot; + scoresArr.mkString(&quot; , &quot;)}) foreach println
                                 
</code></pre>

<h2 id="toc_9">The use of mllib ALS</h2>

<p><a href="http://spark.apache.org/docs/latest/ml-collaborative-filtering.html">Official documents</a></p>

<p>A very good page for explaining the &quot;explicit preference&quot; and &quot;implicit preference&quot; at <a href="http://predictionio.incubator.apache.org/templates/recommendation/training-with-implicit-preference/">PredicctionIO</a></p>

<h2 id="toc_10">Rating</h2>

<p>A more compact class to represent a rating than Tuple3[Int, Int, Double].<br/>
It is kind of like case class <code>Rating(user: Int, product: Int, rating: Double)</code><br/>
<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.recommendation.Rating">reference</a></p>

<h2 id="toc_11">Inside REPL Follow along</h2>

<pre><code class="language-scala">
// find the artist list for userID = 2093760
val targetUserID = 2093760
val joined = cleanedUserArtistData filter (_.user == targetUserID) map (rateEntry =&gt; (rateEntry.product, rateEntry.rating)) join (artistByID)

// Note that these are RDDs, do not forget the collect!
(joined map ({case (artistID, (cnt, name)) =&gt; name + &quot; : &quot; + cnt.toInt.toString}) collect) foreach println


// make recommendation
// Note that it is NOT RDD, but regular scala Array
val recommendations = model.recommendProducts(targetUserID, 5)

val recommendSet = (recommendations map (_.product)).toSet

val relatedArtistNameMap = (artistByID filter (record =&gt; recommendSet.contains(record._1))).collectAsMap

recommendations map (rateEntry =&gt; relatedArtistNameMap.getOrElse(rateEntry.product, &quot;NoName&quot;) + &quot; : &quot; + rateEntry.rating.toString) foreach println

</code></pre>

<h2 id="toc_12">split into Training set and CV set and calculate the AUC</h2>

<pre><code class="language-scala">val Array(trainData, cvData) = cleanedUserArtistData.randomSplit(Array(0.9, 0.1))
trainData.cache()
cvData.cache()
val model = ALS.trainImplicit(trainData, 10, 5, 0.01, 1.0)

// calculate the mean AUC for cvData
// 1. get the prediction on cvData using the model
val cvDataOfUserProduct = cvData map (record =&gt; (record.user, record.product))
// predict take RDD[tuple2]
val predictRating = model.predict(cvDataOfUserProduct)

// construct (user, wrongproduct)
val allProduct = (cleanedUserArtistData map (_.product) distinct).collect()
val allProductSize = allProduct.size
val bAllProduct = sc.broadcast(allProduct)
// using mapPartitions and broadcast to save memory, groupBy shuffles the RDD partitions, the shuffled RDD will have the same groupBy key records together inside one partition.

cvData groupBy (_.user) mapPartitions 
    {case (user, ratings) =&gt; {
        val correctProductSet = (ratings map (_.product)).collect().toSet
        var counter = 0
        val randomGen = new scala.util.Random()
        val wrongProductArr = new ArrayBuffer[Int]()
        while (counter &lt; correctProductSet.size) {
            val productCandidate = bAllProduct.value(randomGen.nextInt(allProductSize))
            if (!correctProductSet.contains(productCandidate)) {
                counter += 1
                wrongProductArr += productCandidate
            }
        }
    }
    wrongProductArr
}

        
            
            
                        



</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">Souce Code Link:</a>
</li>
<li>
<a href="#toc_1">Chapter 1 and 2</a>
<ul>
<li>
<a href="#toc_2">Start REPL</a>
</li>
<li>
<a href="#toc_3">Quick tricks to use</a>
</li>
<li>
<a href="#toc_4">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_5">Note about the <code>getOrElse()</code> method in the above map:</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Chapter 3</a>
<ul>
<li>
<a href="#toc_7">Download dataset and put on HDFS and start REPL</a>
</li>
<li>
<a href="#toc_8">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_9">The use of mllib ALS</a>
</li>
<li>
<a href="#toc_10">Rating</a>
</li>
<li>
<a href="#toc_11">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_12">split into Training set and CV set and calculate the AUC</a>
</li>
</ul>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala]]></title>
    <link href="www.echo-ohce.com/14767220898143.html"/>
    <updated>2016-10-17T09:34:49-07:00</updated>
    <id>www.echo-ohce.com/14767220898143.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Convert between regular collection and parallel collection</a>
</li>
<li>
<a href="#toc_1">function / method handler</a>
</li>
<li>
<a href="#toc_2">Get the type of variable in REPL</a>
</li>
<li>
<a href="#toc_3">Span method</a>
</li>
<li>
<a href="#toc_4">flatMap and Option</a>
</li>
<li>
<a href="#toc_5">Patter Matching with default</a>
</li>
<li>
<a href="#toc_6">Pattern matching in array assignment, variable name must start with lower case letter</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Convert between regular collection and parallel collection</h1>

<p>Using Seq as example. Use <code>.par</code> to convert from regular to parallel, and use <code>.seq</code> to convert back from parallel to regular.</p>

<p>Special note that <code>.toSeq</code>, <code>toMap</code> or <code>toSet</code> are converting between various collection but stay in the same parallel or regular domain.</p>

<p><a href="http://stackoverflow.com/a/12023096/4229125">stackoverflow post</a></p>

<h1 id="toc_1">function / method handler</h1>

<p>look at the following example:</p>

<pre><code class="language-scala">val test = Array(&quot;1.0&quot;, &quot;2.5&quot;, &quot;3.14159&quot;)

//Wrong
test map toDouble
// Because this is wrong
toDouble(test(2))
//correct
test map (_.toDouble)
// Because this is correct
test(2).toDouble

//Wrong
test map parseDouble
//correct
test map java.lang.Double.parseDouble
// Because this is correct
java.lang.Double.parseDouble(test(2))
</code></pre>

<h1 id="toc_2">Get the type of variable in REPL</h1>

<p><code>variableName.getClass.getSimpleName</code></p>

<h1 id="toc_3">Span method</h1>

<p><a href="https://www.garysieling.com/blog/scala-span-example">A good post</a><br/>
The span method lets you split a stream into two parts, by providing a function that detects the dividing line where you want the split to occur. What this doesn’t let you do is to sift through the stream and move each record into the a or b side – you can’t use this to separate even and odd numbers, for instance.</p>

<p>Example:</p>

<pre><code class="language-scala">var (a, b) = Stream.from(1).span(_ &lt; 10)
a: scala.collection.immutable.Stream[Int] = Stream(1, ?)
b: scala.collection.immutable.Stream[Int] = Stream(10, ?)
</code></pre>

<p>From this, it appears that it has actually run through 10 elements immediately, not waiting until you actually need one.</p>

<p>We can see it returns what we’d expect:</p>

<pre><code class="language-scala">scala&gt; a.take(10).toList
res26: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9)

scala&gt; b.take(10).toList
res27: List[Int] = List(10, 11, 12, 13, 14, 15, 16, 17, 18, 19)
</code></pre>

<p>To show that the even/odd filtering doesn’t work, see the example below. Since the first element is “1” (odd) it automatically goes to the “b” side, and you get an empty list for “a”.</p>

<pre><code class="language-scala">scala&gt; var (a, b) = Stream.from(1).span(_ % 2 == 0)
a: scala.collection.immutable.Stream[Int] = Stream()
b: scala.collection.immutable.Stream[Int] = Stream(1, ?)
</code></pre>

<h1 id="toc_4">flatMap and Option</h1>

<p><code>flatMap</code> on <code>Seq[Option]</code> effectively filter out the <code>None</code>s.<br/>
The keypoint of understanding Option is consider it as a Collection with 1 or 0 element. When using the <code>flatMap</code> it explode the inner collection and put all elements from the inner collection into the one big outer collection.</p>

<p><a href="http://danielwestheide.com/blog/2012/12/19/the-neophytes-guide-to-scala-part-5-the-option-type.html">A good reference post</a></p>

<h1 id="toc_5">Patter Matching with default</h1>

<pre><code class="language-scala">something match {
    case &quot;val&quot; =&gt; &quot;default&quot;
    case default =&gt; somefunction(default)
}
</code></pre>

<p>It is not a keyword, just an alias, so this will work as well:</p>

<pre><code class="language-scala">something match {
    case &quot;val&quot; =&gt; &quot;default&quot;
    case everythingElse =&gt; somefunction(everythingElse)
}
</code></pre>

<p>Or using <code>_</code></p>

<pre><code class="language-scala">something match {
    case &quot;val&quot; =&gt; &quot;default&quot;
    case _ =&gt; doSomething()
}
</code></pre>

<h1 id="toc_6">Pattern matching in array assignment, variable name must start with lower case letter</h1>

<p><a href="http://stackoverflow.com/a/8204231/4229125">stackoverflow post</a></p>

<p>If the variable name starts with capital letter, Scala treat it as a Constant</p>

<p>Wrong:</p>

<pre><code class="language-scala">val Array(trainData, CVData) = cleanedUserArtistData.randomSplit(Array(0.9, 0.1))
</code></pre>

<p>Correct:</p>

<pre><code class="language-scala">val Array(trainData, cvData) = cleanedUserArtistData.randomSplit(Array(0.9, 0.1))
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">Convert between regular collection and parallel collection</a>
</li>
<li>
<a href="#toc_1">function / method handler</a>
</li>
<li>
<a href="#toc_2">Get the type of variable in REPL</a>
</li>
<li>
<a href="#toc_3">Span method</a>
</li>
<li>
<a href="#toc_4">flatMap and Option</a>
</li>
<li>
<a href="#toc_5">Patter Matching with default</a>
</li>
<li>
<a href="#toc_6">Pattern matching in array assignment, variable name must start with lower case letter</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[maven]]></title>
    <link href="www.echo-ohce.com/14742423250660.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250660.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">build jar include dependencies</a>
</li>
<li>
<a href="#toc_1">hadoop-common, hadoop-core, hadoop-client</a>
<ul>
<li>
<a href="#toc_2">Another example just found from giraph jar</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Maven build skip Test compilation</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">build jar include dependencies</h1>

<p>For the <code>pom.xml</code> use the <code>maven-assembly-plugin</code>. The lines with respect to this plugin is like follow:  </p>

<pre><code class="language-xml">&lt;build&gt;
   &lt;plugins&gt;
       &lt;plugin&gt;
           &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
           &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
           &lt;configuration&gt;
               &lt;descriptorRefs&gt;
                   &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
               &lt;/descriptorRefs&gt;
           &lt;/configuration&gt;
       &lt;/plugin&gt;
   &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p>The import line is <code>&lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</code>. This means to include and pack all dependencies into the project jar.</p>

<p>the maven command to run is:  </p>

<pre><code class="language-bash">mvn clean package assembly:single -DskipTests
</code></pre>

<h1 id="toc_1">hadoop-common, hadoop-core, hadoop-client</h1>

<p>In order to build a Hadoop map-reduce application you need only hadoop client dependency. (Use new API). Dependencies like hadoop-hdfs,hadoop-common,hadoop-clientapp,hadoop-yarn-api are resolved from this.</p>

<p>To help provide some additional details regarding the differences between Hadoop-common, Hadoop-core and Hadoop-client, from a high-level perspective:</p>

<ul>
<li>Hadoop-common refers to the commonly used utilities and libraries that support the Hadoop modules.</li>
<li>Hadoop-core is the same as Hadoop-common; It was renamed to Hadoop-common in July 2009.</li>
<li>Hadoop-client refers to the client libraries used to communicate with Hadoop&#39;s common components (HDFS, MapReduce, YARN) including but not limited to logging and codecs for example.</li>
</ul>

<p>Generally speaking, for developers who build apps that submit to YARN, run a MR job, or access files from HDFS use Hadoop-client libraries.</p>

<p><a href="http://stackoverflow.com/a/34229580/4229125">stackoverflow post</a></p>

<h2 id="toc_2">Another example just found from giraph jar</h2>

<p>Error message like:</p>

<blockquote>
<p>java.lang.NoSuchMethodError: org.apache.hadoop.ipc.RPC.getServer</p>
</blockquote>

<p>This is because the <code>pom.xml</code> had <code>hadoop-common</code> dependency, should change to <code>hadoop-client</code></p>

<h1 id="toc_3">Maven build skip Test compilation</h1>

<p><code>-DskipTests</code> just skips the test execution: the tests are still compiled.<br/>
<code>-Dmaven.test.skip</code> skips both compilation and execution of the tests.</p>

<p>I need this because I have .gitignore of all the test folders.</p>

<p>This is the command to be used:</p>

<pre><code class="language-bash">mvn clean package assembly:single -Dmaven.test.skip
</code></pre>

<p><a href="http://stackoverflow.com/a/2593834/4229125">stackoverflow post</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">build jar include dependencies</a>
</li>
<li>
<a href="#toc_1">hadoop-common, hadoop-core, hadoop-client</a>
<ul>
<li>
<a href="#toc_2">Another example just found from giraph jar</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Maven build skip Test compilation</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Graphviz Dot]]></title>
    <link href="www.echo-ohce.com/14773416332741.html"/>
    <updated>2016-10-24T13:40:33-07:00</updated>
    <id>www.echo-ohce.com/14773416332741.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Good Resources</a>
</li>
<li>
<a href="#toc_1">Fix the node locations after adding new edges</a>
</li>
<li>
<a href="#toc_2">Layout</a>
</li>
<li>
<a href="#toc_3">command line of dot</a>
</li>
<li>
<a href="#toc_4">Using of color</a>
</li>
<li>
<a href="#toc_5">Maybe better alternative</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Good Resources</h1>

<ol>
<li><a href="http://www.graphviz.org/doc/info/attrs.html">Node, Edge and Graph Attributes - API doc to find parameters to use</a></li>
<li><a href="http://www.tonyballantyne.com/graphs.html#sec-5-2">A good tutorial, very simple and easy</a></li>
<li><a href="http://graphs.grevian.org/reference">Another very quick reference site</a> Some graph showing on the website are not using the default <code>-Kdot</code> engine.</li>
</ol>

<h1 id="toc_1">Fix the node locations after adding new edges</h1>

<p>For example, I made a graph G1, and I want to show that after some steps new edges are added to the graph G2.</p>

<p>Use neato layout engine as following steps </p>

<ol>
<li>Using normal way to generate the graph G1 (normal .dot file)</li>
<li>Output the G1 in <code>gv</code> format<br/>
<code>dot -Tgv -Kdot tinygraphOriginal.dot -o original.gv</code> </li>
<li>Open the <code>gv</code> file and adding edges</li>
<li>Generate the new graph G2 by<br/>
<code>dot -Tpng -Kneato -n original.gv -o C3.png</code></li>
</ol>

<h1 id="toc_2">Layout</h1>

<p>Couple tricks to use: </p>

<ol>
<li>Using subgrap:
<code>clusterrank=local</code>
<code>subgraph cluster_***</code><br/></li>
<li>Using <code>[style=invis]</code> for invisible nodes and edges to align</li>
<li>Using <code>rank=same</code>, <code>group</code> etc</li>
<li>Using <code>weight</code> properties of edges</li>
</ol>

<h1 id="toc_3">command line of dot</h1>

<p>The <a href="http://graphviz.org/content/command-line-invocation">complete arguments</a> are listed here. But the most frequently used one is:</p>

<pre><code class="language-bash">dot -Tpng -Kdot tinygraphOriginal.dot -o original.png
</code></pre>

<h1 id="toc_4">Using of color</h1>

<ul>
<li>In the graph top level specify the color scheme</li>
</ul>

<pre><code class="language-dot">strict graph {
    label=&quot;Original C1 graph&quot;;
    nodesep=2.0
    splines=line;
    colorscheme=&quot;Brewer&quot;;
    ...
    
</code></pre>

<ul>
<li>use color and fontcolor, also use the form <code>/xxx/#</code> to specify the detailed palette and color.</li>
</ul>

<pre><code class="language-dot">1 -- 2 [ label=&quot;0.1&quot;, dir=both, color=&quot;/set312/1&quot;, fontcolor=&quot;/set312/1&quot;];
</code></pre>

<ul>
<li><a href="http://www.graphviz.org/doc/info/colors.html">List of available colors are here</a></li>
</ul>

<h1 id="toc_5">Maybe better alternative</h1>

<p>For small one time use graph, maybe use <code>Omnigraffle</code> is a much better choice.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[intellij]]></title>
    <link href="www.echo-ohce.com/14765768181414.html"/>
    <updated>2016-10-15T17:13:38-07:00</updated>
    <id>www.echo-ohce.com/14765768181414.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, shortcuts I learned. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">List of most frequently used shortcut:</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">List of most frequently used shortcut:</h1>

<ul>
<li>Jump to previous location (navigate back):

<ul>
<li>cmd + [</li>
<li>cmd + ]</li>
</ul></li>
<li> </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[giraph with jython]]></title>
    <link href="www.echo-ohce.com/14742423250588.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250588.html</id>
    <content type="html"><![CDATA[
<p>This post specifically documents the key point of using giraph with jython. Once everything is done, I will write another post for a tutorial with giraph + jython</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Hadoop version</a>
</li>
<li>
<a href="#toc_1">Use <code>jython-standalone</code> dependency not <code>jython</code></a>
</li>
<li>
<a href="#toc_2">Run from jython REPL</a>
</li>
<li>
<a href="#toc_3">Config intellij for Jython and Java</a>
</li>
<li>
<a href="#toc_4">Run giraph jython from Hadoop Yarn</a>
</li>
<li>
<a href="#toc_5">pitfall of <code>TypeHolder</code> and <code>TypesHolder</code></a>
</li>
<li>
<a href="#toc_6">python and java data type</a>
</li>
<li>
<a href="#toc_7">debug print statement inside python file</a>
</li>
<li>
<a href="#toc_8">Jython type to java pitfall</a>
</li>
<li>
<a href="#toc_9">import python package or jython package</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Hadoop version</h1>

<p>In order to use Jython, in the maven pom.xml setting up, do <font color='red'><strong>NOT</strong></font> include other hadoop dependencies, but use the below specific version of hadoop:  </p>

<pre><code class="language-xml">&lt;dependencies&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.apache.giraph&lt;/groupId&gt;
       &lt;artifactId&gt;giraph-core&lt;/artifactId&gt;
       &lt;version&gt;1.1.0-hadoop2&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
       &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
       &lt;version&gt;2.5.0&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.python&lt;/groupId&gt;
       &lt;artifactId&gt;jython-standalone&lt;/artifactId&gt;
       &lt;version&gt;2.7.0&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;junit&lt;/groupId&gt;
       &lt;artifactId&gt;junit&lt;/artifactId&gt;
       &lt;version&gt;4.11&lt;/version&gt;
       &lt;scope&gt;test&lt;/scope&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.mockito&lt;/groupId&gt;
       &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;
       &lt;scope&gt;test&lt;/scope&gt;
       &lt;version&gt;1.9.5&lt;/version&gt;
   &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>

<p>Otherwise, there will be error message like:</p>

<blockquote>
<p>Exception in thread &quot;main&quot; java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected`  </p>
</blockquote>

<p><a href="http://stackoverflow.com/questions/22630323/hadoop-java-lang-incompatibleclasschangeerror-found-interface-org-apache-hadoo">Because Hadoop 1.0.3: JobContext is a class, but in Hadoop 2.0.0: JobContext is an interface</a></p>

<p>Most important points in this pom are:</p>

<ul>
<li>use giraph version of <code>1.1.0-hadoop2</code> because nowadays we are all using hadoop 2.</li>
<li>use <code>jython-standalone</code> not <code>jython</code> (see below)</li>
</ul>

<h1 id="toc_1">Use <code>jython-standalone</code> dependency not <code>jython</code></h1>

<p>Otherwise it will show error message like:  </p>

<blockquote>
<p>Exception in thread &quot;main&quot; ImportError: Cannot import site module and its dependencies: No module named site`  </p>
</blockquote>

<p>This is because <code>jython-standalone-2.7.0.jar</code> has <code>Lib</code> folder but <code>jython-2.7.0.jar</code> does not have that folder.  </p>

<p>Another possible issue about jython with error message like:  </p>

<blockquote>
<p>java.lang.IncompatibleClassChangeError: Found class com.kenai.jffi.InvocationBuffer, but interface was expected  </p>
</blockquote>

<p>is also about the jar of jython. Using <a href="https://github.com/scijava/jython-shaded"><code>jython-shade</code></a> will solve this issue.   </p>

<h1 id="toc_2">Run from jython REPL</h1>

<ul>
<li>For installing the jython, remeber to add the following path to <code>.bashrc</code><br/></li>
</ul>

<pre><code class="language-bash">export PATH=/Users/yugan/jython2.7.0/bin:$PATH
</code></pre>

<ul>
<li>Need import the following two pathes, in order to get the java <code>import</code> work.<br/></li>
</ul>

<pre><code class="language-python">import sys
sys.path.append(&quot;/Users/yugan/.m2/repository/org/apache/giraph/giraph-core/1.1.0/giraph-core-1.1.0-hadoop_2_6_dependencies.jar&quot;)
sys.path.append(&quot;/Users/yugan/.m2/repository/org/apache/giraph/giraph-core/1.1.0/giraph-core-1.1.0.jar&quot;)
sys.path.append(&quot;/Users/yugan/.m2/repository/org/apache/hadoop/hadoop-core/2.5.0/hadoop-core-2.5.0.jar&quot;)

from org.apache.giraph.jython import JythonJob
</code></pre>

<h1 id="toc_3">Config intellij for Jython and Java</h1>

<ol>
<li>using <code>command + ;</code> to call out the project setting.</li>
<li>project SDK should be java (to recognize java packages etc)</li>
<li>In the <code>Facets</code>, add Python framework, and config <code>python interpreter</code> as <code>jython</code></li>
</ol>

<h1 id="toc_4">Run giraph jython from Hadoop Yarn</h1>

<p>Got a working version using my own <code>graph-distance.py</code> example.  </p>

<ol>
<li>python file can be put anywhere, but I put it under the <code>resources</code> folder at:<br/>
<code>/src/main/resources/graph-distance.py</code><br/>
In this python file, it builds the python class <code>GraphDistance</code> which subclasses giraph&#39;s <code>JythonComputation</code>. There is no way that we can give the type parameters (generics), but we are going to handle it in a different way later.<br/></li>
<li>the graph input file can be anywhere, but I put it under the <code>resources</code> folder too.<br/>
It is put on hdfs.<br/></li>
<li>To specify the type parameters of <code>&lt;I, V, E, M, M&gt;</code>, I built the <code>JythonTypes</code> class which implements the <code>TypesHolder</code> interface.</li>
<li>I also created the corresponding <code>I V E M</code> classes if it is necessary.</li>
<li>Build the jar file which need include the custom build type classes, and also the original giraph classes. I used the <code>maven-assembly-plugin</code> to build the jar with all dependencies.</li>
<li>In the working folder on the cluster, we have the <u>python</u> file and the <u>jar</u> file (<code>giraph-jython-1.0-SNAPSHOT-jar-with-dependencies.jar</code>). Then run the shell script.</li>
</ol>

<p>The pom.xml file and the shell runner file:</p>

<pre><code class="language-xml">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;com.adsymp.dpp.giraph.jython&lt;/groupId&gt;
    &lt;artifactId&gt;giraph-jython&lt;/artifactId&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;giraph-jython&lt;/name&gt;

    &lt;parent&gt;
        &lt;groupId&gt;com.adsymp&lt;/groupId&gt;
        &lt;artifactId&gt;base-pom&lt;/artifactId&gt;
        &lt;version&gt;0.0.1&lt;/version&gt;
    &lt;/parent&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.giraph&lt;/groupId&gt;
            &lt;artifactId&gt;giraph-core&lt;/artifactId&gt;
            &lt;version&gt;1.1.0-hadoop2&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;2.5.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.python&lt;/groupId&gt;
            &lt;artifactId&gt;jython-standalone&lt;/artifactId&gt;
            &lt;version&gt;2.7.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.11&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.mockito&lt;/groupId&gt;
            &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
            &lt;version&gt;1.9.5&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;descriptorRefs&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre>

<p>Build the jar:</p>

<pre><code class="language-bash">mvn clean package assembly:single -DskipTests
</code></pre>

<p>run the shell script:</p>

<pre><code class="language-bash">hdfs dfs -rm -r giraph-jython/output
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

yarn jar giraph-jython-1.0-SNAPSHOT-jar-with-dependencies.jar \
        com.adsymp.dpp.giraph.jython.GiraphJythonRunner \
        -Dmapred.job.queue.name=datascience \
        -Dgiraph.zkList=$ZKLIST \
        graph-distance.py \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        --jythonClass GraphDistance \
        --typesHolder com.adsymp.dpp.giraph.jython.JythonTypes \
        -eif org.apache.giraph.io.formats.IntNullTextEdgeInputFormat \
        -eip giraph-jython/input/tiny_graph \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op giraph-jython/output \
        -w 1 \
        -ca giraph.SplitMasterWorker=false
</code></pre>

<h1 id="toc_5">pitfall of <code>TypeHolder</code> and <code>TypesHolder</code></h1>

<p>Be careful, the <code>TypesHolder</code> interface (which is what we need supply to the cmd) is different than the <code>TypeHolder</code>.</p>

<ul>
<li>The first one is at <code>org.apache.giraph.conf.TypesHolder</code> - Interface for classes that are parameterized by all of the Giraph types.<br/></li>
<li>The second one is an inner class at <code>org.apache.giraph.jython.JythonJob</code></li>
</ul>

<h1 id="toc_6">python and java data type</h1>

<p>Converting python data type to java data type has a lot of limitations.</p>

<p>For now, data come from and come into java are all in the type of array(). To use it in python, convert it to list using array.tolist() method.<br/>
To pass it to java, convert it to list then use array(list, <code>typecode</code>) <a href="http://www.jython.org/jythonbook/en/1.0/DataTypes.html#jython-specific-collections">Typecode</a> refers to Table 2-8</p>

<pre><code class="language-python">from org.python.modules.jarray import array

</code></pre>

<h1 id="toc_7">debug print statement inside python file</h1>

<p>Very tricky that the standard python way of print will not work in the python file.</p>

<p><strong>Wrong</strong>:  </p>

<pre><code class="language-python">print &#39;vertext_id: {0:d} received {1:d} messages&#39;.format(int(v_id), count)
</code></pre>

<p><strong>Correct</strong></p>

<pre><code class="language-python">print &#39;vertext_id: %d received %d messages&#39; % (int(v_id), count)
</code></pre>

<h1 id="toc_8">Jython type to java pitfall</h1>

<p>One subtle bug (very hard to debug) is the <code>Writable</code> type. When directly calling giraph&#39;s java method, such as <code>sendMessage(vertexID, MessageOut)</code>, the type of <code>vertexID</code> should be <code>LongWritable</code>, but inside jython, the default type is just <code>long</code>, and the framework will not automatically wrap it for you.</p>

<p><strong>Wrong</strong>  </p>

<pre><code class="language-python">self.sendMessage(id, outgoing_Message)
</code></pre>

<p><strong>Correct</strong>  </p>

<pre><code class="language-python">from org.apache.hadoop.io import LongWritable

self.sendMessage(LongWritable(id), outgoing_Message)
</code></pre>

<p>Why it is so hard to debug? Because there is <strong>NO</strong> error message! try add some print statement inside python file to check the type.</p>

<h1 id="toc_9">import python package or jython package</h1>

<p>At least for my project setting (intellij with jython facets), import from python does not work well.</p>

<p><strong>Work</strong>  </p>

<pre><code class="language-python">from org.python.modules.itertools import itertools

for id1, id2 in itertools.combinations(newSeenIds, 2):
</code></pre>

<p><strong>Not work</strong>  </p>

<pre><code class="language-python">import itertools

for id1, id2 in itertools.combinations(newSeenIds, 2):
</code></pre>

<p><strong>Work</strong>  </p>

<pre><code class="language-python">from java.util import Random

@staticmethod
    def isSampledID(id):
        random = Random(id)
        return random.nextInt(GraphDistance.sample) == 0
</code></pre>

<p><strong>Not work</strong>  </p>

<pre><code class="language-python">import random

@staticmethod
    def isSampledID(id):
        
        return random.randint(1, GraphDistance.sample) == 1
</code></pre>

<p>In favor of import java modules. some python extended libraries is not included in jython. <a href="http://forums.parasoft.com/index.php?showtopic=2302#">refer to this post</a> A side note is that java&#39;s random is exclusive, but jython&#39;s random is inclusive.</p>

<hr/>

<ul>
<li>
<a href="#toc_0">Hadoop version</a>
</li>
<li>
<a href="#toc_1">Use <code>jython-standalone</code> dependency not <code>jython</code></a>
</li>
<li>
<a href="#toc_2">Run from jython REPL</a>
</li>
<li>
<a href="#toc_3">Config intellij for Jython and Java</a>
</li>
<li>
<a href="#toc_4">Run giraph jython from Hadoop Yarn</a>
</li>
<li>
<a href="#toc_5">pitfall of <code>TypeHolder</code> and <code>TypesHolder</code></a>
</li>
<li>
<a href="#toc_6">python and java data type</a>
</li>
<li>
<a href="#toc_7">debug print statement inside python file</a>
</li>
<li>
<a href="#toc_8">Jython type to java pitfall</a>
</li>
<li>
<a href="#toc_9">import python package or jython package</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[vim]]></title>
    <link href="www.echo-ohce.com/14742423250782.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250782.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">move (jump) cursor to previous location or previous files</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">move (jump) cursor to previous location or previous files</h1>

<table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>ma</code></td>
<td>set mark <font color='red'>a</font> at current cursor location</td>
</tr>
<tr>
<td><code>&#39;a</code> - quotemark</td>
<td>jump to line of mark <font color='red'>a</font> (first non-blank character in line)</td>
</tr>
<tr>
<td><code>&#39;a</code> - backtick</td>
<td>jump to position (line and column) of mark <font color='red'>a</font></td>
</tr>
<tr>
<td><code>&#39;&#39;</code> - 2 quotemark</td>
<td>jump to the head of the line of previous location</td>
</tr>
<tr>
<td><code>``</code> -2 backtick</td>
<td>jump to the previous location</td>
</tr>
<tr>
<td><code>:marks</code></td>
<td>list all current marks (including marks of <strong>other files</strong>)</td>
</tr>
<tr>
<td><code>:help mark-motions</code></td>
<td>help for mark related stuff</td>
</tr>
<tr>
<td><code>:help jump-motions</code></td>
<td>help for jump related stuff</td>
</tr>
</tbody>
</table>

<p>reference <a href="http://vim.wikia.com/wiki/Using_marks">using marks</a><br/>
reference <a href="http://stackoverflow.com/questions/5052079/move-cursor-to-its-last-position">stackoverflow</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">move (jump) cursor to previous location or previous files</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[setting]]></title>
    <link href="www.echo-ohce.com/14742423250740.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250740.html</id>
    <content type="html"><![CDATA[
<p>This documents various settings that I normally use. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">python notebook imports and setting</a>
</li>
<li>
<a href="#toc_1">Mac multiple desktop screen spaces setting.</a>
</li>
<li>
<a href="#toc_2">python conda virtual environment and jupyter notebook kernel</a>
</li>
<li>
<a href="#toc_3">Mac OS gets very busy on background process</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">python notebook imports and setting</h1>

<pre><code class="language-python">import os
import pandas as pd
import numpy as np
import math
import plotly.plotly as py
import plotly.offline as po
import plotly.graph_objs as go
import plotly.tools as tls
import colorlover as cl
import cufflinks as cf
import subprocess
from ipywidgets import interact
import glob # for shell expansion
from IPython.display import display, HTML, Image
import matplotlib.pyplot as plt

%matplotlib inline

po.init_notebook_mode()
cf.set_config_file(theme=&#39;white&#39;)
pd.set_option(&#39;precision&#39;, 3) # printing precision
np.set_printoptions(precision=3) # printing precision 
cf.go_offline() # using cufflinks at offline mode
</code></pre>

<h1 id="toc_1">Mac multiple desktop screen spaces setting.</h1>

<p><a href="http://www.tweaking4all.com/os-tips-and-tricks/macosx-tips-and-tricks/mac-multiple-desktops-spaces/">Very good post</a><br/><br/>
Using <code>SteerMouse</code> to set the mouse shortcut.</p>

<h1 id="toc_2">python conda virtual environment and jupyter notebook kernel</h1>

<p>Need install a plugin <code>nb_conda_kernels</code></p>

<ol>
<li>switch to the desired virtual environment</li>
<li>install plugin</li>
<li>run jupyter notebook then pick the desired kernel from notebook</li>
</ol>

<pre><code class="language-bash">source activate environment_name
conda install -c conda-forge nb_conda_kernels
jupyter notebook

source deactive
</code></pre>

<p><a href="https://github.com/Anaconda-Platform/nb_conda_kernels">plugin&#39;s main page</a><br/>
<a href="http://stackoverflow.com/a/38880722">stackoverflow post</a></p>

<h1 id="toc_3">Mac OS gets very busy on background process</h1>

<ol>
<li>It is because of <code>optimal Layout</code></li>
<li>The app to check the system performance is <code>Activity Monitor</code></li>
<li>There is another process named <code>distnoted</code> taking a lot of CPU resources. This is Apple&#39;s kernel process, but it is safely to be removed. Using the script (saved as <code>install_checkdistnoted.sh</code> add a cron job to kill this process when it takes too much resources.<br/>
See the <a href="http://apple.stackexchange.com/a/234478">original post for this scirpt</a><br/></li>
</ol>

<hr/>

<ul>
<li>
<a href="#toc_0">python notebook imports and setting</a>
</li>
<li>
<a href="#toc_1">Mac multiple desktop screen spaces setting.</a>
</li>
<li>
<a href="#toc_2">python conda virtual environment and jupyter notebook kernel</a>
</li>
<li>
<a href="#toc_3">Mac OS gets very busy on background process</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[metrics]]></title>
    <link href="www.echo-ohce.com/14742423250680.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250680.html</id>
    <content type="html"><![CDATA[
<p>This document collects statistics metrics used in my work.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">precision</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">precision</h1>

<p><img src="media/14742423250680/14742450671711.jpg" alt=""/></p>

<hr/>

<ul>
<li>
<a href="#toc_0">precision</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Github Page, Hexo, MWeb, GoDaddy setting]]></title>
    <link href="www.echo-ohce.com/14742423250557.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250557.html</id>
    <content type="html"><![CDATA[
<p>I tried to use MWeb + Hexo to setup this blog, but it does not work out well. Give this up, turn back to MWeb only</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Set two separate GitLab (Corp) and GitHub (Personal) account</a>
</li>
<li>
<a href="#toc_1">Github page</a>
</li>
<li>
<a href="#toc_2">Hexo</a>
</li>
<li>
<a href="#toc_3">Freemind Hexo theme</a>
</li>
<li>
<a href="#toc_4">Mweb</a>
</li>
<li>
<a href="#toc_5">GoDaddy</a>
</li>
<li>
<a href="#toc_6">Hexo works with MathJax / LaTex</a>
</li>
<li>
<a href="#toc_7">Future work</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Set two separate GitLab (Corp) and GitHub (Personal) account</h1>

<p>The most important steps are:<br/>
1. Edit <code>~/.ssh/config</code> file and set up the two hosts.<br/>
2. Generate two separate ssh keys and put them in the folder <code>~/.ssh/</code><br/>
3. The main account is set by <code>git config --global</code> and the other account is set by locally <code>git config</code> overridden. </p>

<p>The main reference post is <a href="https://gist.github.com/rosswd/e1afd2b0b0d515517eac">here</a></p>

<h1 id="toc_1">Github page</h1>

<p>Nothing special, just first need first create an empty repository using the unique name required.</p>

<h1 id="toc_2">Hexo</h1>

<p>Hexo is used to build everything and managers everything. <br/>
It sets the local folder structures, manages the style / theme / plugin, generates the sites, publish to the github pages. Some important things about hexo</p>

<ol>
<li><a href="">hexo installation and integration with github pages</a></li>
<li><a href="https://hexo.io/docs/index.html">Hexo basic commands</a></li>
<li><a href="https://hexo.io/docs/front-matter.html">Hexo Front-matter</a>. The <code>freemind</code> theme that I am using provides extra Front-matter: <code>toc</code> and <code>top</code></li>
<li>[YGMARK] in <code>_config.yml</code> setting, after <code>key:</code> a space must be used, otherwise all sorts of bugs.</li>
</ol>

<h1 id="toc_3">Freemind Hexo theme</h1>

<ul>
<li><a href="https://github.com/wzpan/hexo-theme-freemind/">Installation Official</a></li>
<li><a href="http://baoxiehao.com/2014/05/17/Hexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96/">非常好的一篇博客，详细说明了Freemind Hexo的优化。有时间再仔细看</a></li>
<li><a href="http://masikkk.com/blog/hexo-4-usage-of-hexo-theme/">一篇中文的博客，更详细，并提到freemind search engine的设置</a></li>
</ul>

<h1 id="toc_4">Mweb</h1>

<p>Mweb is simply the Markdown writing program.<br/>
It has a very good full <a href="http://www.mweb.im/markdown-syntax-guide-full-version.html">Markdown syntax guide</a>.<br/>
打开MWeb, ​Command + E 切换到外部模式(使用cmd + L可以切回默认的library模式), 然后把hexo下的source目录拖到左边, 设置它的Display name为github blog(随意), 最重要一点是设置Media Save Path为Absolute, 这样就可以直接使用MWeb的粘贴图片功能.<br/><br/>
<img src="media/14742423250557/14742438643424.jpg" alt=""/></p>

<h1 id="toc_5">GoDaddy</h1>

<ul>
<li>Add a <code>CNAME</code> file in the <code>source</code> folder: <code>www.echo-ohce.com</code></li>
<li><p>Go to GoDaddy and config the domain name</p>

<ul>
<li><p>Add an &quot;A (Host)&quot; record with &quot;host&quot; = <code>@</code> and &quot;Points to&quot; = <code>192.30.252.153</code> or <code>192.30.252.154</code><br/>
<img src="media/14742423250557/14742439346348.png" alt=""/></p></li>
<li><p>Create a CNAME record<br/><br/>
<img src="media/14742423250557/14742439461628.png" alt=""/></p></li>
</ul></li>
<li><p><a href="https://help.github.com/articles/setting-up-an-apex-domain/">Official help page</a></p></li>
<li><p><a href="http://andrewsturges.com/blog/jekyll/tutorial/2014/11/06/github-and-godaddy.html">My reference post</a></p></li>
<li><p>Specical Notes: 在CNAME中的域名不需要<code>http://</code>。</p></li>
</ul>

<h1 id="toc_6">Hexo works with MathJax / LaTex</h1>

<ul>
<li>install plugin <a href="https://github.com/akfish/hexo-math">hexo-math</a> at the root folder of the blog <code>npm install hexo-math --save</code></li>
<li>Update the root <code>_config.xml</code> append the following:</li>
</ul>

<pre><code class="language-xml"># Math
math:
  engine: &#39;mathjax&#39;
  mathjax:
    src: &quot;//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;
    config:
      tex2jax:
        inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ]
        skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;, &#39;code&#39;]
        processEscaps: true
      TeX:
        equationNumbers:
          autoNumber: &quot;AMS&quot;
</code></pre>

<ul>
<li>Inline LaTex <code>$...$</code>: \(x \rightarrow y\)</li>
<li>Block <code>$$...$$</code>:  \[\cos 2\theta = \cos^2 \theta - \sin^2 \theta =  2 \cos^2 \theta - 1\]</li>
<li>Multiple lines <code>{% math %} ... {% endmath %}</code>:</li>
</ul>

<p>{% math %}<br/>
\begin{aligned}<br/>
\dot{x} &amp; = \sigma(y-x) \<br/>
\dot{y} &amp; = \rho x - y - xz \<br/>
\dot{z} &amp; = -\beta z + xy<br/>
\end{aligned}<br/>
{% endmath %}</p>

<h1 id="toc_7">Future work</h1>

<p>GitHub  \(\rightarrow\)  GitLab <br/>
1. <a href="https://yudachi.biz/2016/05/27/moving-to-gitlab-pages/">中文的 GitLab and Hexo</a><br/>
2. <a href="http://blog.vanjor.com/2016/05/hexo-github-blog-install.html">代码双线托管，域名DNS双线解析</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Set two separate GitLab (Corp) and GitHub (Personal) account</a>
</li>
<li>
<a href="#toc_1">Github page</a>
</li>
<li>
<a href="#toc_2">Hexo</a>
</li>
<li>
<a href="#toc_3">Freemind Hexo theme</a>
</li>
<li>
<a href="#toc_4">Mweb</a>
</li>
<li>
<a href="#toc_5">GoDaddy</a>
</li>
<li>
<a href="#toc_6">Hexo works with MathJax / LaTex</a>
</li>
<li>
<a href="#toc_7">Future work</a>
</li>
</ul>


]]></content>
  </entry>
  
</feed>
