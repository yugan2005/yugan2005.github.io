<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Echo-ohcE]]></title>
  <link href="www.echo-ohce.com/atom.xml" rel="self"/>
  <link href="www.echo-ohce.com/"/>
  <updated>2016-10-24T23:19:11-07:00</updated>
  <id>www.echo-ohce.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.coderforart.com/">CoderForArt</generator>

  
  <entry>
    <title type="html"><![CDATA[Advanced Analytics with Spark Reading Notes]]></title>
    <link href="www.echo-ohce.com/14770727250961.html"/>
    <updated>2016-10-21T10:58:45-07:00</updated>
    <id>www.echo-ohce.com/14770727250961.html</id>
    <content type="html"><![CDATA[
<p>This documents the reading notes of this book. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Chapter 1</a>
<ul>
<li>
<a href="#toc_1">Start REPL</a>
</li>
<li>
<a href="#toc_2">Quick tricks to use</a>
</li>
<li>
<a href="#toc_3">Inside REPL Follow along</a>
</li>
</ul>
</li>
</ul>


<hr/>

<h1 id="toc_0">Chapter 1</h1>

<h2 id="toc_1">Start REPL</h2>

<pre><code class="language-bash"># this works on hlab
spark-shell --master yarn-client

# this works on dblab
/home/xiaogu/repo/spark-1.6.1-bin-hadoop2.4/bin/spark-shell --queue datascience
</code></pre>

<h2 id="toc_2">Quick tricks to use</h2>

<ol>
<li>help: <code>:help</code></li>
<li>history / find the variable or function name: <code>:history</code> or <code>:h?</code></li>
<li>paste code: <code>:paste</code></li>
</ol>

<h2 id="toc_3">Inside REPL Follow along</h2>

<pre><code class="language-scala">val rawblocks = sc.textFile(&quot;linkage/*.csv&quot;)
rawblocks.first
val head = rawblocks.take(10)
head.length
head foreach println

def isHeader(line:String):Boolean =
    line.contains(&quot;id_1&quot;)

head filterNot isHeader foreach println
head filter (ele =&gt; ! isHeader(ele)) foreach println
head filter (!isHeader(_)) foreach println

// Note that spark RDD does not have filterNot method
val noheader = rawblocks filter (!isHeader(_))

def toDouble(token:String):Double = {
    if (token.equals(&quot;?&quot;)) Double.NaN;
    else token.toDouble;}
    
val line = head(5)

def parse(line:String):(Int, Int, Array[Double], Boolean) = {
    val tokens = line split (&#39;,&#39;);
    val id1 = tokens(0).toInt;
    val id2 = tokens(1).toInt;
    val scores:Array[Double] = tokens slice (2, 11) map toDouble;
    val matched = tokens(11).toBoolean;
    (id1, id2, scores, matched)}

val tup = parse(line)

case class MatchData(id1:Int, id2:Int, scores:Array[Double], matched:Boolean);

def parse(line:String):MatchData = {
    val tokens = line split (&#39;,&#39;);
    val id1 = tokens(0).toInt;
    val id2 = tokens(1).toInt;
    val scores:Array[Double] = tokens slice (2, 11) map toDouble;
    val matched = tokens(11).toBoolean;
    MatchData(id1, id2, scores, matched)}
    

val parsed = rawblocks filter (!isHeader(_)) map parse;
parsed.cache()

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[maven]]></title>
    <link href="www.echo-ohce.com/14742423250660.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250660.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">build jar include dependencies</a>
</li>
<li>
<a href="#toc_1">hadoop-common, hadoop-core, hadoop-client</a>
</li>
<li>
<a href="#toc_2">Maven build skip Test compilation</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">build jar include dependencies</h1>

<p>For the <code>pom.xml</code> use the <code>maven-assembly-plugin</code>. The lines with respect to this plugin is like follow:  </p>

<pre><code class="language-xml">&lt;build&gt;
   &lt;plugins&gt;
       &lt;plugin&gt;
           &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
           &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
           &lt;configuration&gt;
               &lt;descriptorRefs&gt;
                   &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
               &lt;/descriptorRefs&gt;
           &lt;/configuration&gt;
       &lt;/plugin&gt;
   &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p>The import line is <code>&lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</code>. This means to include and pack all dependencies into the project jar.</p>

<p>the maven command to run is:  </p>

<pre><code class="language-bash">mvn clean package assembly:single -DskipTests
</code></pre>

<h1 id="toc_1">hadoop-common, hadoop-core, hadoop-client</h1>

<p>In order to build a Hadoop map-reduce application you need only hadoop client dependency. (Use new API). Dependencies like hadoop-hdfs,hadoop-common,hadoop-clientapp,hadoop-yarn-api are resolved from this.</p>

<p>To help provide some additional details regarding the differences between Hadoop-common, Hadoop-core and Hadoop-client, from a high-level perspective:</p>

<ul>
<li>Hadoop-common refers to the commonly used utilities and libraries that support the Hadoop modules.</li>
<li>Hadoop-core is the same as Hadoop-common; It was renamed to Hadoop-common in July 2009.</li>
<li>Hadoop-client refers to the client libraries used to communicate with Hadoop&#39;s common components (HDFS, MapReduce, YARN) including but not limited to logging and codecs for example.</li>
</ul>

<p>Generally speaking, for developers who build apps that submit to YARN, run a MR job, or access files from HDFS use Hadoop-client libraries.</p>

<p><a href="http://stackoverflow.com/a/34229580/4229125">stackoverflow post</a></p>

<h1 id="toc_2">Maven build skip Test compilation</h1>

<p><code>-DskipTests</code> just skips the test execution: the tests are still compiled.<br/>
<code>-Dmaven.test.skip</code> skips both compilation and execution of the tests.</p>

<p>I need this because I have .gitignore of all the test folders.</p>

<p>This is the command to be used:</p>

<pre><code class="language-bash">mvn clean package assembly:single -Dmaven.test.skip
</code></pre>

<p><a href="http://stackoverflow.com/a/2593834/4229125">stackoverflow post</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">build jar include dependencies</a>
</li>
<li>
<a href="#toc_1">hadoop-common, hadoop-core, hadoop-client</a>
</li>
<li>
<a href="#toc_2">Maven build skip Test compilation</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Graphviz Dot]]></title>
    <link href="www.echo-ohce.com/14773416332741.html"/>
    <updated>2016-10-24T13:40:33-07:00</updated>
    <id>www.echo-ohce.com/14773416332741.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Good Resources</a>
</li>
<li>
<a href="#toc_1">Fix the node locations after adding new edges</a>
</li>
<li>
<a href="#toc_2">Layout</a>
</li>
<li>
<a href="#toc_3">command line of dot</a>
</li>
<li>
<a href="#toc_4">Using of color</a>
</li>
<li>
<a href="#toc_5">Maybe better alternative</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Good Resources</h1>

<ol>
<li><a href="http://www.graphviz.org/doc/info/attrs.html">Node, Edge and Graph Attributes - API doc to find parameters to use</a></li>
<li><a href="http://www.tonyballantyne.com/graphs.html#sec-5-2">A good tutorial, very simple and easy</a></li>
<li><a href="http://graphs.grevian.org/reference">Another very quick reference site</a> Some graph showing on the website are not using the default <code>-Kdot</code> engine.</li>
</ol>

<h1 id="toc_1">Fix the node locations after adding new edges</h1>

<p>For example, I made a graph G1, and I want to show that after some steps new edges are added to the graph G2.</p>

<p>Use neato layout engine as following steps </p>

<ol>
<li>Using normal way to generate the graph G1 (normal .dot file)</li>
<li>Output the G1 in <code>gv</code> format<br/>
<code>dot -Tgv -Kdot tinygraphOriginal.dot -o original.gv</code> </li>
<li>Open the <code>gv</code> file and adding edges</li>
<li>Generate the new graph G2 by<br/>
<code>dot -Tpng -Kneato -n original.gv -o C3.png</code></li>
</ol>

<h1 id="toc_2">Layout</h1>

<p>Couple tricks to use: </p>

<ol>
<li>Using subgrap:
<code>clusterrank=local</code>
<code>subgraph cluster_***</code><br/></li>
<li>Using <code>[style=invis]</code> for invisible nodes and edges to align</li>
<li>Using <code>rank=same</code>, <code>group</code> etc</li>
<li>Using <code>weight</code> properties of edges</li>
</ol>

<h1 id="toc_3">command line of dot</h1>

<p>The <a href="http://graphviz.org/content/command-line-invocation">complete arguments</a> are listed here. But the most frequently used one is:</p>

<pre><code class="language-bash">dot -Tpng -Kdot tinygraphOriginal.dot -o original.png
</code></pre>

<h1 id="toc_4">Using of color</h1>

<ul>
<li>In the graph top level specify the color scheme</li>
</ul>

<pre><code class="language-dot">strict graph {
    label=&quot;Original C1 graph&quot;;
    nodesep=2.0
    splines=line;
    colorscheme=&quot;Brewer&quot;;
    ...
    
</code></pre>

<ul>
<li>use color and fontcolor, also use the form <code>/xxx/#</code> to specify the detailed palette and color.</li>
</ul>

<pre><code class="language-dot">1 -- 2 [ label=&quot;0.1&quot;, dir=both, color=&quot;/set312/1&quot;, fontcolor=&quot;/set312/1&quot;];
</code></pre>

<ul>
<li><a href="http://www.graphviz.org/doc/info/colors.html">List of available colors are here</a></li>
</ul>

<h1 id="toc_5">Maybe better alternative</h1>

<p>For small one time use graph, maybe use <code>Omnigraffle</code> is a much better choice.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[bash]]></title>
    <link href="www.echo-ohce.com/14742423250478.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250478.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Using curly braces in Bash substitution</a>
</li>
<li>
<a href="#toc_1">nohup to a different file</a>
</li>
<li>
<a href="#toc_2">Append multiple lines to a file</a>
</li>
<li>
<a href="#toc_3">copy from bash shell to clipboard</a>
</li>
<li>
<a href="#toc_4">Hadoop hdfs get all except one folder (file)</a>
</li>
<li>
<a href="#toc_5">Hadoop hdfs / bash get matched folders</a>
</li>
<li>
<a href="#toc_6">check file encoding</a>
</li>
<li>
<a href="#toc_7">du without recursion</a>
</li>
<li>
<a href="#toc_8">checking running job and argument</a>
</li>
<li>
<a href="#toc_9">grep find negate</a>
</li>
<li>
<a href="#toc_10">grep find text in all files under a folder and all of its subfolders</a>
</li>
<li>
<a href="#toc_11">Keep the REPL across sessions</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Using curly braces in Bash substitution</h1>

<p>It is always a good practice to use curly braces like <code>${foo}</code>, but there are following situations it is a must:</p>

<ul>
<li>confusing strings <code>${foo}bar</code> vs. <code>$foobar</code> in which <code>foobar</code> is a single parameter.</li>
<li>expanding arrays <code>${array[42]}</code></li>
<li>expanding positional parameters beyond 9 <code>$8 $9 ${10}</code></li>
</ul>

<p><code>{}</code> is called <em>brace expansion</em>, <code>${}</code> is called <em>variable expansion</em><br/>
<a href="http://stackoverflow.com/questions/8748831/when-do-we-need-curly-braces-in-variables-using-bash">Reference Post</a>  </p>

<h1 id="toc_1">nohup to a different file</h1>

<pre><code class="language-bash">nohup some_command &gt; nohup_file.out 2&gt;&amp;1 &amp;
</code></pre>

<p>in which <code>2&gt;&amp;1</code> means <font color='red'>redirect <code>stderr</code> to the same output as <code>stdout</code></font>.  </p>

<h1 id="toc_2">Append multiple lines to a file</h1>

<p>Using a <em>eoi</em> (end of input) symbol for multiple lines</p>

<pre><code class="language-bash">cat &lt;&lt;EOI &gt;&gt; file
multiple lines
input here
EOI
</code></pre>

<p>in which, the <code>&lt;&lt;EOI</code> is registering a special symbol marking the end of input, which should be in a line by itself; the <code>&gt;&gt; file</code> means it is <font color='red'>append</font> to the file.</p>

<h1 id="toc_3">copy from bash shell to clipboard</h1>

<p>On mac, it is super easy: <code>pbcopy</code> and <code>pbpaste</code>.  </p>

<pre><code class="language-bash">cat ~/.bashrc | pbcopy
</code></pre>

<p>After that command content of the <code>~/.bashrc</code> file will be available for pasting with <code>cmd+v</code> shortcut.</p>

<h1 id="toc_4">Hadoop hdfs get all except one folder (file)</h1>

<p>As noted before, Hadoop use its own <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html#globStatus%28org.apache.hadoop.fs.Path%29">glob</a>. to do the pattern matching for file name and paths.</p>

<pre><code class="language-bash">$ hls alibaba/ipstats/
Found 8 items
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:36 alibaba/ipstats/IP
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/acookie_day
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/avg_acookie_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/avg_umid_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_acookie_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_day
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_umid_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/umid_day  

$ hdfs dfs -get alibaba/ipstats/[^I]*
</code></pre>

<p>The command gets all folders except for <code>alibaba/ipstats/IP</code>  </p>

<h1 id="toc_5">Hadoop hdfs / bash get matched folders</h1>

<p>This works both on bash and hadoop hdfs</p>

<p>Below is the HDFS folder:</p>

<pre><code class="language-bash">Found 6 items
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/FP
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/TP
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/UNKNOWN
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:19 alibaba/graph/stats/cluster-size-histo
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:19 alibaba/graph/stats/coverage
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:45 alibaba/graph/stats/stats
</code></pre>

<p>Need get the folders of <code>cluster-size-histo</code>, <code>coverage</code> and <code>stats</code></p>

<pre><code class="language-bash">hdfs dfs -get alibaba/graph/stats/[cs]*
</code></pre>

<p>Now local folder:</p>

<pre><code class="language-bash">total 8
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 cluster-size-histo
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 coverage
-rw-rw-r-- 1 yu yu  9 Sep 30 19:28 FP_cnt
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 stats
-rw-rw-r-- 1 yu yu  7 Sep 30 19:29 TP_cnt
</code></pre>

<p>Need delete folders of <code>cluster-size-histo</code>, <code>coverage</code> and <code>stats</code>:</p>

<pre><code class="language-bash">rm -rf [cs]*
</code></pre>

<h1 id="toc_6">check file encoding</h1>

<pre><code class="language-bash">file -I filename
</code></pre>

<p>The output is in format of: <code>/Path/To/Filename: fileformat/filetype; charset=encoding</code>. For example:  </p>

<pre><code class="language-bash">file -I Base.pig
Base.pig: application/octet-stream; charset=binary
</code></pre>

<h1 id="toc_7">du without recursion</h1>

<p>only want to see the whole folder&#39;s disk usage, not the details of its sub-folders.  </p>

<pre><code class="language-bash">du -s -h *
</code></pre>

<p><code>-s</code>: no recursion<br/>
<code>-h</code>: human readable</p>

<h1 id="toc_8">checking running job and argument</h1>

<p>the following commands works well:</p>

<pre><code class="language-bash">top -U [username]
ps -fu [username] # My most used one
ps -efl | grep &lt;username&gt; # get all the arguments
ps -efl | grep &lt;command name&gt;  

ps -efl | egrep &#39;\s+yu\s+&#39;
</code></pre>

<h1 id="toc_9">grep find negate</h1>

<p>negative matching, i.e. match lines that do not contain pattern</p>

<pre><code class="language-bash">grep -v [pattern] 
# -v, --invert-match select non-matching lines
</code></pre>

<h1 id="toc_10">grep find text in all files under a folder and all of its subfolders</h1>

<pre><code class="language-bash">grep -r -l -F -n -i -I &quot;string&quot; /path
</code></pre>

<p>in which:<br/>
<code>-r</code> is for recursive<br/>
<code>-l</code> is for showing the file name only (stop reading the file as soon as the string is find)<br/>
<code>-F</code> is searching for literal &quot;fixed string&quot;, not regexp<br/>
<code>-n</code> is printing the line number<br/>
<code>-i</code> is case-insensitive<br/>
<code>-I</code> is ignore binary files<br/>
Also there are:<br/>
<code>--exclude-dir=dir</code> is useful for excluding directories like .svn and .git</p>

<h1 id="toc_11">Keep the REPL across sessions</h1>

<p>This is very useful when running a Spark REPL on a remote ssh cluster, after logging out and close the terminal, next time connect and logging in the ssh cluster, we can pick up the same REPL session!</p>

<pre><code class="language-bash"># before running the REPL
screen
# start the REPL
/home/xiaogu/repo/spark-1.6.1-bin-hadoop2.4/bin/spark-shell --queue datascience

# Do not leave the REPL but leave the screen
Ctrl-a d

# leave the ssh session

# re-log in the ssh session
screen -r
</code></pre>

<p><a href="http://www.tecmint.com/screen-command-examples-to-manage-linux-terminals/">Detailed post for the <code>screen</code> command</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Using curly braces in Bash substitution</a>
</li>
<li>
<a href="#toc_1">nohup to a different file</a>
</li>
<li>
<a href="#toc_2">Append multiple lines to a file</a>
</li>
<li>
<a href="#toc_3">copy from bash shell to clipboard</a>
</li>
<li>
<a href="#toc_4">Hadoop hdfs get all except one folder (file)</a>
</li>
<li>
<a href="#toc_5">Hadoop hdfs / bash get matched folders</a>
</li>
<li>
<a href="#toc_6">check file encoding</a>
</li>
<li>
<a href="#toc_7">du without recursion</a>
</li>
<li>
<a href="#toc_8">checking running job and argument</a>
</li>
<li>
<a href="#toc_9">grep find negate</a>
</li>
<li>
<a href="#toc_10">grep find text in all files under a folder and all of its subfolders</a>
</li>
<li>
<a href="#toc_11">Keep the REPL across sessions</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala]]></title>
    <link href="www.echo-ohce.com/14767220898143.html"/>
    <updated>2016-10-17T09:34:49-07:00</updated>
    <id>www.echo-ohce.com/14767220898143.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Convert between regular collection and parallel collection</a>
</li>
<li>
<a href="#toc_1">function / method handler</a>
</li>
<li>
<a href="#toc_2">Get the type of variable in REPL</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Convert between regular collection and parallel collection</h1>

<p>Using Seq as example. Use <code>.par</code> to convert from regular to parallel, and use <code>.seq</code> to convert back from parallel to regular.</p>

<p>Special note that <code>.toSeq</code>, <code>toMap</code> or <code>toSet</code> are converting between various collection but stay in the same parallel or regular domain.</p>

<p><a href="http://stackoverflow.com/a/12023096/4229125">stackoverflow post</a></p>

<h1 id="toc_1">function / method handler</h1>

<p>look at the following example:</p>

<pre><code class="language-scala">val test = Array(&quot;1.0&quot;, &quot;2.5&quot;, &quot;3.14159&quot;)

//Wrong
test map toDouble
// Because this is wrong
toDouble(test(2))
//correct
test map (_.toDouble)
// Because this is correct
test(2).toDouble

//Wrong
test map parseDouble
//correct
test map java.lang.Double.parseDouble
// Because this is correct
java.lang.Double.parseDouble(test(2))
</code></pre>

<h1 id="toc_2">Get the type of variable in REPL</h1>

<p><code>variableName.getClass.getSimpleName</code></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Convert between regular collection and parallel collection</a>
</li>
<li>
<a href="#toc_1">function / method handler</a>
</li>
<li>
<a href="#toc_2">Get the type of variable in REPL</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[giraph]]></title>
    <link href="www.echo-ohce.com/14742423250614.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250614.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Kill the background job</a>
</li>
<li>
<a href="#toc_1">giraph Unit testing</a>
</li>
<li>
<a href="#toc_2">giraph edge value not Mutable in place</a>
</li>
<li>
<a href="#toc_3">giraph shell runner / command line</a>
</li>
<li>
<a href="#toc_4">Unsolved problem with the input string split in InputFormat class</a>
</li>
<li>
<a href="#toc_5">giraph log file memory info free/total/max</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Kill the background job</h1>

<p>logging on sc2-hive1, run the following command to check the running job on background. (It works for both oozie and normal hadoop jobs)  </p>

<pre><code class="language-bash">ps -u yu
hadoop job -list | grep yu
</code></pre>

<p>When using the <code>ps</code> command and what to see the detailed command argument, get the <code>pid#</code> and run:  </p>

<pre><code class="language-bash">ps --pid pid#
</code></pre>

<p>Hadoop YARN command equivalent to <code>hadoop job -list</code>:  </p>

<pre><code class="language-bash">yarn application -appStates RUNNING -list
</code></pre>

<h1 id="toc_1">giraph Unit testing</h1>

<ul>
<li>When use <code>InternalVertexRunner</code>, comment out all giraph/hadoop counter logs</li>
<li>Most of the case, the unit test fails silently. Adding some <code>sout</code> maybe the only way to debug</li>
<li>the testing string input delimiter should always be <code>space</code> even in reality we can use all kinds of delimiter like <code>\t</code> etc.</li>
</ul>

<p><strong>wrong</strong></p>

<pre><code class="language-java">// tab &#39;\t&#39; delimited
String[] edges = new String[] { &quot;0  1&quot;, &quot;0  3&quot;, &quot;1  0&quot;, &quot;1  3&quot;, &quot;1  2&quot;};
</code></pre>

<p><strong>correct</strong></p>

<pre><code class="language-java">// space delimited
String[] edges = new String[] { &quot;0 1&quot;, &quot;0 3&quot;, &quot;1 0&quot;, &quot;1 3&quot;, &quot;1 2&quot;};
</code></pre>

<h1 id="toc_2">giraph edge value not Mutable in place</h1>

<p>edges are just a reference, it can not be changed in place.<br/>
After mutate the edge value, use <code>vertex.setEdge(id, edgevalue)</code> to change it.</p>

<h1 id="toc_3">giraph shell runner / command line</h1>

<p>This is one file for running giraph with MasterCompute, Aggregator, etc.  </p>

<pre><code class="language-bash">WORKER=177
MEM=5120
HEAP=4352
#zoo keeper
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

hadoop jar dpp-giraph-0.0.1-with-giraph-core.jar \
        com.adsymp.dpp.giraph.lp.LPRunner \
        -Dmapred.job.queue.name=datascience \
        -Dmapreduce.map.memory.mb=$MEM \
        -Dmapreduce.task.timeout=1800000 \
        -Dmapred.task.timeout=1800000 \
        -Dmapreduce.map.java.opts=-Xmx${HEAP}m \
        -Dgiraph.zkList=$ZKLIST \
        com.adsymp.dpp.giraph.lp.WeightedLPComputation \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        -mc com.adsymp.dpp.giraph.lp.ClusterHistogramMC \
        -aw org.apache.giraph.aggregators.TextAggregatorWriter \
        -ca giraph.textAggregatorWriter.frequency=1 \
        -vif com.adsymp.dpp.giraph.lp.LongScoredValueFloatAdjacencyListVertexInputFormat \
        -vip $ADJ \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op $OUT \
        -w $WORKER -ca giraph.SplitMasterWorker=true \
        -ca giraph.zkSessionMsecTimeout=360000 \
        -ca giraph.zkOpsMaxAttempts=20 \
        -ca giraph.zkOpsRetryWaitMsecs=10000 \
        -ca mapred.job.tracker=sc2-rm1:8088 \
        -ca mapreduce.job.tracker=sc2-rm1:8088 \
        -ca com.adsymp.giraph.max.cd=8 \
        -ca giraph.waitingRequestMsecs=300000 \
        -ca com.adsymp.giraph.max.cc=100 \
        -ca com.adsymp.giraph.max.dd=8 \
        -ca com.adsymp.giraph.max.degree=80 \
        -ca com.adsymp.giraph.ignore.dev.types=false \
        -ca com.adsymp.giraph.weight.threshold=0.4 \
        -ca giraph.jobRetryCheckerClass=com.adsymp.dpp.giraph.RetryThriceChecker \
        -ca mapred.job.queue.name=datascience \
        -ca mapred.output.compress=true \
        -ca mapred.task.timeout=1800004 \
        -ca mapreduce.map.memory.mb=$MEM \
        -ca mapreduce.map.java.opts=-Xmx${HEAP}m
</code></pre>

<p>Couple important points:<br/>
1. the line breaker <code>\</code>: must have one space before it and <strong>no</strong> space after it, and every broken line must have it.<br/>
2. Pay special attention to the sequence of the arguments, some of them matters.<br/>
3. The zookeeper list after the line of the Computation class is a must!  </p>

<p>This one file is for running giraph with jython:  </p>

<pre><code class="language-bash">hdfs dfs -rm -r giraph-jython/output
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

yarn jar giraph-jython-1.0-SNAPSHOT-jar-with-dependencies.jar \
        com.adsymp.dpp.giraph.jython.GiraphJythonRunner \
        -Dmapred.job.queue.name=datascience \
        -Dgiraph.zkList=$ZKLIST \
        graph-distance.py \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        --jythonClass GraphDistance \
        --typesHolder com.adsymp.dpp.giraph.jython.JythonTypes \
        -eif org.apache.giraph.io.formats.IntNullTextEdgeInputFormat \
        -eip giraph-jython/input/tiny_graph \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op giraph-jython/output \
        -w 1 \
        -ca giraph.SplitMasterWorker=false
</code></pre>

<h1 id="toc_4">Unsolved problem with the input string split in InputFormat class</h1>

<p>My input file is delimited by <code>^A</code> (i.e.<code>\u0001</code>). Tried to change the <code>LongNullTextEdgeInputFormat.LongNullTextEdgeReader#preprocessLine</code> for splitting by <code>\u0001</code>. Failed.</p>

<p>Temporary workaround is wrote another pig script to convert the input file schema.</p>

<h1 id="toc_5">giraph log file memory info free/total/max</h1>

<p>In the log file of each individual worker, we see the following lines about the real time memory usage: </p>

<blockquote>
<p>2016-10-21 21:35:44,615 INFO [compute-0] org.apache.giraph.graph.ComputeCallable: call: Completed 18 partitions, 159 remaining Memory (free/total/max) = 6154.37M / 16748.50M / 27079.00M.  </p>
</blockquote>

<p>This log is generated by the class: <code>org.apache.giraph.utils.MemoryUtils</code> <a href="https://github.com/apache/giraph/blob/release-1.1/giraph-core/src/main/java/org/apache/giraph/utils/MemoryUtils.java">source code here</a></p>

<p>this class calls <code>java.lang.Runtime</code> class and use the method <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#freeMemory()"><code>freeMemory()</code></a>, <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#maxMemory()"><code>maxMemory()</code></a>, <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#totalMemory()"><code>totalMemory()</code></a> to get the real time memory usage stats</p>

<ul>
<li>free<br/>
the amount of free memory in the Java Virtual Machine. Calling the <code>gc</code> method may result in increasing the value returned by freeMemory. <strong>an approximation to the total amount of memory currently available for future allocated objects</strong></li>
<li>max<br/>
the maximum amount of memory that the Java virtual machine will attempt to use. If there is no inherent limit then the value Long.MAX_VALUE will be returned. <strong>the maximum amount of memory that the virtual machine will attempt to use</strong> </li>
<li>total<br/>
the total amount of memory in the Java virtual machine. The value returned by this method may vary over time, depending on the host environment. (Note that the amount of memory required to hold an object of any given type may be implementation-dependent.) <strong>the total amount of memory currently available for current and future objects</strong></li>
</ul>

<hr/>

<ul>
<li>
<a href="#toc_0">Kill the background job</a>
</li>
<li>
<a href="#toc_1">giraph Unit testing</a>
</li>
<li>
<a href="#toc_2">giraph edge value not Mutable in place</a>
</li>
<li>
<a href="#toc_3">giraph shell runner / command line</a>
</li>
<li>
<a href="#toc_4">Unsolved problem with the input string split in InputFormat class</a>
</li>
<li>
<a href="#toc_5">giraph log file memory info free/total/max</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[python]]></title>
    <link href="www.echo-ohce.com/14742423250719.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250719.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">shell expansion insdie python with  <code>subprocess.call</code></a>
</li>
<li>
<a href="#toc_1">plotly using LaTex</a>
</li>
<li>
<a href="#toc_2">plotly cufflinks pandas dataframe customize the plot</a>
</li>
<li>
<a href="#toc_3">timezone</a>
</li>
<li>
<a href="#toc_4">datetime, pd.Timestamp, np.datetime64</a>
</li>
<li>
<a href="#toc_5">To make a linspace of pd.Timestamp</a>
</li>
<li>
<a href="#toc_6">plotly tick setting: string format, number of ticks and location of ticks.</a>
</li>
<li>
<a href="#toc_7">plotly datetime type tick setting</a>
</li>
<li>
<a href="#toc_8">plotly change the padding size between subplots</a>
</li>
<li>
<a href="#toc_9">read in gz / gzip file</a>
</li>
<li>
<a href="#toc_10">plotly bar chart, use whether value is negative or positive to determine its color</a>
</li>
<li>
<a href="#toc_11">plotly subplot shared axis setting</a>
</li>
<li>
<a href="#toc_12">plotly remove missed dates from time series plot</a>
</li>
<li>
<a href="#toc_13">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</a>
</li>
<li>
<a href="#toc_14">Use <code>:</code> or <code>slice(None)</code></a>
</li>
<li>
<a href="#toc_15">String for pandas datetime period and frequency</a>
</li>
<li>
<a href="#toc_16">Python Library for Probabilistic Graphical Models</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">shell expansion insdie python with  <code>subprocess.call</code></h1>

<p>For python older than 2.7 (like the default on on DB cluster, but we can change the shebang to specify 2.7), we can only use <code>subprocess.call()</code>.<br/><br/>
when using shell expansion with this command, need have <code>shell=True</code>. Better way to have shell expansion inside python is to use module <code>glob</code><br/><br/>
But the following works, because it is not the shell expansion but the <code>imagemagick</code> does the expansion.  </p>

<pre><code class="language-python">subprocess.call([&#39;convert&#39;, &#39;-delay&#39;, &#39;10&#39;, &#39;-loop&#39;, &#39;10&#39;, &#39;temp_1*.png&#39;, &#39;cluster_size_anim.gif&#39;])
</code></pre>

<h1 id="toc_1">plotly using LaTex</h1>

<p>It is not possible to use LaTex in offline mode. (don&#39;t waste time to figure out a way to do that anymore, maybe wait for newer update)<br/><br/>
It works when using the online mode.</p>

<h1 id="toc_2">plotly cufflinks pandas dataframe customize the plot</h1>

<ol>
<li>using the <code>asFigure=True</code> option to get the handle of the figure</li>
<li>using <code>update</code> method to update the layout python dictionary</li>
<li>using <code>po.iplot()</code> to plot the figure</li>
</ol>

<p>Example:  </p>

<pre><code class="language-python">figure = data.iplot(kind=&#39;scatter&#39;, fill=True, asFigure=True)
figure.layout.update(xaxis=dict(nticks=len(data.asfreq(&#39;M&#39;).index)))
po.iplot(figure)
</code></pre>

<h1 id="toc_3">timezone</h1>

<p>A classical scenario is get timestamp in UTC, want to convert to local timezone. Panda can do it for time series</p>

<pre><code class="language-python">ts2 = ts1.
        tz_localize(&#39;UTC&#39;).
        tz_convert(&#39;US/Pacific&#39;)
</code></pre>

<p>the <code>tz_localize(&#39;UTC&#39;)</code> make it timezone aware, the <code>tz_convert(&#39;US/Pacific&#39;)</code> convert it to local timezone.</p>

<h1 id="toc_4">datetime, pd.Timestamp, np.datetime64</h1>

<p>A very good graph to show the converting relationship:</p>

<p><img src="media/14742423250719/14757939209171.jpg" alt=""/></p>

<h1 id="toc_5">To make a linspace of pd.Timestamp</h1>

<p>The <code>pd.date_range(start=None, end=None, periods=None, freq=&#39;D&#39;)</code> method will not work. This method is for different purpose.</p>

<pre><code class="language-python">rng = pd.date_range(&#39;1/1/2011&#39;, periods=72, freq=&#39;H&#39;)

# the same as pd.date_range(start, end, freq=&#39;D&#39;)
rng = pd.date_range(start, end)
</code></pre>

<p>To make the linespace of pd.Timestamp:<br/>
1. convert pd.Timestamp to long type<br/>
2. use np.linspace to make the array of long<br/>
3. convert back to pd.Timestamp array<br/>
4. Special note: If the original timestamp has timezone info, the step 3 need use different method to restore the timezone info</p>

<pre><code class="language-python"># the index of memory_df dataframe is of type pd.Timestamp
# the ticks_array is what we want linspace array of pd.Timestamp
# this is for cases without timezone information
ticks_array = pd.to_datetime(np.linspace(start=memory_df.index[0].value, 
                                         stop=memory_df.index[-1].value, 
                                         num=n_ticks, 
                                         endpoint=True))
                                         
# if need keep timezone info, to_datetime() method is not good anymore.
# for example the index of memory_df dataframe has timezone info
memory_df.index = memory_df.index.tz_localize(&#39;UTC&#39;).tz_convert(&#39;US/Pacific&#39;)

ticks_array = [pd.Timestamp(timetick, tz=memory_df.index.tz) for timetick in 
               (np.linspace(start=memory_df.index[0].value,
                            stop=memory_df.index[-1].value,
                            num=n_ticks, endpoint=True))]
</code></pre>

<h1 id="toc_6">plotly tick setting: string format, number of ticks and location of ticks.</h1>

<ul>
<li>The easiest way to set <code>nticks</code> and <code>tickformat</code>. Like the following example:</li>
</ul>

<pre><code class="language-python">plot_layout = go.Layout(xaxis=dict(nticks=5,
                                   tickformat=&#39;.2f&#39;))
</code></pre>

<ul>
<li>The hard way is to set <code>tickmode=&#39;array&#39;, ticktext=[...], tickvals=[...]</code>. </li>
</ul>

<h1 id="toc_7">plotly datetime type tick setting</h1>

<p>directly use <code>datetime</code> datatype and the plotly tick setting options have subtle bugs, especially when dealing with timezone awared data.</p>

<p>The workaround:<br/>
1. convert the index to string using <code>strftime</code> (these string need to have high enough resolution like <code>&#39;%H:%M:%S:%f&#39;</code>, otherwise, the plot will loose resolution because of having the same xaxis values)<br/>
2. generating <code>ticktext</code> and <code>tickvals</code>,specifing any format you like for <code>ticktext</code>.  </p>

<p>Look the below example:  </p>

<pre><code class="language-python">def visualization(job_id, basepath=&#39;.&#39;):
    memory_file = os.sep.join([basepath, &#39;memory_usage_&#39;+job_id+&#39;.csv&#39;])
    
    memory_df = pd.read_csv(memory_file)
    timestamp = pd.to_datetime(memory_df.timestamp, infer_datetime_format=True)
    memory_df = memory_df.set_index(timestamp).drop(&#39;timestamp&#39;, axis=1)
    memory_df=memory_df/float(1024*1024*1024*1024)
    memory_df.index = memory_df.index.tz_localize(&#39;UTC&#39;).tz_convert(&#39;US/Pacific&#39;)
    
    # change the index to string for plotting purpose
    memory_plot_df = memory_df.copy()
    memory_plot_df.index = pd.Series(data=[ts.strftime(&#39;%H:%M:%S:%f&#39;) for ts in memory_plot_df.index],name=&#39;time&#39;)
    mem_fig = memory_plot_df.iplot(asFigure=True)
    
    # set the xaxis ticks
    n_ticks = 10
    picked_tick_index = ((np.linspace(start=0, stop=len(memory_plot_df.index)-1, num=n_ticks, endpoint=True))
                        .astype(int))
    ticktext=[&#39;:&#39;.join(ts.split(&#39;:&#39;)[:2]) for ts in memory_plot_df.index[picked_tick_index]] # only take hour and mins
    tickvals=memory_plot_df.index[picked_tick_index]
    mem_fig.layout.update(title=&#39;Memory usage for &#39;+job_id,
                          yaxis=dict(title=&#39;TB&#39;),
                          xaxis=dict(title=&#39;time&#39;,
                                     tickmode=&#39;array&#39;,
                                     ticktext=ticktext,
                                     tickvals=tickvals))

    po.iplot(mem_fig, show_link=False)
</code></pre>

<h1 id="toc_8">plotly change the padding size between subplots</h1>

<p>Use <code>specs</code> and <code>vertical_spacing</code> or <code>horizontal_spacing</code> inside <code>tls.make_subplots()</code></p>

<p>Example:  </p>

<pre><code class="language-python">import plotly.tools as tls

figure = tls.make_subplots(rows=2, cols=1, shared_xaxes=True,
                           specs=[[{}], [{}]],
                           vertical_spacing=0.01)
</code></pre>

<h1 id="toc_9">read in gz / gzip file</h1>

<pre><code class="language-python">import gzip

with gzip.open(file_path, &#39;rb&#39;) as f:
    for line in f:
</code></pre>

<h1 id="toc_10">plotly bar chart, use whether value is negative or positive to determine its color</h1>

<pre><code class="language-python"># here sh_MACD.ch is a time series, with values can be positive or negative.
# want to generate the bar chart that is red if its value is positive, or green if negative

colors=(sh_MACD.ch&gt;0).map({True:&#39;red&#39;, False:&#39;green&#39;}).values
trace_macd = go.Bar(x=sh_MACD.index, y=sh_MACD.ch, name = &#39;MACD&#39;,
                    marker=dict(color=colors))
</code></pre>

<p>Key points:<br/>
* use map to convert logical array to the color literals<br/>
* use values to get np.ndarray not pd.Series (because plotly need np.ndarray)<br/>
* use <code>marker</code> filed</p>

<h1 id="toc_11">plotly subplot shared axis setting</h1>

<p>For example, if the subplot is 2 row 1 col, with shared x axis, the axis&#39; names are: [(1,1) x1,y1 ], [(2,1) x1,y2 ]. To set the axis properties in the <code>layout</code> dictionary, using <code>xaxis1</code> instead of <code>xaxis</code>.</p>

<p>The following example is from <code>01-Project/01-Study/PersonalProject/EDA.ipynb</code></p>

<pre><code class="language-python">overlay_plot[&#39;layout&#39;].update(xaxis1=dict(type=&#39;category&#39;))
</code></pre>

<h1 id="toc_12">plotly remove missed dates from time series plot</h1>

<p>change the xaxis&#39; type to <code>category</code>. (refer to the example above)</p>

<h1 id="toc_13">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</h1>

<pre><code>In [65]: df
Out[65]: 
                     A         B         C
first second                              
bar   one     0.895717  0.410835 -1.413681
      two     0.805244  0.813850  1.607920
baz   one    -1.206412  0.132003  1.024180
      two     2.565646 -0.827317  0.569605
foo   one     1.431256 -0.076467  0.875906
      two     1.340309 -1.187678 -2.211372
qux   one    -1.170299  1.130127  0.974466
      two    -0.226169 -1.436737 -2.006747
</code></pre>

<p>select all <code>two</code> data. Two methods:</p>

<ul>
<li>MultiIndex is tuple, using <code>loc</code> and tuple to select, and use <code>slice(None)</code> inside tuple to represent <code>:</code> that we normally use (because <code>:</code> is not recognized inside tuple). <font color='red'> The dataframe MultiIndex must be sorted first.</font></li>
</ul>

<pre><code class="language-python">df=df.sort_index()
selection = df.loc[(slice(None), &#39;two&#39;), :]
</code></pre>

<ul>
<li>Use <code>xs</code> method and use <code>level</code> argument to specify the level. (note that <code>xs</code> by default works on rows, if want to use it on columns, use the <code>axis=1</code> argument)<br/></li>
</ul>

<pre><code class="language-python">selection = df.xs(&#39;two&#39;, level=&#39;second&#39;)
</code></pre>

<h1 id="toc_14">Use <code>:</code> or <code>slice(None)</code></h1>

<p>They mean the same thing, normally in <code>[ ]</code>, we just use <code>:</code>, like <code>[5, :]</code><br/>
However, inside tuple, <code>:</code> is not recognized, we have to use <code>slice(None)</code> instead.</p>

<h1 id="toc_15">String for pandas datetime period and frequency</h1>

<p>Useful official documents</p>

<ul>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases">frequency strings</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offsets">Anchored offset frequency strings</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offset-semantics">Rolling forward or backward to snap on the anchors</a></li>
</ul>

<h1 id="toc_16">Python Library for Probabilistic Graphical Models</h1>

<p><a href="https://github.com/pgmpy/pgmpy">github link</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">shell expansion insdie python with  <code>subprocess.call</code></a>
</li>
<li>
<a href="#toc_1">plotly using LaTex</a>
</li>
<li>
<a href="#toc_2">plotly cufflinks pandas dataframe customize the plot</a>
</li>
<li>
<a href="#toc_3">timezone</a>
</li>
<li>
<a href="#toc_4">datetime, pd.Timestamp, np.datetime64</a>
</li>
<li>
<a href="#toc_5">To make a linspace of pd.Timestamp</a>
</li>
<li>
<a href="#toc_6">plotly tick setting: string format, number of ticks and location of ticks.</a>
</li>
<li>
<a href="#toc_7">plotly datetime type tick setting</a>
</li>
<li>
<a href="#toc_8">plotly change the padding size between subplots</a>
</li>
<li>
<a href="#toc_9">read in gz / gzip file</a>
</li>
<li>
<a href="#toc_10">plotly bar chart, use whether value is negative or positive to determine its color</a>
</li>
<li>
<a href="#toc_11">plotly subplot shared axis setting</a>
</li>
<li>
<a href="#toc_12">plotly remove missed dates from time series plot</a>
</li>
<li>
<a href="#toc_13">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</a>
</li>
<li>
<a href="#toc_14">Use <code>:</code> or <code>slice(None)</code></a>
</li>
<li>
<a href="#toc_15">String for pandas datetime period and frequency</a>
</li>
<li>
<a href="#toc_16">Python Library for Probabilistic Graphical Models</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[intellij]]></title>
    <link href="www.echo-ohce.com/14765768181414.html"/>
    <updated>2016-10-15T17:13:38-07:00</updated>
    <id>www.echo-ohce.com/14765768181414.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, shortcuts I learned. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">List of most frequently used shortcut:</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">List of most frequently used shortcut:</h1>

<ul>
<li>Jump to previous location (navigate back):

<ul>
<li>cmd + [</li>
<li>cmd + ]</li>
</ul></li>
<li> </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[pig]]></title>
    <link href="www.echo-ohce.com/14742423250700.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250700.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">File Name Pattern Substitution</a>
</li>
<li>
<a href="#toc_1"><code>ToDate(milliseconds)</code> gives wrong date</a>
</li>
<li>
<a href="#toc_2">DateTime SimpleDateFormat</a>
</li>
<li>
<a href="#toc_3">Parameter substitution Shell Runner subtle bug</a>
</li>
<li>
<a href="#toc_4">Becareful of <code>DateTime</code> data type in pig</a>
</li>
<li>
<a href="#toc_5">More pitfalls for <code>DateTime</code> data type</a>
</li>
<li>
<a href="#toc_6">Setting of memory</a>
</li>
<li>
<a href="#toc_7">Pig Unit Test <code>override()</code> method</a>
</li>
<li>
<a href="#toc_8">Error message for wrong reference operator</a>
</li>
<li>
<a href="#toc_9">Pig Java UDF Unit Test</a>
</li>
<li>
<a href="#toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></a>
</li>
<li>
<a href="#toc_11">specify function output data type</a>
</li>
<li>
<a href="#toc_12">using <code>u0001</code> as the delimiter</a>
</li>
<li>
<a href="#toc_13">Error with SUBSTRING and SIZE function</a>
</li>
<li>
<a href="#toc_14">oneline to order two fields</a>
</li>
<li>
<a href="#toc_15">sort with duplicates</a>
</li>
<li>
<a href="#toc_16">input and output are reserved keyword in pig</a>
</li>
<li>
<a href="#toc_17">Mystery ERROR 2999</a>
</li>
<li>
<a href="#toc_18">Cumsum</a>
</li>
<li>
<a href="#toc_19">A pretty good graph cluster to pair Pig example code</a>
</li>
<li>
<a href="#toc_20">A pretty good Score bucketize and cumsum precision Pig example code</a>
</li>
<li>
<a href="#toc_21">Parameter Substitution in Pig</a>
</li>
<li>
<a href="#toc_22">SUBSTRING function</a>
</li>
<li>
<a href="#toc_23">Nested FOREACH</a>
</li>
<li>
<a href="#toc_24">Function are allowed inside FOREACH</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">File Name Pattern Substitution</h1>

<p>Pig is using hadoop file <code>glob</code> utilities to process the file name pattern. It is <font color=red>NOT</font> using shell&#39;s <code>glob</code>. Hadoop&#39;s <code>glob</code> are documented <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html#globStatus%28org.apache.hadoop.fs.Path%29">here</a>. Specially, it does not support <code>..</code> operator for a range.</p>

<p><a href="http://stackoverflow.com/questions/3515481/pig-latin-load-multiple-files-from-a-date-range-part-of-the-directory-structur">reference post</a></p>

<h1 id="toc_1"><code>ToDate(milliseconds)</code> gives wrong date</h1>

<p>Bug looks like always return date of some time at <code>1970-01-17</code>. This is because the input is 10 digits in seconds, and input should be in milliseconds. Need \( \times 1000\) on the input to get the correct DateTime.</p>

<h1 id="toc_2">DateTime SimpleDateFormat</h1>

<p>Pig&#39;s <code>DateTime</code> type conforms to the following format:</p>

<pre><code class="language-java">SimpleDateFormat dateFormat = 
    new SimpleDateFormat(&quot;yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSSXXX&quot;);
</code></pre>

<p>The string format is: <code>2001-07-04T12:08:56.235-07:00</code><br/>
<a href="https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html">Java official example table</a></p>

<p>[YGMARK] <code>-07:00</code> timezone format is only supported after JDK 7.</p>

<h1 id="toc_3">Parameter substitution Shell Runner subtle bug</h1>

<p>I like to define <code>_</code> as the output file separator. I usually have parameter <code>SEP= &#39;_&#39;</code> in my pig script and shell runner. However, if this parameter is defined inside shell runner, <em>it has to be escaped</em>, like: <code>SEP=&#39;\_&#39;</code>. Otherwise, this mislead error message will show:  </p>

<blockquote>
<p>ERROR org.apache.pig.Main - ERROR 1000: Error during parsing. Lexical error at line 1, column 6.  Encountered: <EOF> after : &quot;&quot;  </p>
</blockquote>

<p>It does not need be escaped if only defined inside pig script.<br/>
Another side note for seeing the above message. It is very likely caused by unbalanced bracket, quote, <font color='red'>extra space between the bash line separator <code>\</code> and the new line</font> etc.</p>

<h1 id="toc_4">Becareful of <code>DateTime</code> data type in pig</h1>

<p>It is convenient to get the time duration of difference by using the <code>DateTime</code> data type. However, in pig it is a know bug that <a href="https://issues.apache.org/jira/browse/PIG-3953">it can not be compared correctly.</a> Therefore ordering, grouping type of work should use either <code>ToUnixTime()</code> or <code>ToString()</code> before ordering.</p>

<blockquote>
<p>Error: org.joda.time.DateTime.compareTo ...</p>
</blockquote>

<h1 id="toc_5">More pitfalls for <code>DateTime</code> data type</h1>

<ul>
<li>The 2nd argument of <code>ToString(datetime [, format string])</code> is not optional, but a must for pig 0.12 (<a href="https://issues.apache.org/jira/browse/PIG-3805">Bug fixed on 0.13</a>)</li>
<li>For my case, <code>ToString(date, &#39;yyyyMMdd&#39;)</code> works.</li>
<li>All capital function name is not accepted, like <code>TOSTRING()</code> fails.</li>
<li>Also, there is no way to directly read in the <code>DateTime</code> type to pig. It still need be read in as <code>chararray</code>, otherwise it will be just null. (I tested it). <a href="http://stackoverflow.com/a/22052965">Stackoverflow post</a></li>
<li><code>GetMilliSecond(DateTime datetime)</code> is not the function to get the millisecond from epoch. It ... en, as the name indicates ... just get the millisecond of the time. To get the <strong>second</strong> use <code>ToUnixTime(DateTime datetime)</code>. To get the <strong>millisecond</strong> use <code>ToMilliSeconds(DateTime datetime)</code>.</li>
</ul>

<h1 id="toc_6">Setting of memory</h1>

<p>In side pig script or grunt:  </p>

<pre><code class="language-bash">SET mapreduce.map.memory.mb 4096;
SET mapreduce.reduce.memory.mb 8192;
</code></pre>

<h1 id="toc_7">Pig Unit Test <code>override()</code> method</h1>

<p>Code normally like:  </p>

<pre><code class="language-java">PigTest test = new PigTest(pigScript, parameters);
test.override(&quot;alias&quot;, &quot;alias = LOAD ...&quot;);
</code></pre>

<p>Inside the override, can not use variable substitution, otherwise will see error like:  </p>

<blockquote>
<p>org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias  </p>
</blockquote>

<p>Wrong:  </p>

<pre><code class="language-java">test.override(&quot;alias&quot;, &quot;alias = LOAD &#39;$INPUT&#39; USING PigStorage() AS (...);&quot;);
</code></pre>

<p>Should always use actual file path:</p>

<pre><code class="language-java">test.override(&quot;alias&quot;, &quot;alias = LOAD &#39;input.txt&#39; USING PigStorage() AS (...);&quot;);
</code></pre>

<h1 id="toc_8">Error message for wrong reference operator</h1>

<p>If receive following error message:  </p>

<blockquote>
<p>Error: Scalar has more than one row in the output.  </p>
</blockquote>

<p>It is very likely that I got a dot <code>Alias.field</code> where you need a double semi-colon <code>Alias::field</code>.<br/><br/>
<a href="http://stackoverflow.com/questions/22522155/pig-scalar-has-more-than-one-row-in-the-output">stackoverflow post</a>  </p>

<h1 id="toc_9">Pig Java UDF Unit Test</h1>

<p>Find a good <a href="http://www.crackinghadoop.com/unit-test-java-udfs/">post</a> specifically for Java UDF unit test.<br/><br/>
The main point is to use <code>DefaultTuple</code> to inject test cases into the UDF.  </p>

<pre><code class="language-java">String inputText = &quot;09/15/2014&quot;;
DefaultTuple input = new DefaultTuple();
input.append(inputText);
</code></pre>

<h1 id="toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></h1>

<ol>
<li><code>join</code> (more specifically, inner join) acts on more than two relations:<br/>
<code>X = JOIN A BY fieldA, B BY fieldB, C BY fieldC;</code> </li>
<li><p><code>join</code> on duplicates gives cross product on the two relations.  </p>

<pre><code class="language-sql">A = LOAD &#39;data1&#39; AS (a1:int,a2:int,a3:int);
DUMP A;   
(1,2,3)
(4,2,1)
(4,3,3)

B = LOAD &#39;data2&#39; AS (b1:int,b2:int);
DUMP B;
(4,6)
(4,9)

X = JOIN A BY a1, B BY b1;
DUMP X;
(4,2,1,4,6)
(4,3,3,4,6)
(4,2,1,4,9)
(4,3,3,4,9)  
</code></pre></li>
<li><p><code>join</code> on null disregards (filters out) null values. See the <a href="https://pig.apache.org/docs/r0.11.1/basic.html#nulls_join">official document</a>.  </p></li>
<li><p><code>cogroup</code> on duplicates gives nested set of tuples for the two relations (a bag for each side of the relation). If <code>flatten</code> is then used, will generate cross product.  </p></li>
<li><p><code>cogroup</code> or <code>group</code> for <code>null</code> key, will be grouped together per relation. <a href="https://pig.apache.org/docs/r0.11.1/basic.html#nulls_group">official document</a>  </p></li>
<li><p><code>cogroup</code> or <code>group</code> for <code>null</code> value, will just keep the empty bag, unless <code>inner</code> keywords is used.  </p></li>
<li><p><code>flatten</code> of an empty bag will <font color='red'><strong>remove</strong></font> the whole record (for the cross product case of <code>flatten</code> two bags) because <code>flatten</code> an empty bag produces no output. <a href="http://datafu.incubator.apache.org/docs/datafu/guide/more-tips-and-tricks.html">Refer to this post</a> to under this further and a trick (<em>generating a bag with a null tuple</em>) to avoid this behavior for getting &#39;outer join&#39; effect.</p></li>
</ol>

<h1 id="toc_11">specify function output data type</h1>

<p>No matter whether it is builtin function or UDF, it is always better to specify this function&#39;s output data type.  </p>

<p>This is wrong, because the CONCAT output type is <code>bytearray</code>:    </p>

<pre><code class="language-sql">ip_db = FOREACH ip_db_m3 GENERATE MD5(CONCAT(ip_raw, &#39;umpdmp&#39;)) AS ip;   
</code></pre>

<p>This is right by specifying clearly that the output data type is <code>chararray</code>:  </p>

<pre><code class="language-sql">ip_db_m3 = FOREACH ip_db_m2 GENERATE CONCAT(ip_raw, &#39;umpdmp&#39;) AS ip:chararray;
ip_db = FOREACH ip_db_m3 GENERATE MD5(ip) AS ip;   
</code></pre>

<h1 id="toc_12">using <code>\u0001</code> as the delimiter</h1>

<ul>
<li>If want to use <code>LzoPigStorage</code> need be careful about in pig, to specify the parameter of the function, you cannot do it at the code where calls this function, but need do it in the <code>DEFINE</code> statement at the beginning, like below:<br/></li>
</ul>

<pre><code class="language-sql">DEFINE LzoPigStorage com.twitter.elephantbird.pig.store.LzoPigStorage(&#39;\u0001&#39;);   

store OUT into &#39;$OUTPUT&#39; using LzoPigStorage();   
</code></pre>

<ul>
<li>Note that <code>&#39;^A&#39;</code> will <strong>NOT</strong> work, Only <code>&#39;\u0001&#39;</code> works.<br/></li>
<li>If use <code>PigStorage</code> just specify it inside the function like below:<br/></li>
</ul>

<pre><code class="language-sql">A = LOAD &#39;input.txt&#39; USING PigStorage(&#39;,&#39;);   
STORE A INTO &#39;out&#39; USING PigStorage(&#39;\u0001&#39;);   
</code></pre>

<h1 id="toc_13">Error with SUBSTRING and SIZE function</h1>

<p>I was trying to remove the leading <code>.</code> of a ip string, and used <code>SUBSTRING(ip, 1, SIZE(ip)) AS ip_fixed:chararray</code>. The error message reads:  </p>

<blockquote>
<p>ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.SUBSTRING as multiple or none of them fit. Please use an explicit cast  </p>
</blockquote>

<p>The reason is because <code>SIZE()</code> function returns <code>long</code> type, but <code>SUBSTRING()</code> function needs <code>int</code> type. The following code passes: <code>SUBSTRING(ip, 1, (int) SIZE(ip)) AS ip_fixed:chararray</code>  </p>

<h1 id="toc_14">oneline to order two fields</h1>

<pre><code class="language-sql">CC_raw = LOAD &#39;$INPUT_CC&#39; USING PigStorage()
                   AS (id1:chararray, id2:chararray, score:double);
CC_m0 = FOREACH CC_raw GENERATE 
            FLATTEN(((id1 &lt; id2) ? (id1, id2) : (id2, id1))) AS (smallID, largeID),
            score AS score;
</code></pre>

<p>order the two fields <code>id1</code> and <code>id2</code>.</p>

<h1 id="toc_15">sort with duplicates</h1>

<p>sort with heavily duplicated field sometimes generate highly skewed partitions (they are partitioned based on bins, records with the same value of the sorting field will be inside the same bin). This will reduce the performance dramatically.</p>

<p>Solution is to sort with combined fields like <code>BY (f1, f2)</code> in which, <code>f1</code> is the actual field that need be sorted but with a lot of duplicates, <code>f2</code> is another field that is much more evenly distributed to help reduce the skew.</p>

<p>Thanks for MingYan :)  </p>

<h1 id="toc_16">input and output are reserved keyword in pig</h1>

<p>Do not use <code>input</code> or <code>output</code> as relations name, because they are reserved keyword. Error message is like:  </p>

<blockquote>
<p>ERROR org.apache.pig.PigServer - exception during parsing: Error during parsing. <file clean_vertex_result.pig, line 3, column 0>  mismatched input &#39;input&#39; expecting EOF</p>
</blockquote>

<h1 id="toc_17">Mystery ERROR 2999</h1>

<blockquote>
<p>ERROR 2999: Unexpected internal error. null</p>
</blockquote>

<p>One case (I wasted one full day on this!) is I used the same name for both a relation and a field!</p>

<p>Solution: add <code>_r</code> for all relation names</p>

<h1 id="toc_18">Cumsum</h1>

<p>use <code>Over, Stitch, ORDER BY</code></p>

<pre><code class="language-sql">REGISTER /home/adsymp/lib/piggybank.jar;

DEFINE OVER org.apache.pig.piggybank.evaluation.Over(&#39;long&#39;);
DEFINE STITCH org.apache.pig.piggybank.evaluation.Stitch();

output_to_LP_m0 = FOREACH (GROUP pairs_bucketized ALL) {
                        sorted = ORDER pairs_bucketized BY score_bucket DESC;
                        GENERATE FLATTEN(STITCH(sorted, OVER(sorted.tp_inbucket, &#39;sum(long)&#39;), OVER(sorted.fp_inbucket, &#39;sum(long)&#39;)));};
</code></pre>

<ul>
<li><code>STITCH</code> take bags as input, and its output cannot be directly assigned to a relation, i.e. it has to be inside a <code>FOREACH</code> statement <a href="http://stackoverflow.com/a/27798007">Stackoverflow post</a> </li>
<li><code>ORDER</code> makes the rows sorted for the <code>cumsum</code></li>
<li><code>OVER</code> does the <code>cumsum</code> <a href="https://pig.apache.org/docs/r0.12.0/api/org/apache/pig/piggybank/evaluation/Over.html">Official document</a></li>
<li><code>STITCH</code> make the <code>cumsum</code> one-on-one appending with the correct row.</li>
<li>When I tried <code>OVER</code> through <code>grunt</code>, I checked the output schema is <code>null</code>. This is not a bug! In fact, if we didn&#39;t input the string of type in the <code>Over()</code> constructor, this <code>piggybank</code> function does not know what is the output schema.</li>
</ul>

<p>One lesson learned: If it is the logic planning stage error, checking the alias.</p>

<h1 id="toc_19">A pretty good graph cluster to pair Pig example code</h1>

<p><code>/Users/yugan/repo/drawbridge/dpp/workflow-new/miaozhen-scripts/graph/Stats.pig</code></p>

<p>In this code:</p>

<ul>
<li>use of <code>TOKENIZE</code>, <code>LAST_INDEX_OF</code></li>
<li>The way of exploding graph <code>cluster</code> into pairs and get the TP/FP with label data is pretty clean and neat.</li>
<li>use <code>HashString</code> to sample</li>
</ul>

<h1 id="toc_20">A pretty good Score bucketize and cumsum precision Pig example code</h1>

<p><code>/Users/yugan/repo/drawbridge/dpp/workflow-new/miaozhen-scripts/rank/ToPrec2.pig</code></p>

<p>In this code:</p>

<ul>
<li>use <code>$SCORE_FACTOR</code> converting double to long for bucketize.</li>
<li>use of three UDF <code>GetScoreBuckets</code>, <code>AnalyzeScoreBucket</code>, and <code>GetPrecision</code>.</li>
<li>use <code>HashString</code> to sample</li>
<li>use <code>replicated</code> mode for the JOIN</li>
</ul>

<p>But I have also pretty good code using <code>Over</code>, <code>Stitch</code>, <code>ORDER BY</code> to implement cumsum as shown above.</p>

<h1 id="toc_21">Parameter Substitution in Pig</h1>

<p>Official documentation <a href="http://wiki.apache.org/pig/ParameterSubstitution">here</a></p>

<h1 id="toc_22">SUBSTRING function</h1>

<p>It is does not include the second index position<br/>
<code>SUBSTRING(string, startIndex, stopIndex)</code>, <code>startIndex</code> starts from <code>0</code>, and <code>stopIndex</code> is not included.<br/><br/>
Given a field named <code>alpha</code> whose value is <code>ABCDEF</code>, to return substring <code>BCD</code> use this statement: <code>SUBSTRING(alpha,1,4)</code>. Note that <code>1</code> is the index of <code>B</code> (the first character of the substring) and <code>4</code> is the index of <code>E</code> (the character <strong>following</strong> the last character of the substring).</p>

<p>Note that the description is different in the official documents both for Pig 0.12:<br/>
* <a href="https://pig.apache.org/docs/r0.12.0/func.html#substring">Correct official document</a> <br/>
* <a href="https://pig.apache.org/docs/r0.12.0/api/org/apache/pig/builtin/SUBSTRING.html">Wrong official document</a></p>

<h1 id="toc_23">Nested FOREACH</h1>

<ol>
<li>Allowed operation are <code>CROSS</code>, <code>DISTINCT</code>, <code>FILTER</code>, <code>FOREACH</code>, <code>LIMIT</code>, <code>ORDER BY</code>, and Project operation. <a href="http://pig.apache.org/docs/r0.15.0/basic.html#foreach">Official Document</a></li>
<li><code>SPLIT</code> does not work inside nested FOREACH</li>
</ol>

<h1 id="toc_24">Function are allowed inside FOREACH</h1>

<p><a href="http://stackoverflow.com/a/26996221/4229125">an example of using <code>SUM</code> inside FOREACH from stackoverflow</a>  </p>

<pre><code class="language-sql">file = LOAD &#39;input.txt&#39; USING PigStorage() AS (type: chararray, year: chararray,
match_count: float, volume_count: float);
grouped = GROUP file BY type;
group_operat = FOREACH grouped {
                                 sum_m = SUM(file.match_count);
                                 sum_v = SUM(file.volume_count);
                                 GENERATE group,(float)(sum_m/sum_v) as sum_mv;
                                }
</code></pre>

<p>Lesson learned: Do not use 2 levels of <code>FOREACH</code> if it is not absolutely necessary.</p>

<p>Wrong </p>

<pre><code class="language-sql">acookie_log_input = LOAD &#39;$INPUT_ACOOKIE&#39; USING LzoPigStorage()
                       AS (acookie:chararray, ip:chararray, time_stamp:DateTime, useragent:chararray, browser:chararray,
                           os:chararray, url:chararray, province:chararray, acookie_daily_tag:chararray,
                           url_catelevel1:chararray, url_catelevel2:chararray, date:DateTime);
acookie_log_input_m0 = FOREACH acookie_log_input GENERATE acookie AS acookie, SUBSTRING(province, 0, 2) AS province:chararray;
acookie_log_input_m1 = FILTER acookie_log_input_m0 BY province IS NOT NULL;
acookie_log_input_m2 = FOREACH (GROUP acookie_log_input_m1 BY (acookie, province)) GENERATE FLATTEN(group) AS (acookie, province),
                        COUNT(acookie_log_input_m1) AS acookie_province_cnt;
-- Does not work from here
acookie_province = FOREACH (GROUP acookie_log_input_m2 BY acookie) {
                    total_cnt = SUM(acookie_log_input_m2.acookie_province_cnt);
                    normalized = FOREACH acookie_log_input_m2 GENERATE (int) (acookie_province_cnt * 100 / total_cnt) AS hist:int;
                    sorted = ORDER normalized BY hist DESC;
                    GENERATE group AS acookie, BagToString(sorted.hist) AS hist:chararray;};
</code></pre>

<p>Correct</p>

<pre><code class="language-sql">acookie_province_m0 = FOREACH (GROUP acookie_log_input_m2 BY acookie) {
                    total_cnt = SUM(acookie_log_input_m2.acookie_province_cnt);
                    GENERATE group AS acookie, FLATTEN(acookie_log_input_m2.acookie_province_cnt) AS (acookie_province_cnt),
                    total_cnt AS total_cnt;};
acookie_province_m1 = FOREACH acookie_province_m0 GENERATE acookie AS acookie, (int) (acookie_province_cnt*100/total_cnt) AS hist:int;
acookie_province = FOREACH (GROUP acookie_province_m1 BY acookie) {
                    sorted = ORDER acookie_province_m1 BY hist DESC;
                    GENERATE group AS acookie, BagToString(sorted.hist) AS hist:chararray;};
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">File Name Pattern Substitution</a>
</li>
<li>
<a href="#toc_1"><code>ToDate(milliseconds)</code> gives wrong date</a>
</li>
<li>
<a href="#toc_2">DateTime SimpleDateFormat</a>
</li>
<li>
<a href="#toc_3">Parameter substitution Shell Runner subtle bug</a>
</li>
<li>
<a href="#toc_4">Becareful of <code>DateTime</code> data type in pig</a>
</li>
<li>
<a href="#toc_5">More pitfalls for <code>DateTime</code> data type</a>
</li>
<li>
<a href="#toc_6">Setting of memory</a>
</li>
<li>
<a href="#toc_7">Pig Unit Test <code>override()</code> method</a>
</li>
<li>
<a href="#toc_8">Error message for wrong reference operator</a>
</li>
<li>
<a href="#toc_9">Pig Java UDF Unit Test</a>
</li>
<li>
<a href="#toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></a>
</li>
<li>
<a href="#toc_11">specify function output data type</a>
</li>
<li>
<a href="#toc_12">using <code>u0001</code> as the delimiter</a>
</li>
<li>
<a href="#toc_13">Error with SUBSTRING and SIZE function</a>
</li>
<li>
<a href="#toc_14">oneline to order two fields</a>
</li>
<li>
<a href="#toc_15">sort with duplicates</a>
</li>
<li>
<a href="#toc_16">input and output are reserved keyword in pig</a>
</li>
<li>
<a href="#toc_17">Mystery ERROR 2999</a>
</li>
<li>
<a href="#toc_18">Cumsum</a>
</li>
<li>
<a href="#toc_19">A pretty good graph cluster to pair Pig example code</a>
</li>
<li>
<a href="#toc_20">A pretty good Score bucketize and cumsum precision Pig example code</a>
</li>
<li>
<a href="#toc_21">Parameter Substitution in Pig</a>
</li>
<li>
<a href="#toc_22">SUBSTRING function</a>
</li>
<li>
<a href="#toc_23">Nested FOREACH</a>
</li>
<li>
<a href="#toc_24">Function are allowed inside FOREACH</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[technical analysis]]></title>
    <link href="www.echo-ohce.com/14742423250761.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250761.html</id>
    <content type="html"><![CDATA[
<p>This documents my learning of stock technical analysis, keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">python packages to use</a>
</li>
<li>
<a href="#toc_1">python notebook extra setting</a>
</li>
<li>
<a href="#toc_2">Some good blogs</a>
</li>
<li>
<a href="#toc_3">Using tushare to get historical data</a>
</li>
<li>
<a href="#toc_4">Using pandas and yahoo is a better choice of getting daily data</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">python packages to use</h1>

<ol>
<li>zipline - for testing framework</li>
<li>tushare - for chinese stock market data</li>
<li><a href="https://github.com/mrjbq7/ta-lib">talib</a> - for indicators and functions. The official document is <a href="https://mrjbq7.github.io/ta-lib/doc_index.html">here</a></li>
</ol>

<h1 id="toc_1">python notebook extra setting</h1>

<pre><code class="language-python">import zipline as zp
import tushare as ts
ts.set_token(&#39;ef0f57f6c63416271c140912cb1e2bf58e1580f1aa5147efa19538085b21cf60&#39;) # use your own free api key
</code></pre>

<h1 id="toc_2">Some good blogs</h1>

<ol>
<li><a href="http://pythontrader.blogspot.com">Python</a></li>
<li><a href="http://tradingwithpython.blogspot.com">Trading with Python</a></li>
</ol>

<h1 id="toc_3">Using tushare to get historical data</h1>

<ul>
<li>A good <a href="http://www.360doc.com/content/16/0213/20/7249274_534357168.shtml">post</a> to quick check.</li>
<li>The stock codename in ts may be different than normal stock software.</li>
<li>to get the shanghai and shenzhen index use the following code example</li>
</ul>

<pre><code class="language-python">sh_index=ts.get_hist_data(&#39;sh&#39;)
sz_index=ts.get_hist_data(&#39;sz&#39;)
</code></pre>

<h1 id="toc_4">Using pandas and yahoo is a better choice of getting daily data</h1>

<p>tushare is not very stable, it is slow. It also has 3 years limitation.<br/><br/>
For using pandas and yahoo, these are resources:</p>

<ul>
<li><a href="https://yongle.gitbooks.io/data-science/content/note/4pandas/yahoo.html">A good post</a></li>
<li><a href="http://pandas-datareader.readthedocs.io/en/latest/">official docs</a></li>
<li><a href="http://jxyyjm.lofter.com/post/1d11b81d_7bbbc16"></a></li>
</ul>

<p><br/>
.SS<br/>
.SZ</p>

<pre><code class="language-python">import pandas_datareader.data as web

daily = web.DataReader(&#39;000001.SS&#39;, &#39;yahoo&#39;, &#39;2000-01-01&#39;, &#39;2016-01-01&#39;)
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">python packages to use</a>
</li>
<li>
<a href="#toc_1">python notebook extra setting</a>
</li>
<li>
<a href="#toc_2">Some good blogs</a>
</li>
<li>
<a href="#toc_3">Using tushare to get historical data</a>
</li>
<li>
<a href="#toc_4">Using pandas and yahoo is a better choice of getting daily data</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[giraph with jython]]></title>
    <link href="www.echo-ohce.com/14742423250588.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250588.html</id>
    <content type="html"><![CDATA[
<p>This post specifically documents the key point of using giraph with jython. Once everything is done, I will write another post for a tutorial with giraph + jython</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Hadoop version</a>
</li>
<li>
<a href="#toc_1">Use <code>jython-standalone</code> dependency not <code>jython</code></a>
</li>
<li>
<a href="#toc_2">Run from jython REPL</a>
</li>
<li>
<a href="#toc_3">Config intellij for Jython and Java</a>
</li>
<li>
<a href="#toc_4">Run giraph jython from Hadoop Yarn</a>
</li>
<li>
<a href="#toc_5">pitfall of <code>TypeHolder</code> and <code>TypesHolder</code></a>
</li>
<li>
<a href="#toc_6">python and java data type</a>
</li>
<li>
<a href="#toc_7">debug print statement inside python file</a>
</li>
<li>
<a href="#toc_8">Jython type to java pitfall</a>
</li>
<li>
<a href="#toc_9">import python package or jython package</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Hadoop version</h1>

<p>In order to use Jython, in the maven pom.xml setting up, do <font color='red'><strong>NOT</strong></font> include other hadoop dependencies, but use the below specific version of hadoop:  </p>

<pre><code class="language-xml">&lt;dependencies&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.apache.giraph&lt;/groupId&gt;
       &lt;artifactId&gt;giraph-core&lt;/artifactId&gt;
       &lt;version&gt;1.1.0-hadoop2&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
       &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
       &lt;version&gt;2.5.0&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.python&lt;/groupId&gt;
       &lt;artifactId&gt;jython-standalone&lt;/artifactId&gt;
       &lt;version&gt;2.7.0&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;junit&lt;/groupId&gt;
       &lt;artifactId&gt;junit&lt;/artifactId&gt;
       &lt;version&gt;4.11&lt;/version&gt;
       &lt;scope&gt;test&lt;/scope&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.mockito&lt;/groupId&gt;
       &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;
       &lt;scope&gt;test&lt;/scope&gt;
       &lt;version&gt;1.9.5&lt;/version&gt;
   &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>

<p>Otherwise, there will be error message like:</p>

<blockquote>
<p>Exception in thread &quot;main&quot; java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected`  </p>
</blockquote>

<p><a href="http://stackoverflow.com/questions/22630323/hadoop-java-lang-incompatibleclasschangeerror-found-interface-org-apache-hadoo">Because Hadoop 1.0.3: JobContext is a class, but in Hadoop 2.0.0: JobContext is an interface</a></p>

<p>Most important points in this pom are:</p>

<ul>
<li>use giraph version of <code>1.1.0-hadoop2</code> because nowadays we are all using hadoop 2.</li>
<li>use <code>jython-standalone</code> not <code>jython</code> (see below)</li>
</ul>

<h1 id="toc_1">Use <code>jython-standalone</code> dependency not <code>jython</code></h1>

<p>Otherwise it will show error message like:  </p>

<blockquote>
<p>Exception in thread &quot;main&quot; ImportError: Cannot import site module and its dependencies: No module named site`  </p>
</blockquote>

<p>This is because <code>jython-standalone-2.7.0.jar</code> has <code>Lib</code> folder but <code>jython-2.7.0.jar</code> does not have that folder.  </p>

<p>Another possible issue about jython with error message like:  </p>

<blockquote>
<p>java.lang.IncompatibleClassChangeError: Found class com.kenai.jffi.InvocationBuffer, but interface was expected  </p>
</blockquote>

<p>is also about the jar of jython. Using <a href="https://github.com/scijava/jython-shaded"><code>jython-shade</code></a> will solve this issue.   </p>

<h1 id="toc_2">Run from jython REPL</h1>

<ul>
<li>For installing the jython, remeber to add the following path to <code>.bashrc</code><br/></li>
</ul>

<pre><code class="language-bash">export PATH=/Users/yugan/jython2.7.0/bin:$PATH
</code></pre>

<ul>
<li>Need import the following two pathes, in order to get the java <code>import</code> work.<br/></li>
</ul>

<pre><code class="language-python">import sys
sys.path.append(&quot;/Users/yugan/.m2/repository/org/apache/giraph/giraph-core/1.1.0/giraph-core-1.1.0-hadoop_2_6_dependencies.jar&quot;)
sys.path.append(&quot;/Users/yugan/.m2/repository/org/apache/giraph/giraph-core/1.1.0/giraph-core-1.1.0.jar&quot;)
sys.path.append(&quot;/Users/yugan/.m2/repository/org/apache/hadoop/hadoop-core/2.5.0/hadoop-core-2.5.0.jar&quot;)

from org.apache.giraph.jython import JythonJob
</code></pre>

<h1 id="toc_3">Config intellij for Jython and Java</h1>

<ol>
<li>using <code>command + ;</code> to call out the project setting.</li>
<li>project SDK should be java (to recognize java packages etc)</li>
<li>In the <code>Facets</code>, add Python framework, and config <code>python interpreter</code> as <code>jython</code></li>
</ol>

<h1 id="toc_4">Run giraph jython from Hadoop Yarn</h1>

<p>Got a working version using my own <code>graph-distance.py</code> example.  </p>

<ol>
<li>python file can be put anywhere, but I put it under the <code>resources</code> folder at:<br/>
<code>/src/main/resources/graph-distance.py</code><br/>
In this python file, it builds the python class <code>GraphDistance</code> which subclasses giraph&#39;s <code>JythonComputation</code>. There is no way that we can give the type parameters (generics), but we are going to handle it in a different way later.<br/></li>
<li>the graph input file can be anywhere, but I put it under the <code>resources</code> folder too.<br/>
It is put on hdfs.<br/></li>
<li>To specify the type parameters of <code>&lt;I, V, E, M, M&gt;</code>, I built the <code>JythonTypes</code> class which implements the <code>TypesHolder</code> interface.</li>
<li>I also created the corresponding <code>I V E M</code> classes if it is necessary.</li>
<li>Build the jar file which need include the custom build type classes, and also the original giraph classes. I used the <code>maven-assembly-plugin</code> to build the jar with all dependencies.</li>
<li>In the working folder on the cluster, we have the <u>python</u> file and the <u>jar</u> file (<code>giraph-jython-1.0-SNAPSHOT-jar-with-dependencies.jar</code>). Then run the shell script.</li>
</ol>

<p>The pom.xml file and the shell runner file:</p>

<pre><code class="language-xml">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;com.adsymp.dpp.giraph.jython&lt;/groupId&gt;
    &lt;artifactId&gt;giraph-jython&lt;/artifactId&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;giraph-jython&lt;/name&gt;

    &lt;parent&gt;
        &lt;groupId&gt;com.adsymp&lt;/groupId&gt;
        &lt;artifactId&gt;base-pom&lt;/artifactId&gt;
        &lt;version&gt;0.0.1&lt;/version&gt;
    &lt;/parent&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.giraph&lt;/groupId&gt;
            &lt;artifactId&gt;giraph-core&lt;/artifactId&gt;
            &lt;version&gt;1.1.0-hadoop2&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;2.5.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.python&lt;/groupId&gt;
            &lt;artifactId&gt;jython-standalone&lt;/artifactId&gt;
            &lt;version&gt;2.7.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.11&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.mockito&lt;/groupId&gt;
            &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
            &lt;version&gt;1.9.5&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;descriptorRefs&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre>

<p>Build the jar:</p>

<pre><code class="language-bash">mvn clean package assembly:single -DskipTests
</code></pre>

<p>run the shell script:</p>

<pre><code class="language-bash">hdfs dfs -rm -r giraph-jython/output
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

yarn jar giraph-jython-1.0-SNAPSHOT-jar-with-dependencies.jar \
        com.adsymp.dpp.giraph.jython.GiraphJythonRunner \
        -Dmapred.job.queue.name=datascience \
        -Dgiraph.zkList=$ZKLIST \
        graph-distance.py \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        --jythonClass GraphDistance \
        --typesHolder com.adsymp.dpp.giraph.jython.JythonTypes \
        -eif org.apache.giraph.io.formats.IntNullTextEdgeInputFormat \
        -eip giraph-jython/input/tiny_graph \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op giraph-jython/output \
        -w 1 \
        -ca giraph.SplitMasterWorker=false
</code></pre>

<h1 id="toc_5">pitfall of <code>TypeHolder</code> and <code>TypesHolder</code></h1>

<p>Be careful, the <code>TypesHolder</code> interface (which is what we need supply to the cmd) is different than the <code>TypeHolder</code>.</p>

<ul>
<li>The first one is at <code>org.apache.giraph.conf.TypesHolder</code> - Interface for classes that are parameterized by all of the Giraph types.<br/></li>
<li>The second one is an inner class at <code>org.apache.giraph.jython.JythonJob</code></li>
</ul>

<h1 id="toc_6">python and java data type</h1>

<p>Converting python data type to java data type has a lot of limitations.</p>

<p>For now, data come from and come into java are all in the type of array(). To use it in python, convert it to list using array.tolist() method.<br/>
To pass it to java, convert it to list then use array(list, <code>typecode</code>) <a href="http://www.jython.org/jythonbook/en/1.0/DataTypes.html#jython-specific-collections">Typecode</a> refers to Table 2-8</p>

<pre><code class="language-python">from org.python.modules.jarray import array

</code></pre>

<h1 id="toc_7">debug print statement inside python file</h1>

<p>Very tricky that the standard python way of print will not work in the python file.</p>

<p><strong>Wrong</strong>:  </p>

<pre><code class="language-python">print &#39;vertext_id: {0:d} received {1:d} messages&#39;.format(int(v_id), count)
</code></pre>

<p><strong>Correct</strong></p>

<pre><code class="language-python">print &#39;vertext_id: %d received %d messages&#39; % (int(v_id), count)
</code></pre>

<h1 id="toc_8">Jython type to java pitfall</h1>

<p>One subtle bug (very hard to debug) is the <code>Writable</code> type. When directly calling giraph&#39;s java method, such as <code>sendMessage(vertexID, MessageOut)</code>, the type of <code>vertexID</code> should be <code>LongWritable</code>, but inside jython, the default type is just <code>long</code>, and the framework will not automatically wrap it for you.</p>

<p><strong>Wrong</strong>  </p>

<pre><code class="language-python">self.sendMessage(id, outgoing_Message)
</code></pre>

<p><strong>Correct</strong>  </p>

<pre><code class="language-python">from org.apache.hadoop.io import LongWritable

self.sendMessage(LongWritable(id), outgoing_Message)
</code></pre>

<p>Why it is so hard to debug? Because there is <strong>NO</strong> error message! try add some print statement inside python file to check the type.</p>

<h1 id="toc_9">import python package or jython package</h1>

<p>At least for my project setting (intellij with jython facets), import from python does not work well.</p>

<p><strong>Work</strong>  </p>

<pre><code class="language-python">from org.python.modules.itertools import itertools

for id1, id2 in itertools.combinations(newSeenIds, 2):
</code></pre>

<p><strong>Not work</strong>  </p>

<pre><code class="language-python">import itertools

for id1, id2 in itertools.combinations(newSeenIds, 2):
</code></pre>

<p><strong>Work</strong>  </p>

<pre><code class="language-python">from java.util import Random

@staticmethod
    def isSampledID(id):
        random = Random(id)
        return random.nextInt(GraphDistance.sample) == 0
</code></pre>

<p><strong>Not work</strong>  </p>

<pre><code class="language-python">import random

@staticmethod
    def isSampledID(id):
        
        return random.randint(1, GraphDistance.sample) == 1
</code></pre>

<p>In favor of import java modules. some python extended libraries is not included in jython. <a href="http://forums.parasoft.com/index.php?showtopic=2302#">refer to this post</a> A side note is that java&#39;s random is exclusive, but jython&#39;s random is inclusive.</p>

<hr/>

<ul>
<li>
<a href="#toc_0">Hadoop version</a>
</li>
<li>
<a href="#toc_1">Use <code>jython-standalone</code> dependency not <code>jython</code></a>
</li>
<li>
<a href="#toc_2">Run from jython REPL</a>
</li>
<li>
<a href="#toc_3">Config intellij for Jython and Java</a>
</li>
<li>
<a href="#toc_4">Run giraph jython from Hadoop Yarn</a>
</li>
<li>
<a href="#toc_5">pitfall of <code>TypeHolder</code> and <code>TypesHolder</code></a>
</li>
<li>
<a href="#toc_6">python and java data type</a>
</li>
<li>
<a href="#toc_7">debug print statement inside python file</a>
</li>
<li>
<a href="#toc_8">Jython type to java pitfall</a>
</li>
<li>
<a href="#toc_9">import python package or jython package</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[git]]></title>
    <link href="www.echo-ohce.com/14742423250638.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250638.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Back to the unaltered HEAD</a>
</li>
<li>
<a href="#toc_1">Messed up local master, come back to the origin/master</a>
</li>
<li>
<a href="#toc_2">Reset one file</a>
</li>
<li>
<a href="#toc_3">Save change in one branch and apply it (partially apply it) to another branch</a>
</li>
<li>
<a href="#toc_4">Add only modified changes and ignore untracked (newly added) files</a>
</li>
<li>
<a href="#toc_5">Syntax of <code>.gitignore</code></a>
</li>
<li>
<a href="#toc_6">Untracked directory and file</a>
</li>
<li>
<a href="#toc_7">discard unstaged changes in Git</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Back to the unaltered HEAD</h1>

<p>Run these two in order:  </p>

<pre><code class="language-bash">git reset --hard HEAD
git clean -df
</code></pre>

<h1 id="toc_1">Messed up local master, come back to the origin/master</h1>

<pre><code class="language-bash">git checkout -B master origin/master
</code></pre>

<p>If <code>-B</code> is given, the new branch is created if it doesnt exist; otherwise, it is reset.  </p>

<h1 id="toc_2">Reset one file</h1>

<pre><code class="language-bash">git checkout HEAD -- my-file.txt
</code></pre>

<p><a href="http://stackoverflow.com/questions/7147270/hard-reset-of-a-single-file">Reference post1</a><br/><br/>
<a href="http://stackoverflow.com/questions/6561142/difference-between-git-checkout-filename-and-git-checkout-filename/6561160#6561160">Reference post2</a></p>

<h1 id="toc_3">Save change in one branch and apply it (partially apply it) to another branch</h1>

<pre><code class="language-bash">git stash
git checkout branch2
git stash list       # to check the various stash made in different branch
git stash pop        # pop out the last stash, and it will be deleted from the stash
git stash apply stash@{0}    # to select the stash and apply to branch2, the stash still saved in the list
git stash -u   # to stash currenlty untracked (newly added) files
</code></pre>

<h1 id="toc_4">Add only modified changes and ignore untracked (newly added) files</h1>

<pre><code class="language-bash">git add .   # stage all changed / untracked (newly added), but not includes deleted
git add -u  # stage all changed / deleted, but not untracked (newly added)
git add -A  # doing both of above
</code></pre>

<h1 id="toc_5">Syntax of <code>.gitignore</code></h1>

<pre><code class="language-bash">**/foo # matches file or directory &quot;foo&quot; anywhere
foo # the same as above  
**/foo/bar # matches file or   directory &quot;bar&quot; anywhere that is directly under directory &quot;foo&quot;  
abc/** # matches all files inside directory &quot;abc&quot;, infinite depth  
abc/* # matches all files inside directory &quot;abc&quot;, but not its sub-directories.  
a/**/b # match 0 or more directories in between  
a/**/b # matches &quot;a/b&quot;, &quot;a/x/b&quot;, &quot;a/x/y/b&quot; and so on.
</code></pre>

<h1 id="toc_6">Untracked directory and file</h1>

<ol>
<li><p>File previously tracked (in git repository) need remove it from the repo but keep it at local. <em>This is called as delete the cache</em><br/><br/>
<code>git rm -r -f --cached mydirectory/myfile</code><br/><br/>
the <code>-r</code> is recursive, the <code>-f</code> is force (in case, this tracked file has uncommitted change).  </p></li>
<li><p>Only change the local repo&#39;s <code>.gitignore</code> without change the remote repo&#39;s tracking behavior. Add ignore into <code>.git/info/exclude</code> instead of the <code>.gitignore</code> file at root.<br/><br/>
<a href="http://stackoverflow.com/questions/767147/ignore-the-gitignore-file-itself">reference post</a></p></li>
</ol>

<h1 id="toc_7">discard unstaged changes in Git</h1>

<pre><code class="language-bash">git checkout -- .
</code></pre>

<p>Make sure to include the last <code>.</code></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Back to the unaltered HEAD</a>
</li>
<li>
<a href="#toc_1">Messed up local master, come back to the origin/master</a>
</li>
<li>
<a href="#toc_2">Reset one file</a>
</li>
<li>
<a href="#toc_3">Save change in one branch and apply it (partially apply it) to another branch</a>
</li>
<li>
<a href="#toc_4">Add only modified changes and ignore untracked (newly added) files</a>
</li>
<li>
<a href="#toc_5">Syntax of <code>.gitignore</code></a>
</li>
<li>
<a href="#toc_6">Untracked directory and file</a>
</li>
<li>
<a href="#toc_7">discard unstaged changes in Git</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[vim]]></title>
    <link href="www.echo-ohce.com/14742423250782.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250782.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">move (jump) cursor to previous location or previous files</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">move (jump) cursor to previous location or previous files</h1>

<table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>ma</code></td>
<td>set mark <font color='red'>a</font> at current cursor location</td>
</tr>
<tr>
<td><code>&#39;a</code> - quotemark</td>
<td>jump to line of mark <font color='red'>a</font> (first non-blank character in line)</td>
</tr>
<tr>
<td><code>&#39;a</code> - backtick</td>
<td>jump to position (line and column) of mark <font color='red'>a</font></td>
</tr>
<tr>
<td><code>&#39;&#39;</code> - 2 quotemark</td>
<td>jump to the head of the line of previous location</td>
</tr>
<tr>
<td><code>``</code> -2 backtick</td>
<td>jump to the previous location</td>
</tr>
<tr>
<td><code>:marks</code></td>
<td>list all current marks (including marks of <strong>other files</strong>)</td>
</tr>
<tr>
<td><code>:help mark-motions</code></td>
<td>help for mark related stuff</td>
</tr>
<tr>
<td><code>:help jump-motions</code></td>
<td>help for jump related stuff</td>
</tr>
</tbody>
</table>

<p>reference <a href="http://vim.wikia.com/wiki/Using_marks">using marks</a><br/>
reference <a href="http://stackoverflow.com/questions/5052079/move-cursor-to-its-last-position">stackoverflow</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">move (jump) cursor to previous location or previous files</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[setting]]></title>
    <link href="www.echo-ohce.com/14742423250740.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250740.html</id>
    <content type="html"><![CDATA[
<p>This documents various settings that I normally use. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">python notebook imports and setting</a>
</li>
<li>
<a href="#toc_1">Mac multiple desktop screen spaces setting.</a>
</li>
<li>
<a href="#toc_2">python conda virtual environment and jupyter notebook kernel</a>
</li>
<li>
<a href="#toc_3">Mac OS gets very busy on background process</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">python notebook imports and setting</h1>

<pre><code class="language-python">import os
import pandas as pd
import numpy as np
import math
import plotly.plotly as py
import plotly.offline as po
import plotly.graph_objs as go
import plotly.tools as tls
import colorlover as cl
import cufflinks as cf
import subprocess
from ipywidgets import interact
import glob # for shell expansion
from IPython.display import display, HTML, Image
import matplotlib.pyplot as plt

%matplotlib inline

po.init_notebook_mode()
cf.set_config_file(theme=&#39;white&#39;)
pd.set_option(&#39;precision&#39;, 3) # printing precision
np.set_printoptions(precision=3) # printing precision 
cf.go_offline() # using cufflinks at offline mode
</code></pre>

<h1 id="toc_1">Mac multiple desktop screen spaces setting.</h1>

<p><a href="http://www.tweaking4all.com/os-tips-and-tricks/macosx-tips-and-tricks/mac-multiple-desktops-spaces/">Very good post</a><br/><br/>
Using <code>SteerMouse</code> to set the mouse shortcut.</p>

<h1 id="toc_2">python conda virtual environment and jupyter notebook kernel</h1>

<p>Need install a plugin <code>nb_conda_kernels</code></p>

<ol>
<li>switch to the desired virtual environment</li>
<li>install plugin</li>
<li>run jupyter notebook then pick the desired kernel from notebook</li>
</ol>

<pre><code class="language-bash">source activate environment_name
conda install -c conda-forge nb_conda_kernels
jupyter notebook

source deactive
</code></pre>

<p><a href="https://github.com/Anaconda-Platform/nb_conda_kernels">plugin&#39;s main page</a><br/>
<a href="http://stackoverflow.com/a/38880722">stackoverflow post</a></p>

<h1 id="toc_3">Mac OS gets very busy on background process</h1>

<ol>
<li>It is because of <code>optimal Layout</code></li>
<li>The app to check the system performance is <code>Activity Monitor</code></li>
<li>There is another process named <code>distnoted</code> taking a lot of CPU resources. This is Apple&#39;s kernel process, but it is safely to be removed. Using the script (saved as <code>install_checkdistnoted.sh</code> add a cron job to kill this process when it takes too much resources.<br/>
See the <a href="http://apple.stackexchange.com/a/234478">original post for this scirpt</a><br/></li>
</ol>

<hr/>

<ul>
<li>
<a href="#toc_0">python notebook imports and setting</a>
</li>
<li>
<a href="#toc_1">Mac multiple desktop screen spaces setting.</a>
</li>
<li>
<a href="#toc_2">python conda virtual environment and jupyter notebook kernel</a>
</li>
<li>
<a href="#toc_3">Mac OS gets very busy on background process</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[metrics]]></title>
    <link href="www.echo-ohce.com/14742423250680.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250680.html</id>
    <content type="html"><![CDATA[
<p>This document collects statistics metrics used in my work.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">precision</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">precision</h1>

<p><img src="media/14742423250680/14742450671711.jpg" alt=""/></p>

<hr/>

<ul>
<li>
<a href="#toc_0">precision</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Github Page, Hexo, MWeb, GoDaddy setting]]></title>
    <link href="www.echo-ohce.com/14742423250557.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250557.html</id>
    <content type="html"><![CDATA[
<p>I tried to use MWeb + Hexo to setup this blog, but it does not work out well. Give this up, turn back to MWeb only</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Set two separate GitLab (Corp) and GitHub (Personal) account</a>
</li>
<li>
<a href="#toc_1">Github page</a>
</li>
<li>
<a href="#toc_2">Hexo</a>
</li>
<li>
<a href="#toc_3">Freemind Hexo theme</a>
</li>
<li>
<a href="#toc_4">Mweb</a>
</li>
<li>
<a href="#toc_5">GoDaddy</a>
</li>
<li>
<a href="#toc_6">Hexo works with MathJax / LaTex</a>
</li>
<li>
<a href="#toc_7">Future work</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Set two separate GitLab (Corp) and GitHub (Personal) account</h1>

<p>The most important steps are:<br/>
1. Edit <code>~/.ssh/config</code> file and set up the two hosts.<br/>
2. Generate two separate ssh keys and put them in the folder <code>~/.ssh/</code><br/>
3. The main account is set by <code>git config --global</code> and the other account is set by locally <code>git config</code> overridden. </p>

<p>The main reference post is <a href="https://gist.github.com/rosswd/e1afd2b0b0d515517eac">here</a></p>

<h1 id="toc_1">Github page</h1>

<p>Nothing special, just first need first create an empty repository using the unique name required.</p>

<h1 id="toc_2">Hexo</h1>

<p>Hexo is used to build everything and managers everything. <br/>
It sets the local folder structures, manages the style / theme / plugin, generates the sites, publish to the github pages. Some important things about hexo</p>

<ol>
<li><a href="">hexo installation and integration with github pages</a></li>
<li><a href="https://hexo.io/docs/index.html">Hexo basic commands</a></li>
<li><a href="https://hexo.io/docs/front-matter.html">Hexo Front-matter</a>. The <code>freemind</code> theme that I am using provides extra Front-matter: <code>toc</code> and <code>top</code></li>
<li>[YGMARK] in <code>_config.yml</code> setting, after <code>key:</code> a space must be used, otherwise all sorts of bugs.</li>
</ol>

<h1 id="toc_3">Freemind Hexo theme</h1>

<ul>
<li><a href="https://github.com/wzpan/hexo-theme-freemind/">Installation Official</a></li>
<li><a href="http://baoxiehao.com/2014/05/17/Hexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96/">Freemind Hexo</a></li>
<li><a href="http://masikkk.com/blog/hexo-4-usage-of-hexo-theme/">freemind search engine</a></li>
</ul>

<h1 id="toc_4">Mweb</h1>

<p>Mweb is simply the Markdown writing program.<br/>
It has a very good full <a href="http://www.mweb.im/markdown-syntax-guide-full-version.html">Markdown syntax guide</a>.<br/>
MWeb, Command + E (cmd + Llibrary), hexosource, Display namegithub blog(), Media Save PathAbsolute, MWeb.<br/><br/>
<img src="media/14742423250557/14742438643424.jpg" alt=""/></p>

<h1 id="toc_5">GoDaddy</h1>

<ul>
<li>Add a <code>CNAME</code> file in the <code>source</code> folder: <code>www.echo-ohce.com</code></li>
<li><p>Go to GoDaddy and config the domain name</p>

<ul>
<li><p>Add an &quot;A (Host)&quot; record with &quot;host&quot; = <code>@</code> and &quot;Points to&quot; = <code>192.30.252.153</code> or <code>192.30.252.154</code><br/>
<img src="media/14742423250557/14742439346348.png" alt=""/></p></li>
<li><p>Create a CNAME record<br/><br/>
<img src="media/14742423250557/14742439461628.png" alt=""/></p></li>
</ul></li>
<li><p><a href="https://help.github.com/articles/setting-up-an-apex-domain/">Official help page</a></p></li>
<li><p><a href="http://andrewsturges.com/blog/jekyll/tutorial/2014/11/06/github-and-godaddy.html">My reference post</a></p></li>
<li><p>Specical Notes: CNAME<code>http://</code></p></li>
</ul>

<h1 id="toc_6">Hexo works with MathJax / LaTex</h1>

<ul>
<li>install plugin <a href="https://github.com/akfish/hexo-math">hexo-math</a> at the root folder of the blog <code>npm install hexo-math --save</code></li>
<li>Update the root <code>_config.xml</code> append the following:</li>
</ul>

<pre><code class="language-xml"># Math
math:
  engine: &#39;mathjax&#39;
  mathjax:
    src: &quot;//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;
    config:
      tex2jax:
        inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ]
        skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;, &#39;code&#39;]
        processEscaps: true
      TeX:
        equationNumbers:
          autoNumber: &quot;AMS&quot;
</code></pre>

<ul>
<li>Inline LaTex <code>$...$</code>: \(x \rightarrow y\)</li>
<li>Block <code>$$...$$</code>:  \[\cos 2\theta = \cos^2 \theta - \sin^2 \theta =  2 \cos^2 \theta - 1\]</li>
<li>Multiple lines <code>{% math %} ... {% endmath %}</code>:</li>
</ul>

<p>{% math %}<br/>
\begin{aligned}<br/>
\dot{x} &amp; = \sigma(y-x) \<br/>
\dot{y} &amp; = \rho x - y - xz \<br/>
\dot{z} &amp; = -\beta z + xy<br/>
\end{aligned}<br/>
{% endmath %}</p>

<h1 id="toc_7">Future work</h1>

<p>GitHub  \(\rightarrow\)  GitLab <br/>
1. <a href="https://yudachi.biz/2016/05/27/moving-to-gitlab-pages/"> GitLab and Hexo</a><br/>
2. <a href="http://blog.vanjor.com/2016/05/hexo-github-blog-install.html">DNS</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Set two separate GitLab (Corp) and GitHub (Personal) account</a>
</li>
<li>
<a href="#toc_1">Github page</a>
</li>
<li>
<a href="#toc_2">Hexo</a>
</li>
<li>
<a href="#toc_3">Freemind Hexo theme</a>
</li>
<li>
<a href="#toc_4">Mweb</a>
</li>
<li>
<a href="#toc_5">GoDaddy</a>
</li>
<li>
<a href="#toc_6">Hexo works with MathJax / LaTex</a>
</li>
<li>
<a href="#toc_7">Future work</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[bayesian statistics]]></title>
    <link href="www.echo-ohce.com/14742423250523.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250523.html</id>
    <content type="html"><![CDATA[
<p>This documents my personal understandings about bayesian, keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Likelihood function is not a pdf</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Likelihood function is not a pdf</h1>

<p>(We are here only talking about continuous distribution case since I used pdf here.)<br/><br/>
Let the observation as \(O\) and the parameters as \(\theta\), the probability of seeing \(O\) given \(\theta\) is \(p(O|\theta)\). This is a pdf. It is a function of \(O\), and \(\int_{O}{p(O|\theta)\ dO}=1\), which means integrate through all possible observation \(O\), the sum of probability should be 1.<br/><br/>
The likelihood is the other way around as: given the observation \(O\) that we have seen, the likelihood that the parameter is value \(\theta\).<br/>
\[L(O|\theta)=p(O|\theta)\]<br/>
If we do an integration over all possible \(\theta\), \(\int_{\theta}{L(O|\theta)\ d\theta}\neq1\)<br/><br/>
<a href="http://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability">post on stackoverflow about the difference between likelihood and probability</a><br/>
<a href="http://stats.stackexchange.com/questions/31238/what-is-the-reason-that-a-likelihood-function-is-not-a-pdf">post on stackoverflow about likelihood and pdf</a>  </p>

<hr/>

<ul>
<li>
<a href="#toc_0">Likelihood function is not a pdf</a>
</li>
</ul>


]]></content>
  </entry>
  
</feed>
