<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  spark - Echo-ohcE
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="Echo-ohcE" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:www.echo-ohce.com ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; Echo-ohcE</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="tips.html">tips</a></li>
        
            <li><a href="documentation.html">documentation</a></li>
        
            <li><a href="expired.html">expired</a></li>
        
            <li><a href="stock.html">stock</a></li>
        
            <li><a href="theory.html">theory</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
  $(function(){
    $('#menu_item_index').addClass('is_active');
  });
</script>
<div class="row">
  <div class="large-8 medium-8 columns">
      <div class="markdown-body article-wrap">
       <div class="article">
          
          <h1>spark</h1>
     
        <div class="read-more clearfix">
          <span class="date">2017/2/7</span>

          <span>posted in&nbsp;</span> 
          
              <span class="posted-in"><a href='tips.html'>tips</a></span>
           
         
          <span class="comments">
            

            
          </span>

        </div>
      </div><!-- article -->

      <div class="article-content">
      <p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">After join drop some columns cause Error</a>
</li>
<li>
<a href="#toc_1">read Lzo into Dataset</a>
</li>
<li>
<a href="#toc_2">read CSV into Dataset</a>
</li>
<li>
<a href="#toc_3">Don&#39;t use user defined functions in agg yet</a>
</li>
<li>
<a href="#toc_4">Use iterator and for comprehension to get cross product</a>
</li>
<li>
<a href="#toc_5"><font color='salmon'>? Strange java.io.NotSerializableException</font></a>
</li>
<li>
<a href="#toc_6">Using Spark-shell REPL</a>
</li>
<li>
<a href="#toc_7">Running spark job in oozie</a>
</li>
<li>
<a href="#toc_8">Another error for running spark job in oozie</a>
</li>
<li>
<a href="#toc_9">More dependency conflict for running oozie spark workflow</a>
</li>
<li>
<a href="#toc_10">spark config</a>
</li>
<li>
<a href="#toc_11">graphX connectedComponent</a>
</li>
<li>
<a href="#toc_12">Using of Bloom Filter</a>
</li>
<li>
<a href="#toc_13">Add a jar inside the spark REPL</a>
</li>
<li>
<a href="#toc_14">Get size of an object inside spark REPL</a>
</li>
<li>
<a href="#toc_15">Configuration that WON&#39;T work</a>
</li>
<li>
<a href="#toc_16">My experience for OOM debugging</a>
</li>
<li>
<a href="#toc_17">Date in Spark Dataset</a>
</li>
<li>
<a href="#toc_18">Encoder Issue in Dataset</a>
<ul>
<li>
<a href="#toc_19">Unable to find Encoder at Compiling time</a>
</li>
<li>
<a href="#toc_20">Unable to find Encoder at Run time</a>
</li>
</ul>
</li>
<li>
<a href="#toc_21">Split Dataset and save them separately simultaneously</a>
</li>
<li>
<a href="#toc_22">Hint the correct side of broadcast join</a>
</li>
<li>
<a href="#toc_23">Schema after join of two Dataset</a>
</li>
<li>
<a href="#toc_24">Handling A file sequentially</a>
</li>
<li>
<a href="#toc_25">Type Class Reflect of Spark Scala</a>
</li>
<li>
<a href="#toc_26">Spark shell Class initialization Error</a>
</li>
<li>
<a href="#toc_27">Spark date timezone converting</a>
</li>
<li>
<a href="#toc_28">Another Spark from date string to <code>DateType</code> converting</a>
</li>
<li>
<a href="#toc_29">Config spark jars in a folder</a>
</li>
<li>
<a href="#toc_30">Using python inside Spark (Scala) code</a>
</li>
<li>
<a href="#toc_31">DataType inside Dataset Row</a>
</li>
<li>
<a href="#toc_32">Read and Write in LZO format</a>
</li>
<li>
<a href="#toc_33">UDF for Dataset&#39;s column operation</a>
</li>
<li>
<a href="#toc_34">Kill a problematic executor</a>
</li>
<li>
<a href="#toc_35">cache and persist is a transform</a>
</li>
<li>
<a href="#toc_36">Another possible issue about cache</a>
</li>
<li>
<a href="#toc_37">parse URL get domain</a>
</li>
<li>
<a href="#toc_38">Get Seq of Tuple from DataFrame Row</a>
</li>
<li>
<a href="#toc_39">Handling dirty data</a>
</li>
<li>
<a href="#toc_40">User Agent parse</a>
</li>
<li>
<a href="#toc_41">Explode a column of Array to columns without using RDD</a>
</li>
<li>
<a href="#toc_42"><code>WrappedArray</code> in DataSet need use <code>Seq</code> to get</a>
</li>
<li>
<a href="#toc_43">Change all column names once</a>
</li>
<li>
<a href="#toc_44">Read <code>.tar.gz</code> file</a>
</li>
<li>
<a href="#toc_45">Spark join timeout errors</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">After join drop some columns cause Error</h1>

<p>For spark 2.0 Error message is like:</p>

<blockquote>
<p>PartitioningCollection requires all of its partitionings have the same numPartitions.</p>
</blockquote>

<p>Fix it by calling <code>.repartition()</code> on the dataframe.<br/>
<a href="http://stackoverflow.com/a/40125736/4229125">stackoverflow post</a></p>

<p>Need call <code>.repartition()</code> after every <code>join</code> and <code>drop</code> like following:</p>

<pre><code class="language-scala">val injectPair = mbvDS.join(reducedIndexTable, $&quot;strId1&quot; === $&quot;stringId&quot;, &quot;inner&quot;)
        .toDF(&quot;strId1&quot;, &quot;strId2&quot;, &quot;score&quot;, &quot;matchingId1&quot;, &quot;longId1&quot;)
        .drop(&quot;strId1&quot;, &quot;matchingId1&quot;)
        .repartition()
        .join(reducedIndexTable, $&quot;strId2&quot; === $&quot;stringId&quot;, &quot;inner&quot;)
        .toDF(&quot;strId2&quot;, &quot;score&quot;, &quot;longId1&quot;, &quot;matchingId2&quot;, &quot;longId2&quot;)
        .drop(&quot;strId2&quot;, &quot;matchingId2&quot;)
        .repartition()
</code></pre>

<h1 id="toc_1">read Lzo into Dataset</h1>

<p>DB has a Util function<br/>
For example, I need read this label-handler <code>/user/oozie/data/dpp/pairing/label-solve/merge-label-output/2016-11-02/cookie/part-r-00366.lzo</code></p>

<pre><code class="language-scala">import ge.drawbrid.dpp.spark2.util.Util
import spark.implicits._

case class LabelData(handler:String, stringId:String)

val labelPath = &quot;/user/oozie/data/dpp/pairing/label-solve/merge-label-output/2016-11-09/cookie/part-r-003*&quot;

val labels = Util.readLzo(sc, labelPath, &quot;\t&quot;).map({arr =&gt; LabelData(arr(0), arr(1))}).toDS()
</code></pre>

<p>The implementation of this Util function is as:</p>

<pre><code class="language-scala">def readLzo(sc: SparkContext, path: String, delimiter: String) = {
    sc.newAPIHadoopFile(path, classOf[com.hadoop.mapreduce.LzoTextInputFormat],
        classOf[org.apache.hadoop.io.LongWritable],classOf[org.apache.hadoop.io.Text])
      .map(_._2.toString.split(delimiter))
  }
</code></pre>

<p>The output is of type <code>RDD[Array[String]]</code></p>

<h1 id="toc_2">read CSV into Dataset</h1>

<p>Need specify the schema to convert the <code>String</code> type into the appropriate type.</p>

<ul>
<li>Using the case class and <code>Encoders</code> to generate the schema</li>
</ul>

<pre><code class="language-scala">import org.apache.spark.sql.{Encoders, SparkSession}

case class LabelPair(longId1: Long, longId2: Long)

val sparkSession = SparkSession.builder.appName(&quot;Preprocess&quot;).getOrCreate()

import sparkSession.implicits._
val labelPairsPath = &quot;LP_tunning_17/spark_base/pairs/*&quot;
val labelPairsDS = sparkSession.read
                    .option(&quot;sep&quot;, &quot;\t&quot;)
                    .schema(Encoders.product[LabelPair].schema)
                    .csv (labelPairsPath).as[LabelPair]
</code></pre>

<ul>
<li>If the schema is less confusing, can simply use <code>.option(&quot;inferSchema&quot;, &quot;true&quot;)</code></li>
</ul>

<p><a href="http://stackoverflow.com/a/40656642/4229125">reference post</a></p>

<h1 id="toc_3">Don&#39;t use user defined functions in agg yet</h1>

<p><font color='salmon'><strong>Do not use the user defined functions yet!</strong></font></p>

<p>The <code>groupByKey</code> returns <code>KeyValueGroupedDataset</code>, which is not very mature as of now. <code>agg</code> can be applied on <code>KeyValueGroupedDataset</code> only with <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.scalalang.typed$">typed expression</a>. Avoid using it. </p>

<p>However, the <code>groupBy</code> is preferred, because it returns <code>RelationalGroupedDataset</code>, <code>agg</code> can be applied on <code>RelationalGroupedDataset</code> with all sql.functions. <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">Full list of sql.functions</a></p>

<p>List the following pages anyway:  </p>

<ul>
<li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.package">sql.expression package</a></li>
<li><a href="https://docs.cloud.databricks.com/docs/spark/1.6/examples/Dataset%20Aggregator.html">Databrick A example actually not run</a></li>
<li><a href="https://github.com/apache/spark/blob/v2.1.0/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala">The source code</a></li>
<li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator">Aggregator spark scala doc</a></li>
<li><a href="https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/">A post not so clear</a></li>
<li><a href="https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html">Databricks manual</a></li>
</ul>

<p><font color='salmon'><strong>Use <code>mapGroup</code> or <code>flatMapGroup</code> for complex logic.</strong></font></p>

<h1 id="toc_4">Use iterator and for comprehension to get cross product</h1>

<p>iterator by contract only iterate once.</p>

<p>The following two examples illustrate this:</p>

<pre><code class="language-scala">val iter = List(1,2,3,4,5,6).iterator
val (iter1, iter2) = iter.duplicate
(for { i &lt;- iter1; j &lt;- iter2} yield (i, j)).toList

#List[(Int, Int)] = List((1,1), (1,2), (1,3), (1,4), (1,5), (1,6))

(for { i &lt;- List(1,2,3,4,5,6).iterator; j &lt;- List(1,2,3,4,5,6).iterator} yield (i, j)).toList

# List[(Int, Int)] = List((1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (2,1), (2,2), (2,3), (2,4), (2,5), (2,6), (3,1), (3,2), (3,3), (3,4), (3,5), (3,6), (4,1), (4,2), (4,3), (4,4), (4,5), (4,6), (5,1), (5,2), (5,3), (5,4), (5,5), (5,6), (6,1), (6,2), (6,3), (6,4), (6,5), (6,6))

</code></pre>

<h1 id="toc_5"><font color='salmon'>? Strange java.io.NotSerializableException</font></h1>

<p>After I did <code>groupByKey</code> \(\rightarrow\) <code>KeyValueGroupedDataset</code>, then use <code>mapGroups</code>. If the function in <code>mapGroups</code> includes a custom defined function, I get <code>java.io.NotSerializableException</code></p>

<p>I summarized the question on <a href="http://stackoverflow.com/questions/42144617/spark-2-0-a-named-function-inside-mapgroups-for-sql-keyvaluegroupeddataset-caus">stackoverflow</a></p>

<h1 id="toc_6">Using Spark-shell REPL</h1>

<ul>
<li><p>Remember to <code>.unpersist()</code> if it was <code>cache()</code> before. Otherwise, even the variable is redefined, it won&#39;t update</p></li>
<li><p>Sometimes, restart the shell make things different</p></li>
</ul>

<h1 id="toc_7">Running spark job in oozie</h1>

<p>Cannot have both <code>spark2.jar</code> and <code>dpp-giraph-0.0.1-with-giraph-core.jar</code> cached. Some jar have versioning conflict</p>

<p>Error message like this (they are pretty confusing, and are all for the same reason):</p>

<blockquote>
<p>Caused by: java.io.InvalidClassException: org.apache.commons.lang3.time.FastDateFormat; local class incompatible: stream classdesc serialVersionUID = 2, local class serialVersionUID = 1<br/><br/>
ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoSuchFieldError: USE_DEFAULTS java.lang.NoSuchFieldError: USE_DEFAULTS at com.fasterxml.jackson.databind.introspect.JacksonAnnotationIntrospector.findSerializationInclusion(JacksonAnnotationIntrospector.java:498)<br/><br/>
ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.<init>(ZIIIIIII)V<br/>
java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.<init>(ZIIIIIII)V</p>
</blockquote>

<p>The workaround. Put these two jars in the oozie workflow&#39;s root folder (<strong>Not the <code>./lib/</code> folder</strong>) and in the workflow.xml at different <code>&lt;action&gt;</code> call the distributed cache using <code>&lt;file&gt;my.jar#my.jar&lt;/file&gt;</code>. Refer to example of <code>/home/yu/LP_tunning_17/sparkWorkflow</code></p>

<h1 id="toc_8">Another error for running spark job in oozie</h1>

<p>Error like below:</p>

<blockquote>
<p>java.lang.ClassNotFoundException: Class org.apache.spark.deploy.SparkSubmit not found</p>
</blockquote>

<p>This is because the lib for org.apache.spark path is not specified in the oozie job.properties</p>

<p>Wrong job.properties:</p>

<pre><code class="language-bash">oozie.libpath=/user/oozie/share/lib
</code></pre>

<p>Correct job.properties:</p>

<pre><code class="language-bash">oozie.libpath=/user/oozie/share/spark2
</code></pre>

<p><font color='red'><strong>Special Note:</strong> We cannot mix the two <code>libpath</code>, because the jars inside these two pathes have confliction! The below <code>job.properties</code> is wrong!</font></p>

<pre><code class="language-bash">oozie.libpath=/user/oozie/share/lib,/user/oozie/share/spark2
</code></pre>

<p>Mixing these two path will cause error like below:</p>

<blockquote>
<p>Caused by: java.io.InvalidClassException: org.apache.commons.lang3.time.FastDateFormat; local class incompatible: stream classdesc serialVersionUID = 2, local class serialVersionUID = 1</p>
</blockquote>

<p>or:</p>

<blockquote>
<p>java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.<init>(ZIIIIIII)V</p>
</blockquote>

<h1 id="toc_9">More dependency conflict for running oozie spark workflow</h1>

<ul>
<li><p><code>pig-0.12.0-cdh5.2.0.jar</code> jar are conflict with <code>spark</code> action. So it cannot be left inside the workflow&#39;s <code>lib</code> folder. It Will cause errors like below. Needed can use <code>&lt;file&gt;</code> action in workflow to distribute it.</p>

<p>Conflict Error</p>

<blockquote>
<p>java.lang.NoSuchMethodError: org.apache.hadoop.fs.FSOutputSummer.<init>(Lorg/apache/hadoop/util/DataChecksum;)V</p>
</blockquote>

<p>Missing Error</p>

<blockquote>
<p>java.lang.NoClassDefFoundError: org/apache/pig/PigServer</p>
</blockquote></li>
<li><p>Note that <code>dpp-pig-udf-0.0.1-SNAPSHOT.jar</code> has no conflict with spark actions and can be safely kept inside the <code>lib</code></p></li>
</ul>

<h1 id="toc_10">spark config</h1>

<p><a href="https://docs.google.com/document/d/1aER-bobtyf6omZxyFIOFlHPgwbgAwUmq7Aruom9cLu4/edit">zhuo&#39;s doc</a> remember to check his version for update.</p>

<h1 id="toc_11">graphX connectedComponent</h1>

<p><code>connectedComponents</code> requires the EdgeType not be null, VertexType can be null<br/>
Refer to the following code:</p>

<ul>
<li>Code does not work</li>
</ul>

<pre><code class="language-scala">// followers has type org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Nothing]]
// which means the EdgeType ED is Nothing
val followers = sc.parallelize(Array(Edge(2, 1), Edge(4, 1), Edge(1, 2), Edge(6, 3), Edge(7, 3), Edge(7, 6), Edge(6, 7), Edge(3, 7)))

// Note that the method fromEdges need specify the EdgeType and the VertexType, its signature is fromEdges[VD, ED](edges: RDD[Edge[ED]], defaultValue: VD)
val graph = Graph.fromEdges[Null, Nothing](followers, null)

// connectedComponents cannot be used on this type of graph
// error: value connectedComponents is not a member of org.apache.spark.graphx.Graph[Null,Nothing]
</code></pre>

<ul>
<li>Correct code:</li>
</ul>

<pre><code class="language-scala">// followers has type org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]]
// which means the EdgeType ED is Int, and I used a dummy value of 0
val followers = sc.parallelize(Array(Edge(2, 1, 0), Edge(4, 1, 0), Edge(1, 2, 0), Edge(6, 3, 0), Edge(7, 3, 0), Edge(7, 6, 0), Edge(6, 7, 0), Edge(3, 7, 0)))

// Note that the method fromEdges need specify the EdgeType and the VertexType, its signature is fromEdges[VD, ED](edges: RDD[Edge[ED]], defaultValue: VD)
val graph = Graph.fromEdges[Null, Int](followers, null)

// connectedComponents can be used now
val cc = graph.connectedComponents()
</code></pre>

<p><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.Graph$@fromEdges%5BVD,ED%5D(edges:org.apache.spark.rdd.RDD%5Borg.apache.spark.graphx.Edge%5BED%5D%5D,defaultValue:VD,edgeStorageLevel:org.apache.spark.storage.StorageLevel,vertexStorageLevel:org.apache.spark.storage.StorageLevel)(implicitevidence$16:scala.reflect.ClassTag%5BVD%5D,implicitevidence$17:scala.reflect.ClassTag%5BED%5D):org.apache.spark.graphx.Graph%5BVD,ED%5D">fromEdges document</a></p>

<h1 id="toc_12">Using of Bloom Filter</h1>

<p><a href="https://llimllib.github.io/bloomfilter-tutorial/">Basic explanation of bloom filter</a><br/>
The false positive rate is about \((1-e^{-\frac{-kn}{m}})^k\), in which <br/>
\(n\) is number of element expected to test, <br/>
\(k\) is the number of hash functions, <br/>
\(m\) is the number of bits to use.</p>

<p>I used the <code>breeze</code> implementation. The <a href="https://github.com/scalanlp/breeze/blob/releases/v0.13-RC1/math/src/main/scala/breeze/util/BloomFilter.scala">source code is here</a></p>

<p><a href="http://stackoverflow.com/a/31825468/4229125">Here is the reference code from stackoverflow</a></p>

<p>Zhuo&#39;s example is at <code>ge.drawbrid.dpp.spark2.example.BloomFilterJoin</code></p>

<p>My own usage:</p>

<pre><code class="language-scala">import breeze.util.BloomFilter

val bfLongIdCnt = sampleDS.count()
val bfLongId = sampleDS.mapPartitions({ iter =&gt; {
 val b = BloomFilter.optimallySized[Long](bfLongIdCnt.toDouble * 2, 0.05)
 iter.foreach(pair =&gt; {
   b += pair.longId1
   b += pair.longId2
 })
 Iterator(b)
}
}).rdd.treeReduce(_ | _)
val bfLongIdBroadcasted = sc.broadcast(bfLongId)

val indexTableDS = sc.textFile(indexTablePath).map(_.split(&quot;\t&quot;)).map(tokens =&gt; IndexTableData(tokens(0).trim.toLong, tokens(1).trim)).toDS()
val reducedIndexTable = indexTableDS.filter(indexTableData =&gt; bfLongIdBroadcasted.value.contains(indexTableData.longId)).
 mapPartitions({ iter =&gt; {
   var partitionMap = Map[String, Long]()
   iter.foreach(indexTableData =&gt; partitionMap += (indexTableData.stringId -&gt; indexTableData.longId))
   Iterator(partitionMap)
 }
 }).rdd.treeReduce(_ ++ _)
</code></pre>

<h1 id="toc_13">Add a jar inside the spark REPL</h1>

<p>Using the follow command inside the repl</p>

<pre><code class="language-bash">scala&gt; :require ./spark2.jar
Added &#39;/jbod/home/yu/spark_pg/./spark2.jar&#39; to classpath.
</code></pre>

<h1 id="toc_14">Get size of an object inside spark REPL</h1>

<ul>
<li>Use the <a href="https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/util/SizeEstimator.html"><code>SizeEstimator</code></a></li>
</ul>

<pre><code class="language-scala">import org.apache.spark.util.SizeEstimator.estimate

estimate(testObj)/(1024*1024).toFloat // convert to MB
</code></pre>

<ul>
<li>Another way is <code>cache</code> it and check the memory usage from the web UI</li>
</ul>

<h1 id="toc_15">Configuration that WON&#39;T work</h1>

<ul>
<li><p>phased out already    </p></li>
</ul>

<ol>
<li><code>--conf spark.sql.shuffle.partitions=12000</code></li>
</ol>

<h1 id="toc_16">My experience for OOM debugging</h1>

<ol>
<li>Spark 2.0 will automatically do the broadcasting for you. Normally we don&#39;t need worry about that. However, this type of broadcasting is lazy, for some large broadcasting object causing OOM (java heap space) it will be hard to identify its cause. So <font color='salmon'><strong>explicitly broadcast estimated large object helps.</strong></font></li>
<li>For large spark programs, break it up into sections. Run only one section to identify the OOM cause</li>
<li>Use the Application Master log page. It aggregates logs from all workers, can get valuable information, especially about memory usage. But sometimes the Error or Exceptions won&#39;t cause the failure of the whole program (especially network Errors and Exceptions) Spark will automatically restart part of the calculation.</li>
</ol>

<h1 id="toc_17">Date in Spark Dataset</h1>

<p>Use <code>java.sql.Date</code> instead of <code>java.util.Date</code>. Otherwise, there will be Encoder Error.<br/>
<a href="http://stackoverflow.com/questions/31312108/java-util-date-is-not-supported">reference to Stackoverflow</a></p>

<h1 id="toc_18">Encoder Issue in Dataset</h1>

<p><a href="http://stackoverflow.com/questions/36648128/how-to-store-custom-objects-in-a-dataset">Stackoverflow has a good post for this</a></p>

<h2 id="toc_19">Unable to find Encoder at Compiling time</h2>

<p>Error occurs in the Gradle build (compiling) time. Message is as below:</p>

<blockquote>
<p>Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.</p>
</blockquote>

<p>I don&#39;t know the exactly cause, but seems it is related with the <code>mapPartitions</code> on <code>Dataset</code>, which is different than <code>mapPartitions</code> on <code>RDD</code>. If we change the <code>Dataset</code> into <code>RDD</code> first, then use the <code>mapPartitions</code> there will be no issue.</p>

<p>Also worth noticing, the <code>Dataset</code>&#39;s <code>mapPartitions</code> is an experimental and evolving method as of today according to the official doc.</p>

<h2 id="toc_20">Unable to find Encoder at Run time</h2>

<p>A easy workaround is to avoid Dataset for now and turn back to RDD.</p>

<p>An example of code does not work in Spark 2.0 because of Encoder issue:</p>

<pre><code class="language-scala">case class ClusterBaseStats(clusterId: Long, cookieBag: Set[Long], deviceBag: Set[Long])

val isCookie = (id: Long) =&gt; id % 2 == 1 

/**
    * Assumes the LP output has no duplicates
    *
    * @param lpOutputPath          : String. output from LP on HDFS. Schema should be tab separated (longId, longClusterID, intNumNeighbourWithThisId)
    * @param clusterSizeUpperBound : Int. Default 40. Inclusive bound of cluster size that will be considered of getting the stats.
    * @return ClusterBaseStats: base stats (for cluster size &lt;= clusterSizeUpperBound and cluster size &gt; 1) as an intermediate results which will be used as input to get the real stats and avoid repeat calculation
    */
  def getClusterBaseStats(lpOutputPath: String, clusterSizeUpperBound: Int = 40, sparkSession: SparkSession): Dataset[ClusterBaseStats] = {
    import sparkSession.implicits._
    val lpOutputSchema = StructType(Array(
      StructField(&quot;longId&quot;, LongType, nullable = false),
      StructField(&quot;clusterId&quot;, LongType, nullable = false),
      StructField(&quot;numNeighbour&quot;, IntegerType, nullable = true)))

    sparkSession.read.option(&quot;sep&quot;, &quot;\t&quot;).schema(lpOutputSchema).csv(lpOutputPath).drop(&quot;numNeighbour&quot;).
      rdd.map(row =&gt; (row.getLong(1), row.getLong(0))).
      aggregateByKey((mutable.HashSet.empty[Long], mutable.HashSet.empty[Long]))(seqOp = {
        case ((cookieBag, deviceBag), longId) =&gt;
          if (isCookie(longId)) (cookieBag += longId, deviceBag)
          else (cookieBag, deviceBag += longId)
      },
        combOp = {
          case ((cookieBag1, deviceBag1), (cookieBag2, deviceBag2)) =&gt;
            (cookieBag1 ++= cookieBag2, deviceBag1 ++= deviceBag2)
        }).
      map({ case (clusterId, (cookieBag, deviceBag)) =&gt; ClusterBaseStats(clusterId, cookieBag.toSet, deviceBag.toSet) }).
      toDS().
      filter(clusterBaseStats =&gt; {
        val clusterSize = clusterBaseStats.cookieBag.size + clusterBaseStats.deviceBag.size
        clusterSize &lt;= clusterSizeUpperBound &amp;&amp; clusterSize &gt; 1
      })
  }
</code></pre>

<p>This code compiles okay, but Error message at Run time:</p>

<blockquote>
<p>java.lang.UnsupportedOperationException: No Encoder found for Set[scala.Long]<br/>
- field (class: &quot;scala.collection.immutable.Set&quot;, name: &quot;cookieBag&quot;)<br/>
- root class: &quot;ge.drawbrid.dpp.spark2.util.ClusterBaseStats&quot;</p>
</blockquote>

<p>I rewrote it using RDD, them everything is fine:</p>

<pre><code class="language-scala">/**
    * Assumes the LP output has no duplicates
    *
    * @param lpOutputPath          : String. output from LP on HDFS. Schema should be tab separated (longId, longClusterID, intNumNeighbourWithThisId)
    * @param clusterSizeUpperBound : Int. Default 40. Inclusive bound of cluster size that will be considered of getting the stats.
    * @return RDD[(Long, (mutable.HashSet[Long], mutable.HashSet[Long]))]: (clusterId: Long, cookieBag: Set[Long], deviceBag: Set[Long]), for cluster size &lt;= clusterSizeUpperBound and cluster size &gt; 1) as an intermediate results which will be used as input to get the real stats and avoid repeat calculation
    */
  def getClusterBaseStats(lpOutputPath: String, clusterSizeUpperBound: Int = 40, sparkSession: SparkSession): RDD[(Long, (mutable.HashSet[Long], mutable.HashSet[Long]))] = {
    val lpOutputSchema = StructType(Array(
      StructField(&quot;longId&quot;, LongType, nullable = false),
      StructField(&quot;clusterId&quot;, LongType, nullable = false),
      StructField(&quot;numNeighbour&quot;, IntegerType, nullable = true)))

    sparkSession.read.option(&quot;sep&quot;, &quot;\t&quot;).schema(lpOutputSchema).csv(lpOutputPath).drop(&quot;numNeighbour&quot;).
      rdd.map(row =&gt; (row.getLong(1), row.getLong(0))).
      aggregateByKey((mutable.HashSet.empty[Long], mutable.HashSet.empty[Long]))(seqOp = {
        case ((cookieBag, deviceBag), longId) =&gt;
          if (isCookie(longId)) (cookieBag += longId, deviceBag)
          else (cookieBag, deviceBag += longId)
      },
        combOp = {
          case ((cookieBag1, deviceBag1), (cookieBag2, deviceBag2)) =&gt;
            (cookieBag1 ++= cookieBag2, deviceBag1 ++= deviceBag2)
        }).
      filter({ case (clusterId, (cookieBag, deviceBag)) =&gt; {
        val clusterSize = cookieBag.size + deviceBag.size
        clusterSize &lt;= clusterSizeUpperBound &amp;&amp; clusterSize &gt; 1
      }
      })
  }
</code></pre>

<h1 id="toc_21">Split Dataset and save them separately simultaneously</h1>

<p>In spark there is no way to split RDD/Dataset into multiple RDDs/Datasets. But if we just wanna split them and save them into different folders we can do the following:</p>

<ol>
<li>Generate new column whose value can be used to split the RDD/Dataset</li>
<li>use <code>.partitionBy()</code></li>
<li>Then save</li>
</ol>

<p>Example:</p>

<pre><code class="language-scala">case class GpairingInputMultipleFiles(saveType: String, ts: Long, id: String, ip: String, pfc: Int, freq: Int, devtp: String)

case class GpairingInput(ip: String, ts: Long, id: String, pfc: Int, freq: Int, devtp: String) {
  val convert: (String) =&gt; GpairingInputMultipleFiles = { saveType: String =&gt;
    GpairingInputMultipleFiles(saveType, ts, id, ip, pfc, freq, devtp)
  }
}

    val gpairingInputMultipleFiles = gpairingInput.groupByKey(_.id).flatMapGroups((id, iter) =&gt; {
      val groupArr = ArrayBuffer[GpairingInputMultipleFiles]()
      if (id.startsWith(&quot;uuid&quot;) &amp;&amp; iter.length == 1) groupArr += iter.next().convert(&quot;singleObs&quot;)
      else {
        iter.foreach(gpairingInput =&gt; {
          if (gpairingInput.pfc == 1) groupArr += gpairingInput.convert(&quot;dev&quot;)
          else if (gpairingInput.pfc == 2) groupArr += gpairingInput.convert(&quot;mob&quot;)
          else if (gpairingInput.pfc == 3) groupArr += gpairingInput.convert(&quot;web&quot;)
        })
      }
      groupArr.iterator
    })

    val gpairingInputPath = &quot;alibaba_2/firstRun/gpairing_input/&quot;
    gpairingInputMultipleFiles.write.partitionBy(&quot;saveType&quot;).mode(SaveMode.Overwrite).format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;\t&quot;).save(gpairingInputPath)

</code></pre>

<h1 id="toc_22">Hint the correct side of broadcast join</h1>

<p>The trick is calling <code>broadcast()</code> on the smaller RDD/Dataset/DataFrame</p>

<pre><code class="language-scala">import org.apache.spark.sql.functions.broadcast

largedataframe.join(broadcast(smalldataframe), ...)
</code></pre>

<p><a href="http://stackoverflow.com/a/39404486/4229125">stackoverflow</a></p>

<h1 id="toc_23">Schema after join of two Dataset</h1>

<p>for Dataset, we have the <code>left.join(right, usingColumns: Seq[String]): DataFrame</code></p>

<p>The official document says:</p>

<blockquote>
<p>Inner equi-join with another DataFrame using the given column.<br/>
 Different from other join functions, the join column will only appear once in the output, i.e. similar to SQL&#39;s JOIN USING syntax.</p>
</blockquote>

<pre><code class="language-scala">// Joining df1 and df2 using the column &quot;user_id&quot;
df1.join(df2, &quot;user_id&quot;)
</code></pre>

<blockquote>
<p>usingColumn: Name of the column to join on. This column must exist on both sides.</p>
</blockquote>

<p>The schema after join is in this order<br/>
<code>usingColumns(eg. &quot;user_id&quot;), df1 (except for the usingColumns), df2 (except for the usingColumns)</code></p>

<p>This is based on the <a href="http://docs.oracle.com/javadb/10.6.2.1/ref/rrefsqljusing.html">SQL USING syntax documentation</a> </p>

<blockquote>
<p>When a USING clause is specified, an asterisk (*) in the select list of the query will be expanded to the following list of columns (in this order):</p>

<p>All the columns in the USING clause<br/>
All the columns of the first (left) table that are not specified in the USING clause<br/>
All the columns of the second (right) table that are not specified in the USING clause</p>
</blockquote>

<h1 id="toc_24">Handling A file sequentially</h1>

<p>Giraph output during Computation append all superSteps into one file. Need be able to divide the superSteps without changing the vertex value class (If it is possible, it should be much easier to just change the vertex value class to attach a field of current superStep)</p>

<p>One thing worths notice is that not all vertices output at every super step. If at a super step a vertex is not awaken, it will not output.</p>

<p>The following code works. Used mapPartitions</p>

<pre><code class="language-scala">  def getLpOutputWithSS(sparkSession: SparkSession, lpOutputPath: String): Dataset[LpOutputWithSS] = {
    import sparkSession.implicits._
    val lpOutputPath = &quot;LP_tunning_17/scrummageLPonNF/testLP/&quot;
    val lpOutput = sparkSession.read.option(&quot;sep&quot;, &quot;\t&quot;).
      schema(Encoders.product[LpOutputSchema].schema).
      csv(lpOutputPath).drop(&quot;numNeighbour&quot;)

    val lpOutputWithSS = lpOutput.mapPartitions(rowIter =&gt; {
      val partitionArr = ArrayBuffer[LpOutputWithSS]()
      var ssCounter = Map[Long, Int]()
      rowIter.foreach(row =&gt; {
        val longId = row.getLong(0)
        val clusterId = row.getLong(1)
        val ss = ssCounter.getOrElse(longId, -1) + 1
        ssCounter += (longId -&gt; ss)
        partitionArr.append(LpOutputWithSS(longId, clusterId, ss))
      })
      partitionArr.iterator
    })
    lpOutputWithSS
  }
</code></pre>

<p>The following uses RDD, gives exactly the same result</p>

<pre><code class="language-scala">  def getLpOutputWithSS(sparkSession: SparkSession, lpOutputPath: String): Dataset[LpOutputWithSS] = {
    import sparkSession.implicits._

    val sc = sparkSession.sparkContext
    val lpOutput = sc.textFile(lpOutputPath).mapPartitions(stringIter =&gt; {
      val partitionArr = ArrayBuffer[LpOutputWithSS]()
      var ssCounter = Map[Long, Int]()
      stringIter.foreach(str =&gt; {
        val tokens = str.split(&quot;\t&quot;)
        val longId = tokens(0).toLong
        val clusterId = tokens(1).toLong
        val ss = ssCounter.getOrElse(longId, -1) + 1
        ssCounter += (longId -&gt; ss)
        partitionArr.append(LpOutputWithSS(longId, clusterId, ss))
      })
      partitionArr.iterator
    }).toDS()
    lpOutput
  }

</code></pre>

<p>Using RDD maybe more reliable. <a href="http://www.infoobjects.com/spark-core-sc-textfile-vs-sc-wholetextfiles/">A blog talks about the sequentially reading</a></p>

<blockquote>
<p>sc.textFile<br/>
SparkContext’s TextFile method, i.e., sc.textFile in Spark Shell, creates a RDD with each line as an element. If there are 10 files in movies folder, 10 partitions will be created.</p>

<p>sc.wholeTextFiles<br/>
SparkContext’s whole text files method, i.e., sc.wholeTextFiles in Spark Shell, creates a PairRDD with the key being the file name with a path. It’s a full path like “hdfs://m1.zettabytes.com:9000/user/hduser/movies/movie1.txt”. The value is the whole content of file in String. Here the number of partitions will be 1 or more depending upon how many executor cores you have.</p>
</blockquote>

<h1 id="toc_25">Type Class Reflect of Spark Scala</h1>

<p>I know this is kind of big topic. I don&#39;t have time to understand it all. Just keep a record where I am and some temporary workaround</p>

<p>From Jenny:<br/>
<code>ge.drawbrid.dpp.spark2.feature.TextFormatWriter#writeTextOutput</code></p>

<pre><code class="language-scala">def writeTextOutput[ViewMapping &lt;: Product : TypeTag](spark: SparkSession, data: Dataset[ViewMapping], outputPath: String)={

    import spark.implicits._
    data.write.mode(SaveMode.Overwrite)
      .format(&quot;com.databricks.spark.csv&quot;)
      .option(&quot;header&quot;, &quot;false&quot;)
      .option(&quot;codec&quot;, &quot;com.hadoop.compression.lzo.LzopCodec&quot;)
      .option(&quot;delimiter&quot;, COL_DELIM)
      .save(outputPath);

  }
</code></pre>

<h1 id="toc_26">Spark shell Class initialization Error</h1>

<p>Sometimes it is because duplicated definition of case class. For example, if you cached a dataset, which uses a case class A, and later, you redefined the case class using exactly the same signature as A. Both these two classes previous A and current A exist in the spark shell and confuses application.</p>

<p>Likewise, some other wired behaviors if only in spark shell, may be due to the fact of cached old values. The work around is <code>:reset</code>. Or just simply re-start the shell.</p>

<h1 id="toc_27">Spark date timezone converting</h1>

<p>It is kind of tricky to convert the timezone. <a href="http://thread.gmane.org/gmane.comp.lang.scala.spark.user/27374">Discussion thread</a></p>

<p>Some suggest to use <code>joda-time</code> library, <a href="http://stackoverflow.com/a/34669345/4229125">like in this post</a> however, the central idea is using this library to convert it to formatted standard string, then converted to other datetype</p>

<p>My workaround is because our hadoop system are all using UTC. Therefore I can use the <code>sql.functions._</code> to directly convert from Java time (long in millisecond) to Unix time (long in second), to spark timestamp (using local timezone, which is UTC), then to the timezone that I want.</p>

<pre><code class="language-scala">umidTraffic.withColumn(&quot;BJ_date&quot;, from_utc_timestamp(from_unixtime($&quot;ts&quot; / 1000), &quot;Asia/Shanghai&quot;)).show()
</code></pre>

<p>The standard way of using <code>joda-time</code> library is like this:</p>

<pre><code class="language-scala">import org.joda.time.DateTime
import org.joda.time.DateTimeZone
val beijingTZ = DateTimeZone.forID(&quot;Asia/Shanghai&quot;)

def longTsToBJDateStr(ts:Long) : String = {
    (new DateTime(ts*1000, beijingTZ)).toString(&quot;yyyyMMdd&quot;)
}

longTsToBJDateStr(1487422472)
</code></pre>

<h1 id="toc_28">Another Spark from date string to <code>DateType</code> converting</h1>

<p>The sql function <code>to_date()</code> converts the string format to <code>DateType</code>, however it only take standard date string format like <code>yyyy-MM-dd</code>, if we have string format like <code>yyyyMMdd</code> we need add bridge to convert, like below:</p>

<pre><code class="language-scala">val umid_ip_date = umid_log.select(&quot;umid&quot;, &quot;ip&quot;, &quot;date&quot;).distinct.select($&quot;umid&quot;, $&quot;ip&quot;, to_date(from_unixtime(unix_timestamp($&quot;date&quot;, &quot;yyyyMMdd&quot;))).as(&quot;date&quot;))
val umid_ip_oneDayShift = umid_ip_date.select($&quot;umid&quot;, $&quot;ip&quot;, date_add($&quot;date&quot;, 1).as(&quot;date&quot;)).
                            union(umid_ip_date.select($&quot;umid&quot;, $&quot;ip&quot;, date_add($&quot;date&quot;, -1).as(&quot;date&quot;)))   
</code></pre>

<h1 id="toc_29">Config spark jars in a folder</h1>

<p>This is the correct way of specifying a lot of jars in a folder:<br/>
<code>spark.yarn.jars</code> \(\rightarrow\) <code>hdfs://sc2prod/user/oozie/share/spark2/*.jar</code></p>

<h1 id="toc_30">Using python inside Spark (Scala) code</h1>

<p>This is really cool. I was trying to parse the User Agent to see whether it is a mobile device or a desktop device (another topic). <a href="http://detectmobilebrowsers.com/">This website : detectmobilebrowsers</a> is recommended by a lot of posts. It has a python script implementation, I downloaded the script and want to use it inside spark.</p>

<p>The way of using it is as follow, (only works for RDD though):</p>

<ul>
<li><p>I saved the python script on local folder: <code>/home/yu/alibaba_3/gpairing_input/python/mob_desktop.py</code>.</p></li>
<li><p>Inside the python script main body, I am reading from <code>sys.stdin</code> and processing it</p></li>
</ul>

<pre><code class="language-python">#! /usr/local/bin/python2.7
import re
import sys

reg_b = re.compile(r&quot;(android|bb\\d+|meego).+mobile|avantgo|bada\\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\\.(browser|link)|vodafone|wap|windows ce|xda|xiino&quot;, re.I|re.M)
reg_v = re.compile(r&quot;1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\\-(n|u)|c55\\/|capi|ccwa|cdm\\-|cell|chtm|cldc|cmd\\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\\-s|devi|dica|dmob|do(c|p)o|ds(12|\\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\\-|_)|g1 u|g560|gene|gf\\-5|g\\-mo|go(\\.w|od)|gr(ad|un)|haie|hcit|hd\\-(m|p|t)|hei\\-|hi(pt|ta)|hp( i|ip)|hs\\-c|ht(c(\\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\\-(20|go|ma)|i230|iac( |\\-|\\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\\/)|klon|kpt |kwc\\-|kyo(c|k)|le(no|xi)|lg( g|\\/(k|l|u)|50|54|\\-[a-w])|libw|lynx|m1\\-w|m3ga|m50\\/|ma(te|ui|xo)|mc(01|21|ca)|m\\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\\-2|po(ck|rt|se)|prox|psio|pt\\-g|qa\\-a|qc(07|12|21|32|60|\\-[2-7]|i\\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\\-|oo|p\\-)|sdk\\/|se(c(\\-|0|1)|47|mc|nd|ri)|sgh\\-|shar|sie(\\-|m)|sk\\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\\-|v\\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\\-|tdg\\-|tel(i|m)|tim\\-|t\\-mo|to(pl|sh)|ts(70|m\\-|m3|m5)|tx\\-9|up(\\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\\-|your|zeto|zte\\-&quot;, re.I|re.M)

def isMobile(ua):
    b = reg_b.search(ua)
    v = reg_v.search(ua[0:4])
    if b or v:
        return True
    return False

for line in sys.stdin:
    print isMobile(line)
</code></pre>

<ul>
<li>In spark, I put this python script to all executors by <code>SparkContext.addFile(path)</code>, so that it can be run on all executors. After the <code>addFile</code>, the script file will be send to all executor to the current working directory. </li>
</ul>

<pre><code class="language-scala">import org.apache.spark.SparkFiles

val distScript = &quot;/home/yu/alibaba_3/gpairing_input/python/mob_desktop.py&quot;
sc.addFile(distScript)
val distScriptName = &quot;./mob_desktop.py&quot; // The script is at cwd for all exectuors
val pythonRes = acookieInput.map(_.getAs[String](&quot;useragent&quot;)).rdd.pipe(distScriptName).map(str =&gt; str.toBoolean).take(200) // pipe is what does the job.
</code></pre>

<p><a href="http://stackoverflow.com/a/32978183/4229125">reference post</a></p>

<h1 id="toc_31">DataType inside Dataset Row</h1>

<p><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row@apply(i:Int):Any">official doc.</a></p>

<pre><code>BooleanType -&gt; java.lang.Boolean
ByteType -&gt; java.lang.Byte
ShortType -&gt; java.lang.Short
IntegerType -&gt; java.lang.Integer
FloatType -&gt; java.lang.Float
DoubleType -&gt; java.lang.Double
StringType -&gt; String
DecimalType -&gt; java.math.BigDecimal

DateType -&gt; java.sql.Date
TimestampType -&gt; java.sql.Timestamp

BinaryType -&gt; byte array
ArrayType -&gt; scala.collection.Seq (use getList for java.util.List)
MapType -&gt; scala.collection.Map (use getJavaMap for java.util.Map)
StructType -&gt; org.apache.spark.sql.Row
</code></pre>

<h1 id="toc_32">Read and Write in LZO format</h1>

<p>Need have the <code>hadoop-lzo</code> jar loaded into spark (see notes in zeppelin) <br/>
<code>z.load(&quot;/usr/lib/hadoop/lib/hadoop-lzo.jar&quot;)</code> </p>

<p>Use the <code>option(&quot;codec&quot;, &quot;com.hadoop.compression.lzo.LzopCodec&quot;)</code> to read and write.</p>

<p>Example code of reading <code>LzoPigStorage(&#39;\u0001&#39;)</code> outputed file.</p>

<pre><code class="language-scala">import org.apache.spark.sql.Encoders

case class GpairingInput(ts: Long, id: String, ip: String, pfc:Int, freq: Int, devtp: String)

val first_singleObs_path = &quot;/user/yu/alibaba_2/firstRun/freq/single-obs&quot;
val first_singleObs = sparkSession.read.option(&quot;sep&quot;, &quot;\u0001&quot;).
        option(&quot;codec&quot;, &quot;com.hadoop.compression.lzo.LzopCodec&quot;).
        schema(Encoders.product[GpairingInput].schema).
        csv(first_singleObs_path).as[GpairingInput]
        
val acookieType = acookieFromMY.select(concat(lit(&quot;uuid|&quot;), $&quot;acookie&quot;).as(&quot;id&quot;), $&quot;isMob&quot;)
val first_singleObsWType = first_singleObs.join(acookieType, usingColumn = &quot;id&quot;).
        select(&quot;ts&quot;, &quot;id&quot;, &quot;ip&quot;, &quot;pfc&quot;, &quot;freq&quot;, &quot;devtp&quot;, &quot;isMob&quot;)

val second_singleObs_outputpath_mweb = &quot;/user/yu/alibaba_2/secondRun/mweb/freq/single-obs&quot;
val second_singleObs_outputpath_dweb = &quot;/user/yu/alibaba_2/secondRun/dmweb/freq/single-obs&quot;

first_singleObsWType.filter($&quot;isMob&quot;).drop(&quot;isMob&quot;).write.format(&quot;csv&quot;).
    option(&quot;sep&quot;, &quot;\t&quot;).option(&quot;codec&quot;, &quot;com.hadoop.compression.lzo.LzopCodec&quot;).
    mode(&quot;overwrite&quot;).save(second_singleObs_outputpath_mweb)
first_singleObsWType.filter(not($&quot;isMob&quot;)).drop(&quot;isMob&quot;).write.format(&quot;csv&quot;).
    option(&quot;sep&quot;, &quot;\t&quot;).option(&quot;codec&quot;, &quot;com.hadoop.compression.lzo.LzopCodec&quot;).
    mode(&quot;overwrite&quot;).save(second_singleObs_outputpath_dweb)
</code></pre>

<p><strong><font color='red'>Very Important</font></strong><br/>
It is <code>LzopCodec</code> not <code>LzoCodec</code>. If accidentally used the latter one, it will generate <code>lzo_deflate</code> files, which can not be read using normal <code>LzoPigStorage()</code> method.<br/><br/>
<a href="http://stackoverflow.com/questions/16676967/how-to-decompress-lzo-deflate-file">Stackoverflow post</a></p>

<h1 id="toc_33">UDF for Dataset&#39;s column operation</h1>

<p>Easy to learn from example</p>

<pre><code class="language-scala">import org.apache.spark.sql.functions._

val uuidStripper: (String =&gt; String) = (uuid: String) =&gt; uuid.substring(uuid.lastIndexOf(&quot;|&quot;)+1)
val uuidStripperFunc = udf(uuidStripper)


val feat2_maxpair360_path=&quot;alibaba_2/thirdRun/rank/features-all/CD/ALL&quot;
val feat2_maxpair360 = sparkSession.read.option(&quot;sep&quot;, &quot;\t&quot;).csv(feat2_maxpair360_path)
val feat2_maxpair360_join = feat2_maxpair360.withColumn(&quot;umid&quot;, uuidStripperFunc(col(feat2_maxpair360.columns(0)))).
        withColumn(&quot;acookie&quot;, uuidStripperFunc(col(feat2_maxpair360.columns(1))))
        
</code></pre>

<p><em>Note</em>: The <code>udf</code> function is from <code>org.apache.spark.sql.functions</code> package</p>

<p>The above udf actually is exactly <code>substring_index</code> <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$@substring_index(str:org.apache.spark.sql.Column,delim:String,count:Int):org.apache.spark.sql.Column">doc</a></p>

<pre><code>val feat2_maxpair360_join = feat2_maxpair360.
        withColumn(&quot;umid&quot;, substring_index(col(feat2_maxpair360.columns(0)), &quot;|&quot;, -1)).
        withColumn(&quot;acookie&quot;, substring_index(col(feat2_maxpair360.columns(1)), &quot;|&quot;, -1))
</code></pre>

<h1 id="toc_34">Kill a problematic executor</h1>

<p>When I run a very simple task from spark repl (inside Zeppelin notebook), one executor always blocks the process.</p>

<p>Go to the spark webUI, and found that the problematic executor is <code>55 / sc2-hmr15.drawbrid.ge</code>. It always shows as <code>RUNNING</code>. get the <code>Executors</code> page and find this exectuor and use the thread dump, found that the <code>RUNNABLE</code> task are blocked at:</p>

<pre><code>java.net.SocketInputStream.socketRead0(Native Method)  
java.net.SocketInputStream.socketRead(SocketInputStream.java:116)  
java.net.SocketInputStream.read(SocketInputStream.java:170)  
java.net.SocketInputStream.read(SocketInputStream.java:141)  
</code></pre>

<p>There are a lot posts online talking about a bug in the <code>HttpClient</code> lib etc, like <a href="http://stackoverflow.com/questions/38606653/spark-stateful-streaming-job-hangs-at-checkpointing-to-s3-after-long-uptime">this one at stackoverflow</a> or <a href="http://asyncified.io/2016/08/13/leveraging-spark-speculation-to-identify-and-re-schedule-slow-running-tasks/">this post in more detail</a>. What I want to do is just kill the executor and let the spark reassign a new executor, then everything will be fine.</p>

<p>So I <br/>
1. find the container&#39;s id for this executor on this spark job, by checking the executor&#39;s stdout page<br/>
2. Logged into the machine. Go to <code>/var/log/hadoop-yarn</code> and grep the containerId from the log file, from there I can find the <code>ProcessTree 11626 for container-id</code><br/>
3. Double check the pid is correct, by <code>ps 11626 | less</code><br/>
4. Then kill this pid: <code>sudo kill 11626</code></p>

<p>A new executor is assigned! Problem solved.</p>

<h1 id="toc_35">cache and persist is a transform</h1>

<p>I often see that code runs fine in <code>spark-submit</code> but not in spark REPL<br/>
One of the possible reason is the <code>cache()</code> or <code>persist()</code> command.</p>

<p><code>cache()</code> or <code>persist()</code> are a <code>transform</code>, should assign them back to themselves. If put it in a separate line in REPL it will cause problem. In <code>spark-submit</code>, because the code is compiled, the error is silently fixed.</p>

<p>Wrong code:</p>

<pre><code class="language-scala">val reducedConvertedLabelDS =
      if (LPOutputMetricUtil.fileExist(sparkSession, reducedConvertedLabelPath + SUCCEED_STRING))
        sparkSession.read.parquet(reducedConvertedLabelPath).as[ConvertedLabelData]
      else
        LPOutputMetricUtil.reduceConvertLabel(sparkSession, labelInputPath,
          reducedIndexTable = reducedIndexTableDS, labelOutputPath = reducedConvertedLabelPath)
          
reducedConvertedLabelDS.cache()
</code></pre>

<h1 id="toc_36">Another possible issue about cache</h1>

<p>maybe not related with <code>cache()</code></p>

<p>The error message looks like:</p>

<blockquote>
<p>java.lang.IllegalArgumentException: requirement failed: PartitioningCollection requires all of its partitionings have the same numPartitions.<br/>
  at scala.Predef$.require(Predef.scala:219)</p>
</blockquote>

<p>I had this problem because:</p>

<pre><code class="language-scala">val idx_path = &quot;/user/yu/alibaba_2/thirdRun_maxPair360/pair/idx&quot;
val idxStrcutType = StructType(Seq(StructField(&quot;strId&quot;, StringType, false), StructField(&quot;longId&quot;, LongType, false)))
val idx =  sparkSession.read.option(&quot;sep&quot;, &quot;\u0001&quot;).schema(idxStrcutType).csv(idx_path)
val gpcString = gpc.join(idx, $&quot;smallId&quot; === $&quot;longId&quot;).drop(&quot;smallId&quot;, &quot;longId&quot;).
        withColumnRenamed(&quot;strId&quot;, &quot;smallId&quot;).
        join(idx, $&quot;largeId&quot; === $&quot;longId&quot;).drop(&quot;largeId&quot;, &quot;longId&quot;).
        withColumnRenamed(&quot;strId&quot;, &quot;largeId&quot;).
        map(row =&gt; {
            if (row.getAs[String](&quot;smallId&quot;).startsWith(&quot;u&quot;))
                (row.getAs[String](&quot;largeId&quot;), row.getAs[String](&quot;smallId&quot;))
            else
                 (row.getAs[String](&quot;smallId&quot;), row.getAs[String](&quot;largeId&quot;))
        }).toDF(&quot;umid&quot;, &quot;acookie&quot;).filter(row =&gt; row.getAs[String](&quot;acookie&quot;).startsWith(&quot;u&quot;)).
        select(substring_index($&quot;umid&quot;, &quot;|&quot;, -1).as(&quot;umid&quot;), substring_index($&quot;acookie&quot;, &quot;|&quot;, -1).as(&quot;acookie&quot;)).cache()
</code></pre>

<p>If I took the last <code>cache()</code> off, it runs okay</p>

<p>However, after checking on <a href="http://stackoverflow.com/questions/39780784/spark-2-0-0-error-partitioningcollection-requires-all-of-its-partitionings-have">this post on stackoverflow</a>. Seems this is not caused by <code>cache()</code>. Maybe something wrong with the REPL. Using <code>.repartition()</code> can solve this problem.</p>

<h1 id="toc_37">parse URL get domain</h1>

<p>Take the reference from <a href="http://stackoverflow.com/a/4826122/4229125">this stackoverflow post</a></p>

<p>rewrote in Scala. It is more robust than <code>java.net.URL</code></p>

<pre><code class="language-scala">def getHost(url:String): String = {
    if(url == null || url.length == 0)
        return &quot;&quot;

    val doubleslash = 
        if (url.indexOf(&quot;//&quot;) == -1) 0
        else url.indexOf(&quot;//&quot;) + 2

    val end = 
        if (url.indexOf(&#39;/&#39;, doubleslash) == -1) url.length
        else url.indexOf(&#39;/&#39;, doubleslash)

    val port = url.indexOf(&#39;:&#39;, doubleslash)
    
    val realEnd =
        if (port &gt; 0 &amp;&amp; port &lt; end) port
        else end

    url.substring(doubleslash, realEnd);
}

def getDomain(url:String): String = {
    val host = getHost(url)

    var startIndex = 0
    var nextIndex = host.indexOf(&#39;.&#39;)
    var lastIndex = host.lastIndexOf(&#39;.&#39;)
    while (nextIndex &lt; lastIndex) {
        startIndex = nextIndex + 1
        nextIndex = host.indexOf(&#39;.&#39;, startIndex)
    }
    
    if (startIndex &gt; 0)
        host.substring(startIndex)
    else
        host
}
</code></pre>

<h1 id="toc_38">Get Seq of Tuple from DataFrame Row</h1>

<p>One column of the DataFrame is generated by <code>Map.toSeq</code>, so that column is a <code>Seq</code> of <code>tuple2 (key, value)</code>. When I want to extract the <code>key</code> out of the column, I cannot use <code>row.getAs[Seq(String, Int)](&quot;colName&quot;)</code></p>

<p>setup:</p>

<pre><code class="language-scala">val umid_log_path = &quot;/user/yu/alibaba_2/test_a/drawbridge_test_data_umid_log.csv&quot;
val umid_log = sparkSession.read.option(&quot;sep&quot;, &quot;;&quot;).option(&quot;quote&quot;, &quot;\&quot;&quot;).option(&quot;header&quot;, true).csv(umid_log_path)
val umid_convert = umid_log.map(row =&gt; {
    val id = row.getAs[String](&quot;umid&quot;)
    val app_id = row.getAs[String](&quot;app_id&quot;)
    val province = row.getAs[String](&quot;province&quot;).substring(0, 2)
    (id, app_id, province)}).toDF(&quot;id&quot;, &quot;app_id&quot;, &quot;province&quot;)
val umid_aggregated = umid_convert.repartition($&quot;id&quot;).groupByKey(row =&gt; row.getAs[String](&quot;id&quot;)).
        mapGroups({case (id, iterRow) =&gt; {
            var app_idMap = mutable.HashMap[String, Int]()
            val provinceMap = mutable.HashMap[String, Int]()
            for (row &lt;- iterRow) {
                app_idMap.update(row.getAs[String](&quot;app_id&quot;), app_idMap.getOrElse(row.getAs[String](&quot;app_id&quot;), 0)+1)
                provinceMap.update(row.getAs[String](&quot;province&quot;), provinceMap.getOrElse(row.getAs[String](&quot;province&quot;), 0)+1)
            }
            (id, app_idMap.toSeq, provinceMap.toSeq)
        } }).toDF(&quot;id&quot;, &quot;app_idBag&quot;, &quot;provinceBag&quot;)

umid_aggregated.show()
</code></pre>

<p>Does <strong><font color='red'>NOT</font></strong> work:</p>

<pre><code class="language-scala">val umid_appData = umid_aggregated.map(row =&gt; {
            val id = row.getAs[String](&quot;id&quot;)
            val app_id = row.getAs[Seq[(String, Int)]](&quot;app_idBag&quot;).map(_._1)
            (id, app_id)
            }).toDF(&quot;id&quot;, &quot;app_id&quot;)
</code></pre>

<p>Error message:</p>

<blockquote>
<p>java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be cast to scala.Tuple2</p>
</blockquote>

<p>Does work</p>

<pre><code>val umid_appData = umid_aggregated.map(row =&gt; {
            val id = row.getAs[String](&quot;id&quot;)
            val app_id = row.getAs[Seq[Row]](&quot;app_idBag&quot;).map(_.getString(0))
            (id, app_id)
            }).toDF(&quot;id&quot;, &quot;app_id&quot;)
</code></pre>

<p><a href="http://stackoverflow.com/a/37553262/4229125">Reference to stackoverflow post</a></p>

<h1 id="toc_39">Handling dirty data</h1>

<ol>
<li>Whenever the data source is not from yourself, be careful, handler them <strong>one by one</strong>. Otherwise it is gonna be painful to debug</li>
<li>Use <code>count</code> frequently, because this is the cheapest way to scan through all the data and will check all possible malformed data</li>
</ol>

<p>One example of data cleaning</p>

<pre><code class="language-scala">val acookie_mingyang_lsa_rdd = sparkSession.read.option(&quot;sep&quot;, &quot;\t&quot;).
        schema(Encoders.product[Lsa_mingyang].schema).
        csv(acookie_mingyang_lsa_output_path).as[Lsa_mingyang].
        filter(_.reducedFeat != null).rdd.
        map(row =&gt; Row.fromSeq(row.id +: row.reducedFeat.split(&quot;,&quot;).map(strFeat =&gt; 
                                            try {
                                                strFeat.toDouble
                                            } catch {
                                                case _ : Throwable  =&gt; 0.0
                                            }).toSeq))
val acookie_mingyang_lsa_rdd_removedMalformated = acookie_mingyang_lsa_rdd.filter(_.length == 51)
val acookie_mingyang_lsa_dupcolname = sparkSession.createDataFrame(acookie_mingyang_lsa_rdd_removedMalformated, featureStructType)
</code></pre>

<h1 id="toc_40">User Agent parse</h1>

<p>One of the option is using <code>us-parser</code> Scala implementation. The project is located <a href="https://github.com/ua-parser">here</a>   </p>

<p>Need import the jar. </p>

<pre><code class="language-scala">%spark.dep
z.load(&quot;org.uaparser:uap-scala_2.11:0.1.0&quot;)

import org.uaparser.scala.CachingParser
val parser = CachingParser.get(1000) // This get a caching parser with size of 1000
val test_ua = &quot;&quot;&quot;Mozilla/5.0 (Linux; U; Android 5.1; zh-CN; m1 note Build/LMY47D) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/40.0.2214.89 UCBrowser/11.2.5.884 Mobile Safari/537.36&quot;&quot;&quot;
val parsedUa = parser.parse(test_ua)

// parsedUa: org.uaparser.scala.Client = Client(UserAgent(UC Browser,Some(11),Some(2),Some(5)),OS(Android,Some(5),Some(1),None,None),Device(m1 note,Some(Generic_Android),Some(m1 note)))
</code></pre>

<h1 id="toc_41">Explode a column of Array to columns without using RDD</h1>

<p>We can directly select the element of the Array inside this column by index as:</p>

<pre><code class="language-scala">df.select($&quot;Col1&quot;, $&quot;Col2&quot;(0) as &quot;Col2&quot;, $&quot;Col2&quot;(1) as &quot;Col3&quot;, $&quot;Col2&quot;(2) as &quot;Col3&quot;)
</code></pre>

<p><a href="http://stackoverflow.com/a/37392793/4229125">StackOverflow post</a></p>

<h1 id="toc_42"><code>WrappedArray</code> in DataSet need use <code>Seq</code> to get</h1>

<p>The <code>sql.functions.collect_set</code> aggregate function collects distinct elements and saved them into a <code>WrappedArray</code>. This <code>WrappedArray</code> is from <code>mutable</code> package, so when we need get it out, we can not use <code>Array</code> but need use <code>Seq</code>.</p>

<p>The error message is like:</p>

<blockquote>
<p>org.apache.spark.SparkException: Job aborted due to stage failure: Task 87 in stage 63.0 failed 4 times, most recent failure: Lost task 87.3 in stage 63.0 (TID 17961, sc2-hmr175.drawbrid.ge): java.lang.ClassCastException: scala.collection.mutable.WrappedArray$ofRef cannot be cast to [Ljava.lang.Object;</p>
</blockquote>

<p>The set up is as follow:</p>

<pre><code class="language-scala">val umid_date_ipSet = umid_log.select(&quot;umid&quot;, &quot;ip&quot;, &quot;date&quot;).distinct.groupBy($&quot;umid&quot;, $&quot;date&quot;).agg(collect_set($&quot;ip&quot;).as(&quot;umid_ipSet&quot;))
val acookie_date_ipSet = acookie_log.select(&quot;acookie&quot;, &quot;ip&quot;, &quot;date&quot;).distinct.groupBy($&quot;acookie&quot;, $&quot;date&quot;).agg(collect_set($&quot;ip&quot;).as(&quot;acookie_ipSet&quot;))
val mapping_join = mapping.drop(&quot;taobaoid&quot;)
val mapping_umidIpSet_acookieIpSet = umid_date_ipSet.join(broadcast(mapping_join), usingColumn = &quot;umid&quot;).join(acookie_date_ipSet, usingColumns = Seq(&quot;acookie&quot;, &quot;date&quot;))
val tp_umid_acookie_dt = mapping_umidIpSet_acookieIpSet.rdd.map(row =&gt; {
        val umid_Ip4Set = row.getAs[Array[String]](&quot;umid_ipSet&quot;).toSet
        val acookie_Ip4Set = row.getAs[Array[String]](&quot;acookie_ipSet&quot;).toSet
        val co_ip4 = umid_Ip4Set.intersect(acookie_Ip4Set).size
        val umid_Ip3Set = row.getAs[Array[String]](&quot;umid_ipSet&quot;).map(ip =&gt; ip.substring(0, ip.lastIndexOf(&#39;.&#39;))).toSet
        val acookie_Ip3Set = row.getAs[Array[String]](&quot;acookie_ipSet&quot;).map(ip =&gt; ip.substring(0, ip.lastIndexOf(&#39;.&#39;))).toSet
        val co_ip3 = umid_Ip3Set.intersect(acookie_Ip3Set).size
        (row.getAs[String](&quot;umid&quot;), row.getAs[String](&quot;acookie&quot;), co_ip3, co_ip4)
    }).toDF(&quot;umid&quot;, &quot;acookie&quot;, &quot;co_ip3&quot;, &quot;co_ip4&quot;).groupBy($&quot;umid&quot;, $&quot;acookie&quot;).max(&quot;co_ip3&quot;, &quot;co_ip4&quot;).cache()
</code></pre>

<p>Change the <code>row.getAs[Array[String]]</code> to <code>row.getAs[Seq[String]]</code> code passes without issue.</p>

<p>For checking what the output type of <code>collect_set()</code> is, I did:</p>

<pre><code class="language-scala">val test = mapping_umidIpSet_acookieIpSet.take(1)
test: Array[org.apache.spark.sql.Row] = Array([WrappedArray(112.26.78.182),WrappedArray(36.63.47.140, 36.63.95.185)])
</code></pre>

<p><a href="http://stackoverflow.com/questions/33204205/read-array-of-string-from-spark">StackOverflow post</a></p>

<h1 id="toc_43">Change all column names once</h1>

<p>Using Dataset&#39;s method: <code>def toDF(colNames: String*): DataFrame</code>. The key point is how to use <code>*</code></p>

<p>Old dataset has schema [id: string, reducedFeat_1: Double, reducedFeat_2, ... reducedFeat_50: Double], what to add prefix &quot;umid_&quot; in all column names except for the first one</p>

<pre><code class="language-scala">val umid_lsa_renamed = umid_lsa_ori.toDF(&quot;id&quot; +: umid_lsa_ori.columns.tail.map(&quot;umid_&quot; + _): _*)
</code></pre>

<h1 id="toc_44">Read <code>.tar.gz</code> file</h1>

<p>Theoretically, spark is able to handle <code>.tar.gz</code> file naturally. It is in its <a href="https://github.com/apache/spark/blob/branch-2.2/core/src/main/scala/org/apache/spark/util/Utils.scala#L473">source code</a>: <code>if (fileName.endsWith(&quot;.tar.gz&quot;) || fileName.endsWith(&quot;.tgz&quot;))</code></p>

<p>However, <code>.tar.gz</code> is <a href="https://hvivani.com.ar/2014/11/23/mapreduce-compression-and-input-splits/">not a splittable compression format</a>, if we are using <code>sparkSession.read</code>, only a single executor will read the whole <code>.tar.gz</code> file. Therefore, if the size of the file is greater than <code>2GB</code>, we will have errors </p>

<p>The work around is use <code>sc.textFile(path, minPartitions)</code> to read it in then save it to HDFS again using <code>saveAsTextFile</code> and convert it to a splittable compression format.</p>

<p>Or try to use <code>pig</code> to read and convert it to a better compression format.</p>

<h1 id="toc_45">Spark join timeout errors</h1>

<p>See this error a lot when doing join, especially with large dataframe</p>

<blockquote>
<p>Caused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]</p>
</blockquote>

<p>There are two ways to solve the issue</p>

<ol>
<li><p><a href="http://stackoverflow.com/a/41126034/4229125">Post</a> set <code>spark.sql.broadcastTimeout</code> to <code>3600</code> instead of <code>300</code>. This proved to work on Spark 2.0.2 version. </p></li>
<li><p><a href="http://stackoverflow.com/a/36424994/4229125">Post</a> persist both dataframe to force a <code>ShuffleHashJoin</code></p></li>
</ol>

<p>Some more details showing in this type of timeout error</p>

<blockquote>
<p>org.apache.spark.SparkException: Exception thrown in awaitResult: <br/>
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:194)<br/>
    at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:120)<br/>
    at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:229)</p>
</blockquote>

<hr/>

<ul>
<li>
<a href="#toc_0">After join drop some columns cause Error</a>
</li>
<li>
<a href="#toc_1">read Lzo into Dataset</a>
</li>
<li>
<a href="#toc_2">read CSV into Dataset</a>
</li>
<li>
<a href="#toc_3">Don&#39;t use user defined functions in agg yet</a>
</li>
<li>
<a href="#toc_4">Use iterator and for comprehension to get cross product</a>
</li>
<li>
<a href="#toc_5"><font color='salmon'>? Strange java.io.NotSerializableException</font></a>
</li>
<li>
<a href="#toc_6">Using Spark-shell REPL</a>
</li>
<li>
<a href="#toc_7">Running spark job in oozie</a>
</li>
<li>
<a href="#toc_8">Another error for running spark job in oozie</a>
</li>
<li>
<a href="#toc_9">More dependency conflict for running oozie spark workflow</a>
</li>
<li>
<a href="#toc_10">spark config</a>
</li>
<li>
<a href="#toc_11">graphX connectedComponent</a>
</li>
<li>
<a href="#toc_12">Using of Bloom Filter</a>
</li>
<li>
<a href="#toc_13">Add a jar inside the spark REPL</a>
</li>
<li>
<a href="#toc_14">Get size of an object inside spark REPL</a>
</li>
<li>
<a href="#toc_15">Configuration that WON&#39;T work</a>
</li>
<li>
<a href="#toc_16">My experience for OOM debugging</a>
</li>
<li>
<a href="#toc_17">Date in Spark Dataset</a>
</li>
<li>
<a href="#toc_18">Encoder Issue in Dataset</a>
<ul>
<li>
<a href="#toc_19">Unable to find Encoder at Compiling time</a>
</li>
<li>
<a href="#toc_20">Unable to find Encoder at Run time</a>
</li>
</ul>
</li>
<li>
<a href="#toc_21">Split Dataset and save them separately simultaneously</a>
</li>
<li>
<a href="#toc_22">Hint the correct side of broadcast join</a>
</li>
<li>
<a href="#toc_23">Schema after join of two Dataset</a>
</li>
<li>
<a href="#toc_24">Handling A file sequentially</a>
</li>
<li>
<a href="#toc_25">Type Class Reflect of Spark Scala</a>
</li>
<li>
<a href="#toc_26">Spark shell Class initialization Error</a>
</li>
<li>
<a href="#toc_27">Spark date timezone converting</a>
</li>
<li>
<a href="#toc_28">Another Spark from date string to <code>DateType</code> converting</a>
</li>
<li>
<a href="#toc_29">Config spark jars in a folder</a>
</li>
<li>
<a href="#toc_30">Using python inside Spark (Scala) code</a>
</li>
<li>
<a href="#toc_31">DataType inside Dataset Row</a>
</li>
<li>
<a href="#toc_32">Read and Write in LZO format</a>
</li>
<li>
<a href="#toc_33">UDF for Dataset&#39;s column operation</a>
</li>
<li>
<a href="#toc_34">Kill a problematic executor</a>
</li>
<li>
<a href="#toc_35">cache and persist is a transform</a>
</li>
<li>
<a href="#toc_36">Another possible issue about cache</a>
</li>
<li>
<a href="#toc_37">parse URL get domain</a>
</li>
<li>
<a href="#toc_38">Get Seq of Tuple from DataFrame Row</a>
</li>
<li>
<a href="#toc_39">Handling dirty data</a>
</li>
<li>
<a href="#toc_40">User Agent parse</a>
</li>
<li>
<a href="#toc_41">Explode a column of Array to columns without using RDD</a>
</li>
<li>
<a href="#toc_42"><code>WrappedArray</code> in DataSet need use <code>Seq</code> to get</a>
</li>
<li>
<a href="#toc_43">Change all column names once</a>
</li>
<li>
<a href="#toc_44">Read <code>.tar.gz</code> file</a>
</li>
<li>
<a href="#toc_45">Spark join timeout errors</a>
</li>
</ul>



    

      </div>

      <div class="row">
        <div class="large-6 columns">
        <p class="text-left" style="padding:15px 0px;">
      
          <a href="14742423250740.html" 
          title="Previous Post: setting">&laquo; setting</a>
      
        </p>
        </div>
        <div class="large-6 columns">
      <p class="text-right" style="padding:15px 0px;">
      
          <a  href="14981525245438.html" 
          title="Next Post: ssh @ In">ssh @ In &raquo;</a>
      
      </p>
        </div>
      </div>
      <div class="comments-wrap">
        <div class="share-comments">
          <div id="disqus_thread"></div>
<script>

/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */
/*
var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//echo-ohce.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

          

          
        </div>
      </div>
    </div><!-- article-wrap -->
  </div><!-- large 8 -->




 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <h1>Echo-ohcE</h1>
                <div class="site-des">Code For Big Data</div>
                <div class="social">











  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="tips.html"><strong>tips</strong></a>
        
            <a href="documentation.html"><strong>documentation</strong></a>
        
            <a href="expired.html"><strong>expired</strong></a>
        
            <a href="stock.html"><strong>stock</strong></a>
        
            <a href="theory.html"><strong>theory</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="14770727250961.html">Advanced Analytics with Spark Reading Notes</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14742423250557.html">Github Page, Hexo, MWeb, GoDaddy setting</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14858223421956.html">Machine Learning</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14836486252242.html">aws</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14742423250478.html">bash</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
