<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Echo-ohcE]]></title>
  <link href="www.echo-ohce.com/atom.xml" rel="self"/>
  <link href="www.echo-ohce.com/"/>
  <updated>2018-05-01T10:29:48-07:00</updated>
  <id>www.echo-ohce.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Finite State]]></title>
    <link href="www.echo-ohce.com/15216148471093.html"/>
    <updated>2018-03-20T23:47:27-07:00</updated>
    <id>www.echo-ohce.com/15216148471093.html</id>
    <content type="html"><![CDATA[
<p>For Finite State Automata and Finite State Transducer and their application in NLP</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">FSA</a>
<ul>
<li>
<a href="#toc_1">Motivation, Why Automata?</a>
</li>
<li>
<a href="#toc_2">Theory — Finite State Automata and You!</a>
</li>
<li>
<a href="#toc_3">In Practice — An Automaton as a data structure</a>
</li>
<li>
<a href="#toc_4">Teh Codez — Building an Automaton</a>
</li>
<li>
<a href="#toc_5">Automaton Creation Considerations</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Finite-State-Transducers</a>
<ul>
<li>
<a href="#toc_7">Quick Primer on Finite State Machines</a>
</li>
<li>
<a href="#toc_8">FSMs for Language Processing</a>
</li>
<li>
<a href="#toc_9">Finite State Transducers</a>
</li>
<li>
<a href="#toc_10">Converting Rules to FSTs</a>
</li>
<li>
<a href="#toc_11">Extending the FSTs</a>
</li>
<li>
<a href="#toc_12">Composing a single FST</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">Using Finite State Transducers in Lucene</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">FSA</h1>

<p><a href="https://opensourceconnections.com/blog/2013/02/21/lucene-4-finite-state-automaton-in-10-minutes-intro-tutorial/">This article</a> is intended to help you bootstrap your ability to work with Finite State Automata (note automata == plural of automaton). Automata are a unique data structure, requiring a bit of theory to process and understand. Hopefully what’s below can give you a foundation for playing with these fun and useful Lucene data structures!</p>

<h2 id="toc_1">Motivation, Why Automata?</h2>

<p>When working in search, a big part of the job is making sense of loosely-structured text. For example, suppose we have a list of about 1000 valid first names and 100,000 last names. Before ingesting data into a search application, we need to extract first and last names from free-form text.</p>

<p>Unfortunately the data sometimes has full names in the format “LastName, FirstName” like “Turnbull, Doug”. In other places, however, full names are listed “FirstName LastName” like “Doug Turnbull”. Add a few extra representations, and to make sense out of what strings represent valid names becomes a chore.</p>

<p>This becomes especially troublesome when we’re depending on these as natural identifiers for looking up or joining across multiple data sets. Each data set might textually represent the natural identifier in subtly different ways. We want to capture the representations across multiple data sets to ensure our join works properly.</p>

<p>So… Whats a text jockey to do when faced with such annoying inconsistencies?</p>

<p>You might initially think “regular expression”. Sadly, a normal regular expression can’t help in this case. Just trying to write a regular expression that allows a controlled vocabulary of 100k valid last names but nothing else is non-trivial. Not to mention the task of actually using such a regular expression.</p>

<p>But there is one tool that looks promising for solving this problem. Lucene 4.0’s new Automaton API. Lets explore what this API has to offer by first reminding ourselves about a bit of CS theory.</p>

<h2 id="toc_2">Theory — Finite State Automata and You!</h2>

<p>Lucene’s Automaton API provides a way to create and use Finite State Automata(FSAs). For those who don’t remember their senior level computer science, FSA is a computational model whereby input symbols are read by the computer — the automaton — to drive a state machine. The state machine is simply a graph with nodes and labeled, directed edges. Each node in the graph is a state, and each edge is a transition labeled with a potential input symbol. By matching the current node’s edges to the current input symbol, the automaton follows edges to the next state. The next input symbol is read, and based on the transitions of the new state, we transition to yet another state, so-on and so-forth.</p>

<p>More importantly here, FSA’s are a way of specifying a Regular Language. If we think of all the input symbols as elements of a language, we can use an FSA to specify a language and determine if a given input string is valid for the language. We do this by processing the input string, following the transitions of an FSA until we reach a terminus node. If we exhaust the input symbols at such a node, then the string of symbols can be said to match the language. Otherwise, we can say the input string does not match the language.</p>

<p>So for example, in the figure below. The string “nice” that is the sequence of symbols “n”, “i”, “c”, “e” are accepted as a member of the language. All other strings are rejected:</p>

<p><img src="media/15216148471093/15216152173144.jpg" alt=""/><br/>
<em>An FSA specifying a language that accepts the word &quot;nice&quot;</em></p>

<p>A regular language could be just a set of valid strings. Or, it could be something a bit fuzzier like a Levenshtein distance or regular expression which as it turns out can be represented in a regular language. But even more powerfully, it can be a concatenation, union, or intersection of all of these.</p>

<h2 id="toc_3">In Practice — An Automaton as a data structure</h2>

<p>In practice, Lucene automata are useful as as a data structure that bridges between a traditional Set&lt;&gt; and hand-written regular expression. When compared to a HashSet or TreeSet the memory representation (can be) much, much smaller, with very fast lookups. Moreover, Automata give you fuzzier features like regular expression matching.</p>

<p>Its not a general purpose set replacement, however. For an automaton, the set is all the strings of symbols that match the language. However, due to all the fuzzy matching potentially using “regular expressions” enumerating all the input strings that match the language, that is enumerating the members of the set, is non-trivial and might never terminate. Think about it this way, traversal involves manually traversing a graph, so every node, whether a terminus or not, must be visited. This traversal might never terminate because you could have a * regular expression. So enumerating the strings that match the language will involve infinitely repeating the pattern before the *.</p>

<p>Another factor to consider is that while lookup time and memory usage are much, much smaller when compared to a Set&lt;&gt;, indexing can be very time consuming. For the use case of creating an automaton up front to specify a relatively static language once this isn’t a big deal. But for data structures that change frequently, automatons shouldn’t be the first choice.</p>

<p>When compared to a regular expression, the Automaton lends itself to being able to hold more complex regular languages than the ones you could specify in a traditional regular expression. For example, it would be difficult to specify a regular expression that had 500,000 potential first names followed by 1,000 last names unioned with 1,000 last names, a comma, followed by one of 500,000 first names. That’s Perl that’s not even write only. Automaton’s give you the ability to effectively write an extremely rich and large a “regular expression” in readable code in a way that won’t make generating/parsing such a regular expression a giant chore.</p>

<p>Next, lets see how we actually build a useful automaton</p>

<h2 id="toc_4">Teh Codez — Building an Automaton</h2>

<p>(Note all code below can be found at <a href="https://github.com/o19s/lucene_automata_tutorial">this</a> github repo)</p>

<p>To show off the API, lets take the example we discussed at the outset. Lets say our language allows two forms of full names “FirstName LastName” and “LastName, FirstName”. For simplicity, lets validate only a set of names. First Names: {&quot;Doug&quot;, &quot;John&quot;} and last names {&quot;Berryman&quot;, &quot;Turnbull&quot;}. So in our world all the valid names are “Doug Berryman”, “Doug Turnbull”, “John Berryman”, and “John Turnbull”. Forms where last name comes first followed by a comma, followed by the first name (ie “Turnbull, Doug”) are also considered valid.</p>

<p>So how do we use the Lucene API to build at Automaton to validate this syntax?</p>

<p>There are two key classes that you’ll use over-and-over for building meaningful automata. First the BasicAutomata class provides static methods for constructing automata out of simple building blocks (in our case individual strings). Second the BasicOperations class provides utility methods for combining Automata into unions, intersections, or concatenations of other automata.</p>

<p>Outside of these two central classes, we can also fold in additional automata from other classes. For example, regular expressions via the RegExp class.</p>

<p>Ok, now lets actually start putting together some code. Lets first look at the form “FirstName LastName”. We want to specify a language that takes any of our first names {“John”, “Doug”}, followed by some number of whitespace characters, then followed by any of our last names {“Turnbull”, “Berryman”}.</p>

<p>Our first piece of code forms the foundation for all of our other automata. One of the basic automata we need to build is simply one built from a set of valid strings. For example, an automaton for last names that says “Turnbull” is valid, “Berryman” is valid, but “Pugh” is not.</p>

<p>As this task is going to be a pretty common, occurrence, we’ll be using this utility function that builds an automaton for accepting only the values from the passed in String array:</p>

<pre><code class="language-java">/**
 * @param strs
 *   All the strings that we want to allow in the returned language
 * @return
 *   An automaton that allows only the passed in strings
 */
public static Automaton stringUnionAutomaton(String[] strs) {
    Automaton strUnion = null;
    // Simply loop through the strings and place them in the automaton
    for (String str: strs) {
        // Basic building block, make an automaton that accepts a singl
        // string
        Automaton currStrAutomaton = BasicAutomata.makeString(str);
        if (strUnion == null) {
            strUnion = currStrAutomaton;
        }
        else {
            // Combine the current string with the Automata for the
            // previous string, saying that this new string is also valid
            strUnion = BasicOperations.union(strUnion, currStrAutomaton);
        }
    }
    return strUnion;
}
</code></pre>

<p>Notice how on every iteration, we create an automaton for the current string. The first iteration initializes the resulting automaton(strUnion) to the current string’s automata (currStrAutomaton&#39;). On subsequent iterations, we set the resultingstrUnionto the union ofcurrStrAutomatonand itself. Finally returningstrUnion` as the union of all the strings passed in.</p>

<p>With this building block, we can build up a more complex Automaton for our FirstName LastName form:</p>

<pre><code class="language-java">/**
 * @param firstNames
 *   Set of allowable first names
 * @param lastNames
 *   Set of allowable last names
 * @return
 *   An automaton that allows FirstName\s+LastName
 */
public static Automaton createFirstBeforeLastAutomaton(String[] firstNames, String[] lastNames) {
    List&lt;Automaton&gt; allAutomatons = new ArrayList&lt;Automaton&gt;();
    // Use our builder to create an automaton that allows all the first names
    allAutomatons.add(stringUnionAutomaton(firstNames));

    // Add in an Automaton that allows any whitespace by using
    // the regular expression
    RegExp anyNumberOfSpaces = new RegExp(&quot;[ \t]+&quot;);
    Automaton anySpaces = anyNumberOfSpaces.toAutomaton();
    allAutomatons.add(anySpaces);

    // Add in an Automaton that allows all our last names
    allAutomatons.add(stringUnionAutomaton(lastNames));

    // Return the concatenation off all these automatons
    return BasicOperations.concatenate(allAutomatons);
}
</code></pre>

<p>In this code, we’re building three automata and returning the concatenation of all three. So to be a member of the concatenated automaton’s language, if a string passes the first automaton, then with additional characters passes the second automaton, and finally as characters are exhausted finishes the last automaton, we’ll consider this a valid member of this language.</p>

<p>Note the use of the RegExp class. This class supports basic regular-expression matching and allows us to match one-or-more tabs or spaces in the input.</p>

<p>The LastName, FirstName form is similar:</p>

<pre><code class="language-java">public static Automaton createLastBeforeFirstAutomaton(String[] firstNames, String[] lastNames) {
    List&lt;Automaton&gt; allAutomatons = new ArrayList&lt;Automaton&gt;();
    allAutomatons.add(stringUnionAutomaton(lastNames));

    RegExp commaPlusAnyNumberOfSpaces = new RegExp(&quot;,[ \t]+&quot;);
    allAutomatons.add(commaPlusAnyNumberOfSpaces.toAutomaton());
    allAutomatons.add(stringUnionAutomaton(firstNames));
    return BasicOperations.concatenate(allAutomatons);
}
</code></pre>

<p>The only difference here is we’re validating last names before first and our regex separator now has a comma. Otherwise, this is very similar to the other form.</p>

<p>To finish things off, we simply have to create an automata that takes the union of both forms, allowing strings of either language to be valid in the resulting language:</p>

<pre><code class="language-java">public static Automaton createNameValidator(String[] firstNames,
                                            String[] lastNames) {
    return BasicOperations.union(createFirstBeforeLastAutomaton(firstNames, lastNames),
                                 createLastBeforeFirstAutomaton(firstNames, lastNames));
}
</code></pre>

<p>Woohoo! You should be ready to go! Just keep in mind one or two things when building more complex automata:</p>

<h2 id="toc_5">Automaton Creation Considerations</h2>

<p>One of the things that makes the Automaton special is the potential minimal in-memory representation of the data structure. This gives you powerful lookup capabilities against a complex language with a large vocabulary, but noticeably increases index time when compared to a traditional data structure.</p>

<p>To ensure the minimal representation of an Automaton, its important to note that Lucene may not be always keeping the most optimal representation of the data structure in memory. Without minimizing, you could have problems with lookup speed and will certainly have problems exhausting the jvm heap.</p>

<p>For example, if we said that “Ed” and “Eddy” are valid strings in our language, we might initially have something like:</p>

<pre><code>         []---E---&gt;[]---d---[*]
          \
           ---E---&gt;[]---d---[]---d---[]---y---[*]
</code></pre>

<p>Add this up over time, and we end up with a horrendous looking graph that leaves you wondering why anyone would ever bother using Automata!</p>

<p>Part of the secret sauce to Lucene’s Automaton’s is minimizations. Instead of representing the graph as a gnarly web of duplicated gobbly, gook, we can perform the minimization operation on the graph above to be simply:</p>

<pre><code>         []---E---&gt;[]---d---[*]
                        \
                         d---[]---y---[*]
</code></pre>

<p>By periodically calling Automata.minimize you can reduce the memory footprint of your automaton.</p>

<p>You can track roughly how big your automaton is getting with Automata.getNumberOfStates() and Automata.getNumberOfTransitions. Its generally a good idea to keep an eye on the size of your automaton and deal with any bloat that occurs during indexing.</p>

<h1 id="toc_6">Finite-State-Transducers</h1>

<p><a href="http://infolocata.com/mirovia/finite-state-transducers-for-natural-language-processing/">Finite State Transducers</a> provide a method for performing mathematical operations on ordered collections of context-sensitive rewrite rules such as those commonly used to implement fundamental natural language processing tasks. Multiple rules may be composed into a single pass, mega rule, significantly increasing the efficiency of rule-based systems.</p>

<h2 id="toc_7">Quick Primer on Finite State Machines</h2>

<p>A finite-state machine (FSM) or automata is an abstract mathematical model of computation that is capable of storing a status or state and changing this state based on input. While not as powerful as other computational models such as Turing machines due to their limited memory, FSMs are applicable to a number of electronic modeling, engineering, and language processing problems. An FSM can be represented as a set of nodes representing the various states of the system and labeled edges between these nodes where the edges represent transitions from one state to another and the labels represent conditions on these transitions. A stream of input (an input tape containing a string) is then ‘processed’ by the FSM, potentially causing a number of state transitions. The simple FSM example below is a remarkably accurate computational model for a ferret:</p>

<p><img src="media/15216148471093/15216155212383.jpg" alt=""/></p>

<p>The daily behavior of my ferret, Pangur Bán, would best be represented by an input tape containing the string, ‘tired,tired,tired,tired,hungry,tired,tired,tired,bored,tired,tired,tired’:</p>

<h2 id="toc_8">FSMs for Language Processing</h2>

<p>In language processing, an FSM containing a start state (node) and an end state can be used to generate or recognize a language defined by all possible combinations of symbols (conditional labels) on each of the edges generated by traversing the FSM from the start state to the end state. The class of languages generated by finite automata is known as the class of regular languages.</p>

<h2 id="toc_9">Finite State Transducers</h2>

<p>A finite state transducer (FST) is a special type of finite state machine. Specifically, an FST has two tapes, an input string and an output string. Rather than just traversing (and accepting or rejecting) an input string, an FST translates the contents of its input string to its output string. Or put another way, it accepts a string on its input tape and generates another string on its output tape.</p>

<p>Context-Sensitive Rules<br/>
FSTs are particularly useful in the implementation of certain natural language processing tasks. Context-sensitive rewriting rules (ex: ax → bx) are adequate for implementing certain computational linguistic tasks such as morphological stemming and part-of-speech tagging. Such rewrite rules are also computationally equivalent to finite-state transducers, providing a unique path for optimizing rule based systems.</p>

<p>Take, for example, the following set of ordered context-sensitive rules:</p>

<p>1)  change ‘c’ to ‘b’ if followed by ‘x’       cx → bx<br/>
2)  change ‘a’ to ‘b’ if preceded by ‘rs’      rsa → rsb<br/>
3)  change ‘b’ to ‘a’ if preceded by ‘rs’ and followed by ‘xy’     rsbxy → rsaxy<br/>
Given the following string on the input tape:</p>

<p>rsaxyrscxy</p>

<p>the application of the given rule set would proceed as follows:</p>

<p>1) rsaxyrscxy → rsaxyrsbxy<br/>
2) rsaxyrsbxy → rsbxyrsbxy<br/>
3) rsbxyrsbxy → rsaxyrsaxy</p>

<p>The time required to apply a sequence of context-sensitive rules is dependent upon the number of rules, the size of the context window, and the number of characters in the input string. The inefficiencies  in such an implementation are highlighted in this example by the multi-step transformation required to translate ‘c‘ to ‘b‘ then ‘b‘ to ‘a‘ by rules 1 and 3, and the redundant transformation of ‘a‘ to ‘b‘ and back to ‘a‘ by rules 2 and 3. Finite State Transducers provide a path to eliminate these inefficiencies. But first we need to convert the rules to State Machines.</p>

<h2 id="toc_10">Converting Rules to FSTs</h2>

<p>To do this, we simple represent each rule as an FST where each link between states represent the acceptance of an input character and the expression of the corresponding output character. This input / output combination is denoted within an FST by labeling edges with both the input and output character separated by the ‘/’ character. Following is an FST for each of the above rules:<br/>
<img src="media/15216148471093/15216155691636.jpg" alt=""/></p>

<h2 id="toc_11">Extending the FSTs</h2>

<p>While the FSTs above represent our set of context-sensitive rules, they would be of little use in matching against an input string as each is designed to process exactly the context widow described in its corresponding rule. To make each FST applicable to a string of arbitrary length and perform the necessary translation each time the rule is fired, we will need to extend each of the Transducers. We do this by allowing for every possible input in our language at each state. For example, rule 1 must be able to handle the string rsaxyrscxy. Since rule 1 matches the string ‘cx‘ and outputs the string ‘bx‘, it must handle the characters ‘r‘, ‘s‘, ‘a‘, ‘x‘, ‘y‘, ‘r‘, and ‘s‘ before finally encountering ‘c‘ and ‘x‘. Then it must handle the final ‘y‘ character. Each of these characters (and all other possible characters) could be explicitly listed on its own individual edge, but to simplify, we can create a single edge labeled with ‘?/?‘, to match and output any character not already represented on another edge leading from that state. FST extension of rules with a trailing context will also require the use of edges with an input but no output (labeled with ‘ε‘ for output) and edges with multiple outputs. Following is the extension for each of the above FSTs:<br/>
<img src="media/15216148471093/15216156059817.jpg" alt=""/></p>

<h2 id="toc_12">Composing a single FST</h2>

<p>One of the operations that can be performed on a pair of FSTs is composition. The composition operation will take two deterministic FSTs, A and B, and combine their nodes and edges into a single deterministic FST. The resulting Transducer will accept an input string of arbitrary length and output the equivalent of applying Transducer A followed by Transducer B. A full explanation of the FST composition algorithm is beyond the scope of this write-up. Following is the FST resulting from the composition of FSTs 1 and 2 followed by the composition of the resulting FST and FST 3:<br/>
<img src="media/15216148471093/15216156228351.jpg" alt=""/></p>

<p>Note that while the output of applying the final FST to the input string ‘rsaxyrscxy’ is exactly equivalent to the output of applying each of the individual context-sensitive rules (‘rsaxyrsaxy‘), the transformation required only a single pass through the FST and did not result in any inefficient transformations. While the time required to apply the original rules was dependent upon the number of rules, the size of the context window, and the number of characters on the input tape, application time of the final FST is dependent only upon the number of characters on the input tape.</p>

<h1 id="toc_13">Using Finite State Transducers in Lucene</h1>

<p><a href="http://blog.mikemccandless.com/2010/12/using-finite-state-transducers-in.html">FSTs</a> are finite-state machines that map a term (byte sequence) to an arbitrary output. They also look cool:<br/>
<img src="media/15216148471093/15216158111078.jpg" alt=""/><br/>
That FST maps the sorted words mop, moth, pop, star, stop and top to their ordinal number (0, 1, 2, ...). As you traverse the arcs, you sum up the outputs, so stop hits 3 on the s and 1 on the o, so its output ordinal is 4. The outputs can be arbitrary numbers or byte sequences, or combinations, etc. -- it&#39;s pluggable.</p>

<p>Essentially, an FST is a SortedMap<ByteSequence,SomeOutput>, if the arcs are in sorted order. With the right representation, it requires far less RAM than other SortedMap implementations, but has a higher CPU cost during lookup. The low memory footprint is vital for Lucene since an index can easily have many millions (sometimes, billions!) of unique terms.</p>

<p>There&#39;s a great deal of theory behind FSTs. They generally support the same operations as FSMs (determinize, minimize, union, intersect, etc.). You can also compose them, where the outputs of one FST are intersected with the inputs of the next, resulting in a new FST.</p>

<p>There are some nice general-purpose FST toolkits (OpenFst looks great) that support all these operations, but for Lucene I decided to implement this neat algorithm which incrementally builds up the minimal unweighted FST from pre-sorted inputs. This is a perfect fit for Lucene since we already store all our terms in sorted (unicode) order.</p>

<p>The resulting implementation (currently a patch on LUCENE-2792) is fast and memory efficient: it builds the 9.8 million terms in a 10 million Wikipedia index in ~8 seconds (on a fast computer), requiring less than 256 MB heap. The resulting FST is 69 MB. It can also build a prefix trie, pruning by how many terms come through each node, with even less memory.</p>

<p>Note that because addition is commutative, an FST with numeric outputs is not guaranteed to be minimal in my implementation; perhaps if I could generalize the algorithm to a weighted FST instead, which also stores a weight on each arc, that would yield the minimal FST. But I don&#39;t expect this will be a problem in practice for Lucene.</p>

<p>In the patch I modified the SimpleText codec, which was loading all terms into a TreeMap mapping the BytesRef term to an int docFreq and long filePointer, to use an FST instead, and all tests pass!</p>

<p>There are lots of other potential places in Lucene where we could use FSTs, since we often need map the index terms to &quot;something&quot;. For example, the terms index maps to a long file position; the field cache maps to ordinals; the terms dictionary maps to codec-specific metadata, etc. We also have multi-term queries (eg Prefix, Wildcard, Fuzzy, Regexp) that need to test a large number of terms, that could work directly via intersection with the FST instead (many apps could easily fit their entire terms dict in RAM as an FST since the format is so compact). The FST could be used for a key/value store. Lots of fun things to try!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[advanced Analytics with Spark Reading Notes]]></title>
    <link href="www.echo-ohce.com/14770727250961.html"/>
    <updated>2016-10-21T10:58:45-07:00</updated>
    <id>www.echo-ohce.com/14770727250961.html</id>
    <content type="html"><![CDATA[
<p>This documents the reading notes of this book. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Souce Code Link:</a>
</li>
<li>
<a href="#toc_1">Chapter 1 and 2</a>
<ul>
<li>
<a href="#toc_2">Start REPL</a>
</li>
<li>
<a href="#toc_3">Quick tricks to use</a>
</li>
<li>
<a href="#toc_4">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_5">Note about the <code>getOrElse()</code> method in the above map:</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Chapter 3</a>
<ul>
<li>
<a href="#toc_7">Download dataset and put on HDFS and start REPL</a>
</li>
<li>
<a href="#toc_8">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_9">The use of mllib ALS</a>
</li>
<li>
<a href="#toc_10">Rating</a>
</li>
<li>
<a href="#toc_11">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_12">split into Training set and CV set and calculate the AUC</a>
</li>
</ul>
</li>
</ul>


<hr/>

<h1 id="toc_0">Souce Code Link:</h1>

<p><a href="https://github.com/sryza/aas/tree/1st-edition">github</a></p>

<h1 id="toc_1">Chapter 1 and 2</h1>

<h2 id="toc_2">Start REPL</h2>

<pre><code class="language-bash"># this works on hlab
spark-shell --master yarn-client

# this works on dblab (this one is already outdated)
/home/xiaogu/repo/spark-1.6.1-bin-hadoop2.4/bin/spark-shell --queue datascience

# this works on dblab for spark 2.1.0 (Scala 2.11.8, Java 1.7.0_45)
# the spark-shell is a shell command with necessary parameters for db
cd ~/spark_pg
spark-shell
</code></pre>

<h2 id="toc_3">Quick tricks to use</h2>

<ol>
<li>help: <code>:help</code></li>
<li>history / find the variable or function name: <code>:history</code> or <code>:h?</code></li>
<li>paste code: <code>:paste</code></li>
</ol>

<h2 id="toc_4">Inside REPL Follow along</h2>

<pre><code class="language-scala">val rawblocks = sc.textFile(&quot;linkage/*.csv&quot;)
rawblocks.first
val head = rawblocks.take(10)
head.length
head foreach println

def isHeader(line:String):Boolean =
    line.contains(&quot;id_1&quot;)

head filterNot isHeader foreach println
head filter (ele =&gt; ! isHeader(ele)) foreach println
head filter (!isHeader(_)) foreach println

// Note that spark RDD does not have filterNot method
val noheader = rawblocks filter (!isHeader(_))

def toDouble(token:String):Double = {
    if (token.equals(&quot;?&quot;)) Double.NaN;
    else token.toDouble;}
    
val line = head(5)

def parse(line:String):(Int, Int, Array[Double], Boolean) = {
    val tokens = line split (&#39;,&#39;);
    val id1 = tokens(0).toInt;
    val id2 = tokens(1).toInt;
    val scores:Array[Double] = tokens slice (2, 11) map toDouble;
    val matched = tokens(11).toBoolean;
    (id1, id2, scores, matched)}

val tup = parse(line)

case class MatchData(id1:Int, id2:Int, scores:Array[Double], matched:Boolean);

def parse(line:String):MatchData = {
    val tokens = line split (&#39;,&#39;);
    val id1 = tokens(0).toInt;
    val id2 = tokens(1).toInt;
    val scores:Array[Double] = tokens slice (2, 11) map toDouble;
    val matched = tokens(11).toBoolean;
    MatchData(id1, id2, scores, matched)}
    

val parsed = rawblocks filter (!isHeader(_)) map parse;
parsed.cache()

val mds = head filterNot isHeader map parse;

val grouped = mds.groupBy(_.matched);

val matchCounts = parsed.map(_.matched).countByValue()
val matchCountsSeq = matchCounts.toSeq

import java.lang.Double.isNaN

// Using the stats Action on RDD[Double]
val stats = for (i &lt;- 0 until 9) yield (parsed map 
                (_.scores(i)) filter (!isNaN(_)) stats)
                
// write and load the NAStatCounter.scala
:load spark_book/NAStatCounter.scala

// use foldLeft, zip, map to build the stat with missing values

val samp_m0 = rawblocks take 10
val samp_m1 = samp_m0 filterNot isHeader map parse

val sample = samp_m1.foldLeft(samp_m1.head.scores map (x =&gt; new NAStatCounter())) ((statCounterArry, record) =&gt; statCounterArry zip record.scores map {case (preStatCounter, newScore) =&gt; preStatCounter.add(newScore)})
// Couple things remember:
// 1. samp_m1.foldLeft()() is correct, do not use infix here, samp_m1 foldLeft () () is wrong!
// 2. map {case () =&gt;} is correct, do not use map(case ()=&gt;)
// 3. Using zip to zip up two arrays, that&#39;s how we get the corresponding foldings

// Note foldLeft is in Scala. In spark use aggregate. Need And the aggregate API is different than scala’s foldLeft. here is a good explaination:
http://cuipengfei.me/blog/2014/10/31/spark-fold-aggregate-why-not-foldleft/

val statsm = statsWithMissing(parsed filter (_.matched) map (_.scores))
val statsn = statsWithMissing(parsed filter (!_.matched) map (_.scores))

case class Scored(md : MatchData, score : Double)
val ct = parsed map (row =&gt; Scored(row, (Seq(2,5,6,7,8) map (idx =&gt; if (row.scores(idx).isNaN) 0 else row.scores(idx))).sum))
ct.cache()

val allTrue = (ct map (_.md.matched)).countByValue.getOrElse(true, 0).asInstanceOf[Number].doubleValue

val allFalse = (ct map (_.md.matched)).countByValue.getOrElse(false, 0).asInstanceOf[Number].doubleValue

// using different score cut threshold to divide true/false matches
import breeze.linalg._
val scoreRange = breeze.linalg.linspace( (ct map (_.score)).min, (ct map (_.score)).max, 5)

val truePositive = scoreRange map (cut =&gt; (ct filter (_.score&gt;cut) map (_.md.matched)).countByValue().getOrElse(true, 0).asInstanceOf[Number].doubleValue)

</code></pre>

<h2 id="toc_5">Note about the <code>getOrElse()</code> method in the above map:</h2>

<p>Because what&#39;s in the map (given by <code>countByValue()</code>) is of type <code>Long</code>, and the default value I gave is of type <code>Int</code>, the <code>getOrElse()</code> method returns the super type for both that is <code>AnyVal</code>.</p>

<p><code>AnyVal</code> has no easy way to cast, so far what I found seems the easiest way is to first convert to type <code>Number</code> then cast to <code>Int</code>. <font color='red'> It can not be convert to type <code>Int</code> directly, otherwise exception will be thrown. </font> </p>

<p>put it all together, for a given map:</p>

<pre><code class="language-scala">val value = map.getOrElse(key, 0).asInstanceOf[Number].doubleValue
</code></pre>

<h1 id="toc_6">Chapter 3</h1>

<h2 id="toc_7">Download dataset and put on HDFS and start REPL</h2>

<p><code>http://bit.ly/1KiJdOR</code></p>

<p>The default spark-shell on hlab has driver memory and executor memory of 512MB, which are not enough for the job that we are doing for this chapter. Use the following setting for setting the memory</p>

<p><a href="http://stackoverflow.com/questions/31463382/increase-java-heap-size-in-spark-on-yarn">stackoverflow post</a></p>

<p><a href="https://spark.apache.org/docs/1.2.0/configuration.html">official spark option list</a></p>

<pre><code class="language-bash"># this works on hlab
spark-shell --master yarn-client --driver-memory 2g --executor-memory 2g --conf spark.rdd.compress=true
</code></pre>

<h2 id="toc_8">Inside REPL Follow along</h2>

<pre><code class="language-scala">val rawUserArtistData = sc.textFile(&quot;spark_book/user_artist_data.txt&quot;, 20)

val overLimitCnt = (rawUserArtistData map (_.split(&quot;\\s+&quot;) map (_.toLong)) 
                    filter (arr =&gt; arr(0) &gt; java.lang.Integer.MAX_VALUE 
                                || arr(1)  &gt; java.lang.Integer.MAX_VALUE)).count()
//Note that in scala, we use .size, but for spark RDD, we need use .count(), because it is parallel partitioned.

val rawArtistData =sc.textFile(&quot;spark_book/artist_data.txt&quot;, 20)

//note the following is use char of single quote
val test = rawArtistData take 10
val line0 = test(0) span (_ != &#39;\t&#39;) match {case (id, name) =&gt; (id.toInt, name.trim)}

//Using the flatMap and Option and pattern match with default to handle the dirty data
val artistByID = rawArtistData flatMap (_ span (_ != &#39;\t&#39;) 
                                        match { 
                                            case(id, name) =&gt; {
                                                try {
                                                    Some((id.toInt, name.trim))
                                                } catch {
                                                    case e:NumberFormatException =&gt; None
                                                }}
                                            case anythingElse =&gt; None})
                                        
val rawArtistAlias = sc.textFile(&quot;spark_book/artist_alias.txt&quot;, 20)

//Just as before, using Option to catch exceptions
val artistAliasOption = (rawArtistAlias map (_ span (_ != &#39;\t&#39;) 
                                             match {case (id1, id2) 
                                                      =&gt; try { 
                                                          (Some(id1.trim.toInt, id2.trim.toInt)) } 
                                                         catch { 
                                                          case e: NumberFormatException=&gt;None};
                                                    case _ =&gt; None}))

//remove Options
val artistAliasRDD = artistAliasOption flatMap (x=&gt;x)

//collect as Map
val artistAlias = artistAliasRDD collectAsMap

// pay special attention to the pattern matching of Array
val formatedUserArtistData =(rawUserArtistData map (_ split(&quot;\\s+&quot;) match 
    { case Array(id1Str, id2Str, cntStr) =&gt; try {
                                                Some(Array(id1Str.trim.toInt,
                                                           id2Str.trim.toInt,
                                                           cntStr.trim.toInt))
                                                }
                                            catch {
                                                case e:Throwable =&gt; None
                                                };
      case _ =&gt; None })) flatMap (x=&gt;x)
// build the Rating class used in MLLib
import org.apache.spark.mllib.recommendation._
// broadcast to save resource and speed up, then clean the alias
val bArtistAlias = sc.broadcast(artistAlias)

// special note:
// 1. use the broadcasted thing, need refer to its .value.
// 2. pay attention to how to use the pattern matching inside map anonymous func
val cleanedUserArtistData = (formatedUserArtistData map 
     ({case Array(userId, artistId, cnt) =&gt; 
       Rating(userId, bArtistAlias.value.getOrElse(artistId, artistId), cnt)
       })
    ).cache()

val model = ALS.trainImplicit(cleanedUserArtistData, 10, 5, 0.01, 1.0)                                                 

// see what model looks like
model.userFeatures take 10 map ({case (userId, scoresArr) =&gt; userId.toString + &quot; : &quot; + scoresArr.mkString(&quot; , &quot;)}) foreach println

model.productFeatures take 10 map ({case (productId, scoresArr) =&gt; productId.toString + &quot; : &quot; + scoresArr.mkString(&quot; , &quot;)}) foreach println
                                 
</code></pre>

<h2 id="toc_9">The use of mllib ALS</h2>

<p><a href="http://spark.apache.org/docs/latest/ml-collaborative-filtering.html">Official documents</a></p>

<p>A very good page for explaining the &quot;explicit preference&quot; and &quot;implicit preference&quot; at <a href="http://predictionio.incubator.apache.org/templates/recommendation/training-with-implicit-preference/">PredicctionIO</a></p>

<h2 id="toc_10">Rating</h2>

<p>A more compact class to represent a rating than Tuple3[Int, Int, Double].<br/>
It is kind of like case class <code>Rating(user: Int, product: Int, rating: Double)</code><br/>
<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.recommendation.Rating">reference</a></p>

<h2 id="toc_11">Inside REPL Follow along</h2>

<pre><code class="language-scala">
// find the artist list for userID = 2093760
val targetUserID = 2093760
val joined = cleanedUserArtistData filter (_.user == targetUserID) map (rateEntry =&gt; (rateEntry.product, rateEntry.rating)) join (artistByID)

// Note that these are RDDs, do not forget the collect!
(joined map ({case (artistID, (cnt, name)) =&gt; name + &quot; : &quot; + cnt.toInt.toString}) collect) foreach println


// make recommendation
// Note that it is NOT RDD, but regular scala Array
val recommendations = model.recommendProducts(targetUserID, 5)

val recommendSet = (recommendations map (_.product)).toSet

val relatedArtistNameMap = (artistByID filter (record =&gt; recommendSet.contains(record._1))).collectAsMap

recommendations map (rateEntry =&gt; relatedArtistNameMap.getOrElse(rateEntry.product, &quot;NoName&quot;) + &quot; : &quot; + rateEntry.rating.toString) foreach println

</code></pre>

<h2 id="toc_12">split into Training set and CV set and calculate the AUC</h2>

<pre><code class="language-scala">val Array(trainData, cvData) = cleanedUserArtistData.randomSplit(Array(0.9, 0.1))
trainData.cache()
cvData.cache()
val model = ALS.trainImplicit(trainData, 10, 5, 0.01, 1.0)

// calculate the mean AUC for cvData
// 1. get the prediction on cvData using the model
val cvDataOfUserProduct = cvData map (record =&gt; (record.user, record.product))
// predict take RDD[tuple2]
val predictRating = model.predict(cvDataOfUserProduct)

// construct (user, wrongproduct)
val allProduct = (cleanedUserArtistData map (_.product) distinct).collect()
val allProductSize = allProduct.size
val bAllProduct = sc.broadcast(allProduct)
// using mapPartitions and broadcast to save memory, groupBy shuffles the RDD partitions, the shuffled RDD will have the same groupBy key records together inside one partition.

cvData groupBy (_.user) mapPartitions 
    {case (user, ratings) =&gt; {
        val correctProductSet = (ratings map (_.product)).collect().toSet
        var counter = 0
        val randomGen = new scala.util.Random()
        val wrongProductArr = new ArrayBuffer[Int]()
        while (counter &lt; correctProductSet.size) {
            val productCandidate = bAllProduct.value(randomGen.nextInt(allProductSize))
            if (!correctProductSet.contains(productCandidate)) {
                counter += 1
                wrongProductArr += productCandidate
            }
        }
    }
    wrongProductArr
}

        
            
            
                        



</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">Souce Code Link:</a>
</li>
<li>
<a href="#toc_1">Chapter 1 and 2</a>
<ul>
<li>
<a href="#toc_2">Start REPL</a>
</li>
<li>
<a href="#toc_3">Quick tricks to use</a>
</li>
<li>
<a href="#toc_4">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_5">Note about the <code>getOrElse()</code> method in the above map:</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Chapter 3</a>
<ul>
<li>
<a href="#toc_7">Download dataset and put on HDFS and start REPL</a>
</li>
<li>
<a href="#toc_8">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_9">The use of mllib ALS</a>
</li>
<li>
<a href="#toc_10">Rating</a>
</li>
<li>
<a href="#toc_11">Inside REPL Follow along</a>
</li>
<li>
<a href="#toc_12">split into Training set and CV set and calculate the AUC</a>
</li>
</ul>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[aws]]></title>
    <link href="www.echo-ohce.com/14836486252242.html"/>
    <updated>2017-01-05T12:37:05-08:00</updated>
    <id>www.echo-ohce.com/14836486252242.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">setting the credential and connect to ec2</a>
</li>
<li>
<a href="#toc_1">Deploy a jupyter notebook server on aws</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">setting the credential and connect to ec2</h1>

<ul>
<li>Need install <code>awscli</code> by:</li>
</ul>

<pre><code class="language-bash">conda install -c conda-forge awscli
</code></pre>

<ul>
<li>Then run:</li>
</ul>

<pre><code class="language-bash">aws configure
</code></pre>

<ul>
<li><p>Log in the management account of aws, and check the Access Key ID and Secret Access Key for the developer account for the configure setting</p></li>
<li><p>After setting up, <code>boto3</code> can connect to the aws ec2</p></li>
</ul>

<pre><code class="language-python">import boto3
region = &#39;us-west-2&#39;
instance_id = &#39;i-0fa00c2c21a729e6e&#39;
ec2 = boto3.resource(&#39;ec2&#39;, region_name = region)
i = ec2.Instance(id=instance_id)
i.start()
print i.state
print &#39;{} or by DNS name: {}&#39;.format(i.public_ip_address, i.public_dns_name)
i.stop()
</code></pre>

<h1 id="toc_1">Deploy a jupyter notebook server on aws</h1>

<ul>
<li><a href="http://yangjie.me/2015/08/26/Run-Jupyter-Notebook-Server-on-AWS-EC2/">reference post 1</a></li>
<li><a href="http://navoshta.com/aws-tensorflow/">set up jupyter and tensorflow on aws</a></li>
<li><a href="https://gist.github.com/dengemann/3245552bcdbf7f8f2293abb96c2348f4">Here is a security group script for ipython on aws</a></li>
</ul>

<hr/>

<ul>
<li>
<a href="#toc_0">setting the credential and connect to ec2</a>
</li>
<li>
<a href="#toc_1">Deploy a jupyter notebook server on aws</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[bash]]></title>
    <link href="www.echo-ohce.com/14742423250478.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250478.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Using curly braces in Bash substitution</a>
</li>
<li>
<a href="#toc_1">nohup to a different file</a>
</li>
<li>
<a href="#toc_2">Append multiple lines to a file</a>
</li>
<li>
<a href="#toc_3">copy from bash shell to clipboard</a>
</li>
<li>
<a href="#toc_4">Hadoop hdfs get all except one folder (file)</a>
</li>
<li>
<a href="#toc_5">Hadoop hdfs / bash get matched folders</a>
</li>
<li>
<a href="#toc_6">check file encoding</a>
</li>
<li>
<a href="#toc_7">du without recursion</a>
</li>
<li>
<a href="#toc_8">checking running job and argument</a>
</li>
<li>
<a href="#toc_9">check the running job with complete arguments</a>
</li>
<li>
<a href="#toc_10">grep find negate</a>
</li>
<li>
<a href="#toc_11">grep find text in all files under a folder and all of its subfolders</a>
</li>
<li>
<a href="#toc_12">Keep the REPL across sessions</a>
</li>
<li>
<a href="#toc_13">Schedule Auto jobs on Mac using crontab</a>
</li>
<li>
<a href="#toc_14">Hadoop Yarn get the Aggregated Log for a job</a>
</li>
<li>
<a href="#toc_15">History search</a>
</li>
<li>
<a href="#toc_16">check what pid is listening to the port</a>
</li>
<li>
<a href="#toc_17">change mode for folder and file recursively</a>
</li>
<li>
<a href="#toc_18">input absolute path in mac os finder</a>
</li>
<li>
<a href="#toc_19">split file by regex using <code>csplit</code></a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Using curly braces in Bash substitution</h1>

<p>It is always a good practice to use curly braces like <code>${foo}</code>, but there are following situations it is a must:</p>

<ul>
<li>confusing strings <code>${foo}bar</code> vs. <code>$foobar</code> in which <code>foobar</code> is a single parameter.</li>
<li>expanding arrays <code>${array[42]}</code></li>
<li>expanding positional parameters beyond 9 <code>$8 $9 ${10}</code></li>
</ul>

<p><code>{}</code> is called <em>brace expansion</em>, <code>${}</code> is called <em>variable expansion</em><br/>
<a href="http://stackoverflow.com/questions/8748831/when-do-we-need-curly-braces-in-variables-using-bash">Reference Post</a>  </p>

<h1 id="toc_1">nohup to a different file</h1>

<pre><code class="language-bash">nohup some_command &gt; nohup_file.out 2&gt;&amp;1 &amp;
</code></pre>

<p>in which <code>2&gt;&amp;1</code> means <font color='red'>redirect <code>stderr</code> to the same output as <code>stdout</code></font>.  </p>

<h1 id="toc_2">Append multiple lines to a file</h1>

<p>Using a <em>eoi</em> (end of input) symbol for multiple lines</p>

<pre><code class="language-bash">cat &lt;&lt;EOI &gt;&gt; file
multiple lines
input here
EOI
</code></pre>

<p>in which, the <code>&lt;&lt;EOI</code> is registering a special symbol marking the end of input, which should be in a line by itself; the <code>&gt;&gt; file</code> means it is <font color='red'>append</font> to the file.</p>

<h1 id="toc_3">copy from bash shell to clipboard</h1>

<p>On mac, it is super easy: <code>pbcopy</code> and <code>pbpaste</code>.  </p>

<pre><code class="language-bash">cat ~/.bashrc | pbcopy
</code></pre>

<p>After that command content of the <code>~/.bashrc</code> file will be available for pasting with <code>cmd+v</code> shortcut.</p>

<h1 id="toc_4">Hadoop hdfs get all except one folder (file)</h1>

<p>As noted before, Hadoop use its own <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html#globStatus%28org.apache.hadoop.fs.Path%29">glob</a>. to do the pattern matching for file name and paths.</p>

<pre><code class="language-bash">$ hls alibaba/ipstats/
Found 8 items
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:36 alibaba/ipstats/IP
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/acookie_day
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/avg_acookie_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/avg_umid_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_acookie_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_day
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/tot_umid_cnt
drwxr-xr-x   - yu hdfs          0 2016-09-09 21:38 alibaba/ipstats/umid_day  

$ hdfs dfs -get alibaba/ipstats/[^I]*
</code></pre>

<p>The command gets all folders except for <code>alibaba/ipstats/IP</code>  </p>

<h1 id="toc_5">Hadoop hdfs / bash get matched folders</h1>

<p>This works both on bash and hadoop hdfs</p>

<p>Below is the HDFS folder:</p>

<pre><code class="language-bash">Found 6 items
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/FP
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/TP
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:38 alibaba/graph/stats/UNKNOWN
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:19 alibaba/graph/stats/cluster-size-histo
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:19 alibaba/graph/stats/coverage
drwxr-xr-x   - zwang hdfs          0 2016-09-30 00:45 alibaba/graph/stats/stats
</code></pre>

<p>Need get the folders of <code>cluster-size-histo</code>, <code>coverage</code> and <code>stats</code></p>

<pre><code class="language-bash">hdfs dfs -get alibaba/graph/stats/[cs]*
</code></pre>

<p>Now local folder:</p>

<pre><code class="language-bash">total 8
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 cluster-size-histo
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 coverage
-rw-rw-r-- 1 yu yu  9 Sep 30 19:28 FP_cnt
drwxrwxr-x 2 yu yu 43 Sep 30 19:37 stats
-rw-rw-r-- 1 yu yu  7 Sep 30 19:29 TP_cnt
</code></pre>

<p>Need delete folders of <code>cluster-size-histo</code>, <code>coverage</code> and <code>stats</code>:</p>

<pre><code class="language-bash">rm -rf [cs]*
</code></pre>

<h1 id="toc_6">check file encoding</h1>

<pre><code class="language-bash">file -I filename
</code></pre>

<p>The output is in format of: <code>/Path/To/Filename: fileformat/filetype; charset=encoding</code>. For example:  </p>

<pre><code class="language-bash">file -I Base.pig
Base.pig: application/octet-stream; charset=binary
</code></pre>

<h1 id="toc_7">du without recursion</h1>

<p>only want to see the whole folder&#39;s disk usage, not the details of its sub-folders.  </p>

<pre><code class="language-bash">du -s -h *
</code></pre>

<p><code>-s</code>: no recursion<br/>
<code>-h</code>: human readable</p>

<h1 id="toc_8">checking running job and argument</h1>

<p>the following commands works well:</p>

<pre><code class="language-bash">top -U [username]
ps -fu [username] # My most used one
ps -efl | grep &lt;username&gt; # get all the arguments
ps -efl | grep &lt;command name&gt;  

ps -efl | egrep &#39;\s+yu\s+&#39;
</code></pre>

<p>This is another good example, used to check <code>mongo</code> instance running in the system:</p>

<pre><code class="language-bash">ps -ef | grep mongod | grep -v grep | wc -l | tr -d &#39; &#39;
</code></pre>

<p>Step-by-Step:</p>

<ul>
<li><p>The <strong><code>ps -ef | grep mongod</code></strong> part return all the running processes, that have any relation to the supplied string, i.e. <code>mongod</code>, e.g. have the string in the executable path, have the string in the username, etc.</p></li>
<li><p>When you run the previous command, the <code>grep mongod</code> also becomes a process containing the string <code>mongod</code> in the <code>COMMAND</code> column of <code>ps</code> output, so it will also appear in the output. For that reason you need to eliminate it by piping <strong><code>grep -v grep</code></strong>, which filters all the lines from the input that contain the string <code>grep</code>.</p></li>
<li><p>So now you have all possible lines that contain string <code>mongod</code> and are not the instances of <code>grep</code>. What to do? Count them, and do that with <strong><code>wc -l</code></strong>.</p></li>
<li><p><code>wc -l</code> output contains additional formatting, i.e. spaces, so just for the sake of the beauty, run <strong><code>tr -d &#39; &#39;</code></strong> to remove the redundant spaces.</p></li>
</ul>

<p>As a result you will get a single number, representing the number of processes you <code>grep</code>&#39;ed for.</p>

<h1 id="toc_9">check the running job with complete arguments</h1>

<ol>
<li><code>ps -fu username</code> \(\rightarrow\) get the pid for the running job that would like to find the arguments.</li>
<li><code>ps -fu username | grep #pid</code> using the pid obtained in last step to get the complete arguments in nice format</li>
</ol>

<h1 id="toc_10">grep find negate</h1>

<p>negative matching, i.e. match lines that do not contain pattern</p>

<pre><code class="language-bash">grep -v [pattern] 
# -v, --invert-match select non-matching lines
</code></pre>

<h1 id="toc_11">grep find text in all files under a folder and all of its subfolders</h1>

<pre><code class="language-bash">grep -r -l -F -n -i -I &quot;string&quot; /path
</code></pre>

<p>in which:<br/>
<code>-r</code> is for recursive<br/>
<code>-l</code> is for showing the file name only (stop reading the file as soon as the string is find)<br/>
<code>-F</code> is searching for literal &quot;fixed string&quot;, not regexp<br/>
<code>-n</code> is printing the line number<br/>
<code>-i</code> is case-insensitive<br/>
<code>-I</code> is ignore binary files<br/>
Also there are:<br/>
<code>--exclude-dir=dir</code> is useful for excluding directories like .svn and .git</p>

<h1 id="toc_12">Keep the REPL across sessions</h1>

<p>This is very useful when running a Spark REPL on a remote ssh cluster, after logging out and close the terminal, next time connect and logging in the ssh cluster, we can pick up the same REPL session!</p>

<pre><code class="language-bash"># before running the REPL
screen
# start the REPL
/home/xiaogu/repo/spark-1.6.1-bin-hadoop2.4/bin/spark-shell --queue datascience

# Do not leave the REPL but leave the screen
Ctrl-a d

# leave the ssh session

# re-log in the ssh session
screen -r
</code></pre>

<p><a href="http://www.tecmint.com/screen-command-examples-to-manage-linux-terminals/">Detailed post for the <code>screen</code> command</a></p>

<p><font color='salmon'><strong>Important Note</strong></font></p>

<p>After get into the screen, need source the <code>~/.bash_profile</code>, <code>~/.bashrc</code> to get the regular bash setting into effective. Also for iTerm2, need set the mouse to scroll up and down <a href="http://stackoverflow.com/questions/36594420/how-can-i-turn-off-scrolling-the-history-in-iterm2">Mouse setting for iTerm2</a> </p>

<h1 id="toc_13">Schedule Auto jobs on Mac using crontab</h1>

<p><a href="https://ole.michelsen.dk/blog/schedule-jobs-with-crontab-on-mac-osx.html">Schedule jobs with crontab on Mac OS X</a></p>

<p>This can be used to build and update personal stock repo.</p>

<h1 id="toc_14">Hadoop Yarn get the Aggregated Log for a job</h1>

<p>(Thanks Mingyang)</p>

<pre><code class="language-bash">yarn logs -applicationId application_1478897002704_67148 -appOwner yu  
</code></pre>

<h1 id="toc_15">History search</h1>

<p>I use very frequently <code>Ctrl-r</code> to get the command. However, not in the most efficient way. It should be:</p>

<ol>
<li><code>Ctrl-r</code></li>
<li>type in the start of the command</li>
<li>repeat pressing <code>Ctrl-r</code> to do the reverse search</li>
<li>use <code>Ctrl-s</code> to do the forward search with the partially typed in command</li>
</ol>

<p>In order to enable <code>Ctrl-s</code>, <a href="http://stackoverflow.com/a/36331088/4229125">the following setting need be configured in <code>~/.bashrc</code></a></p>

<pre><code class="language-bash"># Y.G. to enable Ctrl-S for the reverse command history search
stty -ixon
</code></pre>

<h1 id="toc_16">check what pid is listening to the port</h1>

<p><a href="http://unix.stackexchange.com/a/106572">from this post, and works for mac</a></p>

<pre><code class="language-bash">lsof -i :4444 # I used this to check whether my Selenium server has been shut down
</code></pre>

<h1 id="toc_17">change mode for folder and file recursively</h1>

<p>Change folder mode to 755 and change file mode to 644, and change all executable files to 755. The base model here is <code>pig</code></p>

<pre><code class="language-bash"># change all folders to 755, which is needed for files to be 755
find ./pig -type d -exec chmod 755 {} \;
# change all files to be 644
find ./pig -type f -exec chmod 644 {} \;
# change all executable files to be 755
find . -name &quot;*.sh&quot; f -exec chmod 755 {} \;
</code></pre>

<h1 id="toc_18">input absolute path in mac os finder</h1>

<p>Especially useful for hidden folders<br/>
<code>CMD + SHIFT + G</code>: its name is <code>Go to the folder</code></p>

<p><a href="http://www.alexandre-gomes.com/?p=376">reference</a></p>

<h1 id="toc_19">split file by regex using <code>csplit</code></h1>

<p>Note for mac OSX, the need install the GNU version of <code>csplit</code>:</p>

<pre><code class="language-bash">brew install coreutils
</code></pre>

<p>then call <code>gcsplit</code> instead of <code>csplit</code>. <a href="https://stackoverflow.com/a/4323899/4229125">Reference</a></p>

<p><a href="https://xaizek.github.io/2014-07-23/splitting-text-files-by-pattern/">This post</a> is a good tutorial of using this tool.</p>

<p>To my simply use case of splitting the file by <code>=====</code>:</p>

<pre><code class="language-bash"># temp_diff is the file I need split on
gcsplit temp_diff /=====/ {*}
</code></pre>

<hr/>

<ul>
<li>
<a href="#toc_0">Using curly braces in Bash substitution</a>
</li>
<li>
<a href="#toc_1">nohup to a different file</a>
</li>
<li>
<a href="#toc_2">Append multiple lines to a file</a>
</li>
<li>
<a href="#toc_3">copy from bash shell to clipboard</a>
</li>
<li>
<a href="#toc_4">Hadoop hdfs get all except one folder (file)</a>
</li>
<li>
<a href="#toc_5">Hadoop hdfs / bash get matched folders</a>
</li>
<li>
<a href="#toc_6">check file encoding</a>
</li>
<li>
<a href="#toc_7">du without recursion</a>
</li>
<li>
<a href="#toc_8">checking running job and argument</a>
</li>
<li>
<a href="#toc_9">check the running job with complete arguments</a>
</li>
<li>
<a href="#toc_10">grep find negate</a>
</li>
<li>
<a href="#toc_11">grep find text in all files under a folder and all of its subfolders</a>
</li>
<li>
<a href="#toc_12">Keep the REPL across sessions</a>
</li>
<li>
<a href="#toc_13">Schedule Auto jobs on Mac using crontab</a>
</li>
<li>
<a href="#toc_14">Hadoop Yarn get the Aggregated Log for a job</a>
</li>
<li>
<a href="#toc_15">History search</a>
</li>
<li>
<a href="#toc_16">check what pid is listening to the port</a>
</li>
<li>
<a href="#toc_17">change mode for folder and file recursively</a>
</li>
<li>
<a href="#toc_18">input absolute path in mac os finder</a>
</li>
<li>
<a href="#toc_19">split file by regex using <code>csplit</code></a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[bayesian statistics]]></title>
    <link href="www.echo-ohce.com/14742423250523.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250523.html</id>
    <content type="html"><![CDATA[
<p>This documents my personal understandings about bayesian, keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Likelihood function is not a pdf</a>
</li>
<li>
<a href="#toc_1">MLE maximum Likelihood Estimation</a>
</li>
<li>
<a href="#toc_2">Gamma distribution in <code>scipy</code></a>
</li>
<li>
<a href="#toc_3">Exponential distribution</a>
</li>
<li>
<a href="#toc_4">Normal distribution</a>
<ul>
<li>
<a href="#toc_5">Known std of $sigma<sup>2$</sup></a>
</li>
<li>
<a href="#toc_6">Unknown std of $sigma<sup>2$</sup></a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">pdf, pmf, cdf, ppf explained</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Likelihood function is not a pdf</h1>

<p>(We are here only talking about continuous distribution case since I used pdf here.)<br/><br/>
Let the observation as \(O\) and the parameters as \(\theta\), the probability of seeing \(O\) given \(\theta\) is \(p(O|\theta)\). This is a pdf. It is a function of \(O\), and \(\int_{O}{p(O|\theta)\ dO}=1\), which means integrate through all possible observation \(O\), the sum of probability should be 1.<br/><br/>
The likelihood is the other way around as: given the observation \(O\) that we have seen, the likelihood that the parameter is value \(\theta\).<br/>
\[L(O|\theta)=p(O|\theta)\]<br/>
If we do an integration over all possible \(\theta\), \(\int_{\theta}{L(O|\theta)\ d\theta}\neq1\)<br/><br/>
<a href="http://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability">post on stackoverflow about the difference between likelihood and probability</a><br/>
<a href="http://stats.stackexchange.com/questions/31238/what-is-the-reason-that-a-likelihood-function-is-not-a-pdf">post on stackoverflow about likelihood and pdf</a>  </p>

<h1 id="toc_1">MLE maximum Likelihood Estimation</h1>

<p><a href="https://onlinecourses.science.psu.edu/stat504/node/28">Here</a> is a clear explained MLE course page. It gives Binomial distribution example and Poisson example</p>

<h1 id="toc_2">Gamma distribution in <code>scipy</code></h1>

<p><a href="https://en.wikipedia.org/wiki/Gamma_distribution">Wiki page</a></p>

<p>There are different ways to specify the gamma distribution, which all use two shape parameters:</p>

<ol>
<li><p>\(\alpha\) and \(\beta\) (\(\beta\) is called <code>rate</code>)<br/>
\[\Gamma(\alpha, \beta): pdf \rightarrow f(x; \alpha,\beta)=\frac{\beta^{\alpha}x^{\alpha-1}e^{-x\beta}}{\Gamma(\alpha)}\]<br/>
mean: \(\frac{\alpha}{\beta}\)<br/>
std: \(\frac{\sqrt{\alpha}}{\beta}\)</p></li>
<li><p>\(k\) and or \(\theta\) <br/>
\(\alpha=k\)<br/>
\(\theta=\frac{1}{\beta}\), and it is called <code>scale</code>.<br/>
\[\Gamma(k, \theta): pdf \rightarrow f(x; k,\theta)=\frac{1}{\theta}\frac{\frac{x}{\theta}^{k-1}e^{-\frac{x}{\theta}}}{\Gamma(k)}\]</p></li>
</ol>

<p>In <code>scipy.stats.gamma</code>, it uses the 2nd form, and the shape parameter <code>a</code> is \(k\), the <code>scale</code> is \(\theta\), for default value of <code>scale=1</code>, the pdf reduce to:<br/>
\[\Gamma(a, 1): pdf \rightarrow f(x; a,1)=\frac{x^{a-1}e^{-x}}{\Gamma(a)}\]<br/>
as shown on the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html#scipy.stats.gamma">scipy page</a></p>

<h1 id="toc_3">Exponential distribution</h1>

<p>like waiting for bus, earthquake, etc.  </p>

<p>We know it is exponential distribution: \( pdf(x) = \lambda e^{-\lambda x}\)<br/>
and we observer \(n\) observations: \(\tilde X ={x_1, x_2, ... x_n}\)<br/>
Now we want to get the estimate of \(\lambda\) (because its mean, i.e. expected value is \( \frac{1}{\lambda}\)).</p>

<ul>
<li><p>Likelihood function<br/>
\[ L(\tilde X | \lambda) = \Pi_{i=1}^{n}\lambda e^{-\lambda x_i} = \lambda^{n}e^{-\lambda \Sigma_{i=1}^{n}x_i}\] </p></li>
<li><p>Conjugate prior for \(\lambda\): </p>

<ul>
<li>Prior: \(pdf_{prior}(\lambda) = \Gamma(\alpha, \beta)\)</li>
<li>\(\alpha\) in the exponential-gamma model means the sample size (which is different than the poisson-gamma model, in which, \(\beta\) means the sample size)</li>
<li>mean and std are still the same for gamma distribution as before.</li>
<li>Posterior: \(pdf_{posterior}(\lambda) = \Gamma(\alpha+num_{newSample},  \beta+\Sigma(newSample))\)$</li>
</ul></li>
<li><p>If we have obtained the posterior and want to use the posterior to get the possibility density function pdf of random variable \(X\):<br/>
\[pdf_{posterior}(x) = \int pdf(x)* pdf_{posterior}(\lambda) d\lambda\]</p></li>
</ul>

<h1 id="toc_4">Normal distribution</h1>

<p>It mostly come from Central Limit Theorem.</p>

<h2 id="toc_5">Known std of \(\sigma^2\)</h2>

<p>We know it is normal distribution: \(pdf(x)=N(x|\mu,\sigma_0^2)\)<br/>
and we observer \(n\) observations: \(\tilde X ={x_1, x_2, ... x_n}\)<br/>
Now we want to get the estimate of \(\mu\)</p>

<ul>
<li><p>Likelihood function<br/>
\[ L(\tilde X | \mu ) = \Pi_{i=1}^{n}N(x_i|\mu,\sigma_0^2)\]</p></li>
<li><p>Conjugate prior for \(\mu\):</p>

<ul>
<li>Prior: \(pdf_{prior}(\mu)=N(\mu|m_0, s_0^2)\)</li>
<li>effective sample size of the prior: \(\frac{\sigma_0^2}{s_0^2}\)</li>
<li>Posterior: \(pdf_{posterior}(\mu)=N(\mu|(\frac{n}{n+\frac{\sigma_0^2}{s_0^2}}\bar x + \frac{\frac{\sigma_0^2}{s_0^2}}{n+\frac{\sigma_0^2}{s_0^2}}m_0 ), (\frac{1}{\frac{n}{\sigma_0^2}+\frac{1}{s_0^2}}))\)</li>
</ul></li>
<li><p>If we have obtained the posterior and want to use the posterior to get the possibility density function pdf of random variable \(X\):<br/>
\[pdf_{posterior}(x) = \int pdf(x)* pdf_{posterior}(\mu) d\mu\]<br/>
<strong>A quick way of calculation:</strong><br/>
\[\int N(x|\mu,\sigma_0^2)* N(\mu|m_0, s_0^2) d\mu=N(x|m_0, (s_0^2+\sigma_0^2))\]</p></li>
</ul>

<h2 id="toc_6">Unknown std of \(\sigma^2\)</h2>

<h1 id="toc_7">pdf, pmf, cdf, ppf explained</h1>

<ul>
<li>continuous distribution does not have point probabilities. pdf is the density function for a continuous distribution, and it is not defined for discrete distribution.
\[ P(x_1&lt;=X &lt;= x_2) = \int_{x_1}^{x_2} pdf(x)dx\]</li>
<li>pmf is the point probabilities of discrete distribution, and it is not defined for continuous distribution.
\[ P(X=x_1) = pmf(x_1)\]</li>
<li>cdf gives the accumulated probabilities, and it is defined for both the continuous and discrete distributions
\[ P(X&lt;=x_1) = cdf(x_1)\]</li>
<li>ppf is the inverse of cdf, and is mostly used to calculate the confidence interval. The result of ppf is <strong>not</strong> probability but the actual value of the random variable \(X\).
\[ P(X&lt;=ppf(\beta)) = \beta\]</li>
</ul>

<hr/>

<ul>
<li>
<a href="#toc_0">Likelihood function is not a pdf</a>
</li>
<li>
<a href="#toc_1">MLE maximum Likelihood Estimation</a>
</li>
<li>
<a href="#toc_2">Gamma distribution in <code>scipy</code></a>
</li>
<li>
<a href="#toc_3">Exponential distribution</a>
</li>
<li>
<a href="#toc_4">Normal distribution</a>
<ul>
<li>
<a href="#toc_5">Known std of $sigma<sup>2$</sup></a>
</li>
<li>
<a href="#toc_6">Unknown std of $sigma<sup>2$</sup></a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">pdf, pmf, cdf, ppf explained</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[giraph]]></title>
    <link href="www.echo-ohce.com/14742423250614.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250614.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Kill the background job</a>
</li>
<li>
<a href="#toc_1">giraph Unit testing</a>
</li>
<li>
<a href="#toc_2">giraph edge value not Mutable in place</a>
</li>
<li>
<a href="#toc_3">giraph shell runner / command line</a>
</li>
<li>
<a href="#toc_4">Unsolved problem with the input string split in InputFormat class</a>
</li>
<li>
<a href="#toc_5">giraph log file memory info free/total/max</a>
</li>
<li>
<a href="#toc_6">Giraph Edge and Message memory optimization</a>
<ul>
<li>
<a href="#toc_7">Edge memory management</a>
</li>
<li>
<a href="#toc_8">Message memory management</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">Use MessageStore</a>
</li>
<li>
<a href="#toc_10">The memory cost for java classes</a>
</li>
<li>
<a href="#toc_11">Mutate Graph</a>
<ul>
<li>
<a href="#toc_12">When there is a conflict for the mutate request</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">The timeout parameter for waiting resources</a>
</li>
<li>
<a href="#toc_14">duplicated job tracker setting is a must</a>
</li>
<li>
<a href="#toc_15">Use both Edge Input and Vertex Value Input</a>
</li>
<li>
<a href="#toc_16">Output during computation</a>
</li>
<li>
<a href="#toc_17">A lot of supersteps cause counters.LimitExceededException</a>
</li>
<li>
<a href="#toc_18">giraph jar Hadoop building dependency</a>
</li>
<li>
<a href="#toc_19">CPU bounded computation</a>
</li>
<li>
<a href="#toc_20">Class implements Writable needs empty constructor</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Kill the background job</h1>

<p>logging on sc2-hive1, run the following command to check the running job on background. (It works for both oozie and normal hadoop jobs)  </p>

<pre><code class="language-bash">ps -u yu
hadoop job -list | grep yu
</code></pre>

<p>When using the <code>ps</code> command and what to see the detailed command argument, get the <code>pid#</code> and run:  </p>

<pre><code class="language-bash">ps --pid pid#
</code></pre>

<p>Hadoop YARN command equivalent to <code>hadoop job -list</code>:  </p>

<pre><code class="language-bash">yarn application -appStates RUNNING -list
</code></pre>

<h1 id="toc_1">giraph Unit testing</h1>

<ul>
<li>When use <code>InternalVertexRunner</code>, comment out all giraph/hadoop counter logs</li>
<li>Most of the case, the unit test fails silently. Adding some <code>sout</code> maybe the only way to debug</li>
<li>the testing string input delimiter should always be <code>space</code> even in reality we can use all kinds of delimiter like <code>\t</code> etc.</li>
</ul>

<p><strong>wrong</strong></p>

<pre><code class="language-java">// tab &#39;\t&#39; delimited
String[] edges = new String[] { &quot;0  1&quot;, &quot;0  3&quot;, &quot;1  0&quot;, &quot;1  3&quot;, &quot;1  2&quot;};
</code></pre>

<p><strong>correct</strong></p>

<pre><code class="language-java">// space delimited
String[] edges = new String[] { &quot;0 1&quot;, &quot;0 3&quot;, &quot;1 0&quot;, &quot;1 3&quot;, &quot;1 2&quot;};
</code></pre>

<ul>
<li><p>The <code>InternalVertexRunner.run(conf, graph)</code> that we call in the unit testing, by default assumes the graph is <code>VertexInputData</code>. If instead we are using <code>EdgeInputData</code>, we should call <code>InternalVertexRunner.run(conf, null, graph)</code>. The complete signature is <code>public static Iterable&lt;String&gt; run(<br/>
  GiraphConfiguration conf,<br/>
  String[] vertexInputData,<br/>
  String[] edgeInputData)</code></p></li>
<li><p>include the follow in <code>pom.xml</code> in order to use the <code>MockUtils</code> in unit testing</p></li>
</ul>

<pre><code class="language-xml">&lt;!-- https://mvnrepository.com/artifact/org.apache.giraph/giraph-examples --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.giraph&lt;/groupId&gt;
    &lt;artifactId&gt;giraph-examples&lt;/artifactId&gt;
    &lt;version&gt;1.1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<ul>
<li>The sequence of adding edge is important: Must make the <code>env</code> first before adding <code>edges</code>. Like following:</li>
</ul>

<pre><code class="language-java">Vertex&lt;LongWritable, FDCVertexValue, FDCEdgeValue&gt; v6 = new DefaultVertex&lt;LongWritable, FDCVertexValue, FDCEdgeValue&gt;();
FDCVertexValue v6Value = new FDCVertexValue();
FDCClusterVector v6Cluster = new FDCClusterVector((short) 1, (short) 0, 0, 1, 0.0f, 0.6f);
v6Value.setClusterVector(v6Cluster);
v6Value.setClusterId(6L);
v6Value.setPreClusterId(0);
LongWritable v6Id = new LongWritable(6L);

WorkerContext workerContext = new FDCWorkerContext();

FDCComputeSS2 computation = new FDCComputeSS2();
FDCMockUtils.MockedEnvironment&lt;LongWritable, FDCVertexValue, FDCEdgeValue, FDCMessageSS2, FDCMessageSS3&gt; env = FDCMockUtils.prepareVertexAndComputation(
      v6, v6Id, v6Value, false, computation, 2L, workerContext);


FDCEdgeValue edgeValueToV7 = new FDCEdgeValue(0.6f, 7L);
LongWritable v7Id = new LongWritable(7L);
Edge&lt;LongWritable, FDCEdgeValue&gt; edgeToV7 = EdgeFactory.create(v7Id, edgeValueToV7);

// This addEdge must be later than creating env
v6.addEdge(edgeToV7);
</code></pre>

<ul>
<li>In the Unit testing <code>setUp()</code>, the <code>computation</code> instance need be separated for each environment <code>env</code>. Otherwise, the environment will be messed up.</li>
</ul>

<pre><code class="language-java">computationV6 = new FDCComputeSS2();
v6 = new DefaultVertex&lt;LongWritable, FDCVertexValue, FDCEdgeValue&gt;();
...
envV6 = FDCMockUtils.prepareVertexAndComputation(v6, v6Id, v6Value, false, computationV6, superStep, workerContext);


computationV4 = new FDCComputeSS2();
v4 = new DefaultVertex&lt;LongWritable, FDCVertexValue, FDCEdgeValue&gt;();
...
envV4 = FDCMockUtils.prepareVertexAndComputation(v4, v4Id, v4Value, false, computationV4, superStep, workerContext);

</code></pre>

<h1 id="toc_2">giraph edge value not Mutable in place</h1>

<p>edges are just a reference, it can not be changed in place.<br/>
After mutate the edge value, use <code>vertex.setEdgeValue(targetVertexId, edgeValue)</code> to change it.<br/>
<a href="https://giraph.apache.org/apidocs/org/apache/giraph/graph/Vertex.html#setEdgeValue-I-E-">Official Document</a></p>

<h1 id="toc_3">giraph shell runner / command line</h1>

<p>This is one file for running giraph with MasterCompute, Aggregator, etc.  </p>

<pre><code class="language-bash">WORKER=177
MEM=5120
HEAP=4352
#zoo keeper
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

hadoop jar dpp-giraph-0.0.1-with-giraph-core.jar \
        com.adsymp.dpp.giraph.lp.LPRunner \
        -Dmapred.job.queue.name=datascience \
        -Dmapreduce.map.memory.mb=$MEM \
        -Dmapreduce.task.timeout=1800000 \
        -Dmapred.task.timeout=1800000 \
        -Dmapreduce.map.java.opts=-Xmx${HEAP}m \
        -Dgiraph.zkList=$ZKLIST \
        com.adsymp.dpp.giraph.lp.WeightedLPComputation \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        -mc com.adsymp.dpp.giraph.lp.ClusterHistogramMC \
        -aw org.apache.giraph.aggregators.TextAggregatorWriter \
        -ca giraph.textAggregatorWriter.frequency=1 \
        -vif com.adsymp.dpp.giraph.lp.LongScoredValueFloatAdjacencyListVertexInputFormat \
        -vip $ADJ \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op $OUT \
        -w $WORKER -ca giraph.SplitMasterWorker=true \
        -ca giraph.zkSessionMsecTimeout=360000 \
        -ca giraph.zkOpsMaxAttempts=20 \
        -ca giraph.zkOpsRetryWaitMsecs=10000 \
        -ca mapred.job.tracker=sc2-rm1:8088 \
        -ca mapreduce.job.tracker=sc2-rm1:8088 \
        -ca com.adsymp.giraph.max.cd=8 \
        -ca giraph.waitingRequestMsecs=300000 \
        -ca com.adsymp.giraph.max.cc=100 \
        -ca com.adsymp.giraph.max.dd=8 \
        -ca com.adsymp.giraph.max.degree=80 \
        -ca com.adsymp.giraph.ignore.dev.types=false \
        -ca com.adsymp.giraph.weight.threshold=0.4 \
        -ca giraph.jobRetryCheckerClass=com.adsymp.dpp.giraph.RetryThriceChecker \
        -ca mapred.job.queue.name=datascience \
        -ca mapred.output.compress=true \
        -ca mapred.task.timeout=1800004 \
        -ca mapreduce.map.memory.mb=$MEM \
        -ca mapreduce.map.java.opts=-Xmx${HEAP}m
</code></pre>

<p>Couple important points:<br/>
1. the line breaker <code>\</code>: must have one space before it and <strong>no</strong> space after it, and every broken line must have it.<br/>
2. Pay special attention to the sequence of the arguments, some of them matters.<br/>
3. The zookeeper list after the line of the Computation class is a must!  </p>

<p>This one file is for running giraph with jython:  </p>

<pre><code class="language-bash">hdfs dfs -rm -r giraph-jython/output
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

yarn jar giraph-jython-1.0-SNAPSHOT-jar-with-dependencies.jar \
        com.adsymp.dpp.giraph.jython.GiraphJythonRunner \
        -Dmapred.job.queue.name=datascience \
        -Dgiraph.zkList=$ZKLIST \
        graph-distance.py \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        --jythonClass GraphDistance \
        --typesHolder com.adsymp.dpp.giraph.jython.JythonTypes \
        -eif org.apache.giraph.io.formats.IntNullTextEdgeInputFormat \
        -eip giraph-jython/input/tiny_graph \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op giraph-jython/output \
        -w 1 \
        -ca giraph.SplitMasterWorker=false
</code></pre>

<h1 id="toc_4">Unsolved problem with the input string split in InputFormat class</h1>

<p>My input file is delimited by <code>^A</code> (i.e.<code>\u0001</code>). Tried to change the <code>LongNullTextEdgeInputFormat.LongNullTextEdgeReader#preprocessLine</code> for splitting by <code>\u0001</code>. Failed.</p>

<p>Temporary workaround is wrote another pig script to convert the input file schema.</p>

<h1 id="toc_5">giraph log file memory info free/total/max</h1>

<p>In the log file of each individual worker, we see the following lines about the real time memory usage: </p>

<blockquote>
<p>2016-10-21 21:35:44,615 INFO [compute-0] org.apache.giraph.graph.ComputeCallable: call: Completed 18 partitions, 159 remaining Memory (free/total/max) = 6154.37M / 16748.50M / 27079.00M.  </p>
</blockquote>

<p>This log is generated by the class: <code>org.apache.giraph.utils.MemoryUtils</code> <a href="https://github.com/apache/giraph/blob/release-1.1/giraph-core/src/main/java/org/apache/giraph/utils/MemoryUtils.java">source code here</a></p>

<p>this class calls <code>java.lang.Runtime</code> class and use the method <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#freeMemory()"><code>freeMemory()</code></a>, <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#maxMemory()"><code>maxMemory()</code></a>, <a href="http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#totalMemory()"><code>totalMemory()</code></a> to get the real time memory usage stats</p>

<ul>
<li>free<br/>
the amount of free memory in the Java Virtual Machine. Calling the <code>gc</code> method may result in increasing the value returned by freeMemory. <strong>an approximation to the total amount of memory currently available for future allocated objects</strong></li>
<li>max<br/>
the maximum amount of memory that the Java virtual machine will attempt to use. If there is no inherent limit then the value Long.MAX_VALUE will be returned. <strong>the maximum amount of memory that the virtual machine will attempt to use</strong> </li>
<li>total<br/>
the total amount of memory in the Java virtual machine. The value returned by this method may vary over time, depending on the host environment. (Note that the amount of memory required to hold an object of any given type may be implementation-dependent.) <strong>the total amount of memory currently available for current and future objects</strong></li>
</ul>

<h1 id="toc_6">Giraph Edge and Message memory optimization</h1>

<p>From the book chapter 11:</p>

<blockquote>
<p>Giraph does some memory management on its own. For instance, it stores some of the data, like edges, messages and vertex values, serialized inside of byte arrays that it allocates at the beginning of the computation. Still, it offers a pure Java object-oriented API. To achieve this abstraction, Giraph keeps a number of objects around, internally called representative objects, which it reinitializes with data coming from these binary arrays before passing them to the user, and which it serializes back to the arrays after the user is done with them. This mechanism is used, for example, for the Iterable containing the messages passed to the compute function, or the default implementation of the OutEdges interface where edges are stored for each vertex.</p>
</blockquote>

<p>With this said, it is different between edges and messages.</p>

<h2 id="toc_7">Edge memory management</h2>

<p>Edge in its full name should be <code>Edge&lt;I,E&gt;</code> in which <code>I</code> is the vertexID type (which is the vertexIDs that the edge is pointing to, i.e. <code>OutEdges</code>) and <code>E</code> is the edgeValue type. So we need have a data structure that builds the 1-1 corresponds between the pointing-to ID and the corresponding edgeValue.</p>

<p>The naive way of implementing it is just a <code>List&lt;I, E&gt;</code> or <code>Map&lt;I, E&gt;</code>,  the first one only provides iterator but is cheaper, and the second one provides random access efficiently but is more expensive.</p>

<p>Actually, the two different real implementations are: </p>

<ol>
<li>Provides only Iterable - <code>ByteArrayEdges</code>

<ol>
<li>Use a reusable <code>edge&lt;I,E&gt;</code> object for iterating all edges </li>
<li>The underlying data structure is byte array. Every time, it is deserialized (or serialized) to the byte array</li>
<li>It is already the most memory efficient implementation. The cons are: Not random access, CPU intensive<br/></li>
</ol></li>
<li>Provide random access - <code>OpenHashMapOutedges</code>

<ol>
<li>Can not use byte array anymore, because the edgeValue changes, not fix length of the byte array anymore.</li>
<li>For each vertex, all the edges are save in one Map, using pointing-to ID as the key, and the edgeValue as the Map value.</li>
<li>Use <code>fastutil</code> primitive map type.</li>
<li>Still using a reusable <code>edge&lt;I,E&gt;</code> object for iterating all edges.</li>
<li>Efficient <code>getEdgeValue</code> and <code>setEdgeValue</code> methods.</li>
</ol></li>
</ol>

<p><strong><font color='red'>Special Notes:</font></strong>  </p>

<ul>
<li>Can I use <code>ByteArrayEdges</code> but instructing the Map inside <code>compute</code> method for fast random access? It has pros and cons. 

<ul>
<li>Pros: the edges are saved in memory efficient byte arrays for all vertexes. Otherwise, for each vertex, it needs a map. Because in a giraph worker thread, the graph is processed partition after partition, and in a partition, the graph is processed vertex after vertex, so at a single time, for one worker thread, there will be only one map. </li>
<li>Cons: Stressed on CPU for creating and destroying a lot of maps. Also serialize and deserializes.<br/></li>
</ul></li>
<li>Hashmap actually is quite expensive w.r.t the memory usage, because of the load factor and avoid collision</li>
<li>When using the default <code>ByteArrayEdges</code>, and called <code>getEdgeValue</code> and <code>setEdgeValue</code> method, under the hood, for the <code>getEdgeValue</code>, it just scan through the immutable edges iterator, and for the  <code>setEdgeValue</code> it scan through the <code>MutableEdge</code> iterator.<br/></li>
</ul>

<blockquote>
<p>If the VertexEdges implementation has a specialized random-access method, we use that; otherwise, we scan the edges.</p>
</blockquote>

<h2 id="toc_8">Message memory management</h2>

<p>Different than the Edges, Message are always stored in a <strong>serialized format inside of byte arrays</strong>. We are using the same concept of re-usable iterator to pass the messages to the compute method.</p>

<p>Each giraph <strong>worker</strong> has one <strong>MessageStore</strong>, which is the message &#39;inbox&#39; for <strong>all</strong> the vertexes of this <strong>worker</strong>.</p>

<p>So, the message store, is organized by <strong>Partition index</strong> then further organized by <strong>Vertex index</strong>, essentially a nested map, with partitions as keys of the outer map, and vertex IDs as keys of the inner maps.</p>

<p>The default implementation of giraph uses <strong>original java HashMap</strong> and is not very efficient. There is some space of saving by using the <code>fastutil</code>. (comparing <code>long</code>: 8 bytes, and <code>Long</code>:24 bytes)</p>

<p>This optimization should be done always, because it only sacrifices generality of the partition ID and vertex ID type.</p>

<p><strong><font color='red'><a href="https://github.com/apache/giraph/blob/release-1.1/giraph-core/src/main/java/org/apache/giraph/comm/messages/primitives/long_id/LongByteArrayMessageStore.java">ALREADY HAVE A IMPLEMENTATION</a>: Just use <code>LongByteArrayMessageStore</code> </font></strong></p>

<h1 id="toc_9">Use MessageStore</h1>

<p><strong><font color=red>The book&#39;s content is outdated as of giraph v1.1</font></strong></p>

<p>According to the book, I must first write a class that implements <code>MessageStoreFactory</code> interface, and this factory generates the <code>MessageStore</code> implementation that I need.</p>

<p>Then specify the parameter of <code>giraph.messageStoreFactoryClass</code> to let giraph know.</p>

<p>But actually, the default giraph already implemented the <code>LongByteArrayMessageStore</code></p>

<ol>
<li><a href="http://giraph.apache.org/options.html">the default configuration</a> lists: 

<ol>
<li><code>messageEncodeAndStoreType</code> = <code>BYTEARRAY_PER_PARTITION</code></li>
<li><code>messageStoreFactoryClass</code> = <code>InMemoryMessageStoreFactory</code></li>
</ol></li>
<li>The <a href="https://github.com/apache/giraph/blob/release-1.1/giraph-core/src/main/java/org/apache/giraph/comm/messages/InMemoryMessageStoreFactory.java#L131">source code</a> of <code>InMemoryMessageStoreFactory</code> shows for vertexID <code>LongWritable</code>, <code>messageEncodeAndStoreType</code> = <code>BYTEARRAY_PER_PARTITION</code>, <code>messageStore = new LongByteArrayMessageStore()</code></li>
</ol>

<h1 id="toc_10">The memory cost for java classes</h1>

<p>Remember the load factors for Hash*** stuff, which make it expensive.</p>

<p>A good series posts here:<br/>
<a href="http://java-performance.info/overview-of-memory-saving-techniques-java/">An overview of memory saving techniques in Java</a><br/>
<a href="http://java-performance.info/memory-consumption-of-java-data-types-1/">Memory consumption of popular Java data types – part 1</a><br/>
<a href="http://java-performance.info/memory-consumption-of-java-data-types-2/">Memory consumption of popular Java data types – part 2</a></p>

<h1 id="toc_11">Mutate Graph</h1>

<p>Three different mechanism of mutating a graph</p>

<p>It is in the book of chapter 8</p>

<h2 id="toc_12">When there is a conflict for the mutate request</h2>

<p>It is resolved by <code>VertexResolver</code></p>

<blockquote>
<p>The default vertex resolver performs the following operations: </p>

<ol>
<li>If there were any edge removal requests, first apply these removals. </li>
<li>If there was a request to remove the vertex, then remove it. This is achieved by setting the return Vertex object to null . </li>
<li>If there was a request to add the vertex, and it does not exist, then create the vertex. </li>
<li>If the vertex has messages sent to it, and it does not exist, then create the vertex. </li>
<li>If there was a request to add edges to the vertex, if the vertex does not exist, first create and then add the edges; otherwise, simply add the edges. </li>
</ol>

<p>The order of this list is important because it defines exactly the way that Giraph resolves any conflicts by default. This means that if, for instance, there is request to remove a vertex and at the same time a request to add it, then because the default resolver checks the vertex creation after it does the deletion, it ends up creating the vertex.</p>
</blockquote>

<p><code>-ca giraph.vertexResolverClass=MyVertexResolver</code></p>

<h1 id="toc_13">The timeout parameter for waiting resources</h1>

<pre><code class="language-bash">-Dgiraph.maxMasterSuperstepWaitMsecs=18700000 \
</code></pre>

<h1 id="toc_14">duplicated job tracker setting is a must</h1>

<p>In <code>oozie</code> or just run in command line, for certain version of giraph, we must specify both <code>mapred.job.tracker</code> and <code>mapreduce.job.tracker</code> parameters</p>

<p>Otherwise, there is be error message looks like below:</p>

<blockquote>
<p>java.lang.IllegalArgumentException: checkLocalJobRunnerConfiguration: When using LocalJobRunner, must have only one worker since only 1 task at a time!</p>
</blockquote>

<p>This bug actually is reported <a href="https://issues.apache.org/jira/browse/GIRAPH-1001">here</a></p>

<h1 id="toc_15">Use both Edge Input and Vertex Value Input</h1>

<p>EdgeInput to build edges, and vertex value input to attach vertex information to the vertexes.</p>

<p>From <a href="http://giraph.apache.org/io.html">Giraph official document</a> </p>

<blockquote>
<p>To summarize, VertexInputFormat is usually used by itself, whereas EdgeInputFormat may be used in combination with VertexValueInputFormat.</p>
</blockquote>

<p>The way of specifying <code>VertexValueInputFormat</code> in the command line parameter is exactly the same as using <code>VertexInputFormat</code></p>

<p>The schema of EdgeInput is: <code>VId1, VId2, EdgeValue</code> \(\rightarrow\) type info: <code>&lt;I, E&gt;</code><br/>
The schema of VertexValueInput is : <code>VId, VValue</code> \(\rightarrow\) type info: <code>&lt;I, V&gt;</code></p>

<p>See my latest push of <code>algo-109-v1</code> for example <code>VIF=&#39;com.adsymp.dpp.giraph.jython.inputformats.LongTextTextVertexValueInputFormat&#39;</code></p>

<h1 id="toc_16">Output during computation</h1>

<p>The main class is <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/GiraphTextOutputFormat.java">GiraphTextOutputFormat</a></p>

<ol>
<li>Set <code>giraph.doOutputDuringComputation</code> <a href="http://giraph.apache.org/options.html">doc</a></li>
<li>Change behavior of <code>ImmutableClassesGiraphConfiguration</code>

<ol>
<li>generate <code>SynchronizedSuperstepOutput</code> as <code>SuperstepOutput</code>, the <code>ImmutableClassesGiraphConfiguration</code> is passed to <code>SynchronizedSuperstepOutput</code> constructor <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/conf/ImmutableClassesGiraphConfiguration.java#L462">doc</a></li>
<li>Inside <code>SynchronizedSuperstepOutput</code>, the <code>ImmutableClassesGiraphConfiguration</code> also create <code>WrappedVertexOutputFormat</code>. <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/superstep_output/SynchronizedSuperstepOutput.java#L64">doc</a></li>
<li>The <code>WrappedVertexOutputFormat</code> wraps <code>VertexOutputFormat</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/conf/ImmutableClassesGiraphConfiguration.java#L386">doc</a></li>
<li>The <code>VertexOutputFormat</code> is created by <font color='salmon'><strong>User specified Vertex Output Format Class</strong></font> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/conf/ImmutableClassesGiraphConfiguration.java#L357">doc</a></li>
<li>The <code>WrappedVertexOutputFormat</code> uses the wrapped <code>VertexOutputFormat</code> class&#39;s method <code>createVertexWriter</code> <font color='salmon'><strong>This is the place we can hook up the logic of creating SS aware outputpaht</strong></font>to create the base <code>VertexWriter</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/internal/WrappedVertexOutputFormat.java#L68">doc</a>, and then wrap on this base <code>VertexWriter</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/internal/WrappedVertexOutputFormat.java#L71">doc</a></li>
</ol></li>
<li><code>VertexWriter</code> has following abstract method: 

<ol>
<li>itself has <code>initialize</code> and <code>close</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/VertexWriter.java">doc</a>. </li>
<li>It also inherit from interface <code>SimpleVertexWriter</code> with method <code>writeVertex</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/SimpleVertexWriter.java#L43">doc</a></li>
<li>It also inherit from concrete class <code>DefaultImmutableClassesGiraphConfigurable</code> with method <code>setConf</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/conf/DefaultImmutableClassesGiraphConfigurable.java#L39">doc</a></li>
</ol></li>
<li>Assume that we are using <code>org.apache.giraph.io.formats.IdWithValueTextOutputFormat</code> as the Vertex Output Format class. 

<ol>
<li>Its <code>createVertexWriter</code> methods gives <code>IdWithValueVertexWriter</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/IdWithValueTextOutputFormat.java#L54">doc</a></li>
<li>This Vertex Output Format class extends <code>TextVertexOutputFormat</code> and its writer extends <code>TextVertexWriterToEachLine</code>. </li>
</ol></li>
<li>The <code>TextVertexOutputFormat</code> extends <code>VertexOutputFormat</code> and its <code>TextVertexWriterToEachLine</code> extends <code>TextVertexWriter</code> <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/TextVertexOutputFormat.java#L148">doc</a>, which is also defined in the same class <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/TextVertexOutputFormat.java#L87">doc</a></li>
<li>The <code>TextVertexOutputFormat</code> delegate <code>GiraphTextOutputFormat</code> to get the <code>OutputCommitter</code>, which defines the output path in Hadoop <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/TextVertexOutputFormat.java#L49">doc</a></li>
<li>The real work of generate the output path inside <code>GiraphTextOutputFormat</code> is <a href="https://github.com/apache/giraph/blob/release-1.2/giraph-core/src/main/java/org/apache/giraph/io/formats/GiraphTextOutputFormat.java#L60">here</a></li>
</ol>

<p>DAMN! I wasted a lot of time. Actually, the output is just appended in the output folder. That&#39;s it.</p>

<h1 id="toc_17">A lot of supersteps cause counters.LimitExceededException</h1>

<p>Because by default, giraph use Hadoop counters to record giraph stats, like how many messages, how many vertexes, edges etc. in each superstep, if the superstep is very large, we will have error like this:</p>

<blockquote>
<p>.MasterThread: masterThread: Master algorithm failed with LimitExceededException<br/>
org.apache.hadoop.mapreduce.counters.LimitExceededException: Too many counters: 1025 max=1024</p>
</blockquote>

<p>The way of fixing it is to set <code>-ca giraph.useSuperstepCounters=false</code> in the runner to turn off the recording mechanism for every superstep.</p>

<h1 id="toc_18">giraph jar Hadoop building dependency</h1>

<p>When build the giraph jar, correct Hadoop version dependency is a must. For my case, the main cluster run at <code>Hadoop 2.5.0-cdh5.3.0</code> </p>

<p>When I use <code>pom.xml</code> like follow, it fails: (This pom was supposed to be used for a different client&#39;s Hadoop environment)  </p>

<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
  &lt;version&gt;2.6.4&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt;
  &lt;version&gt;2.6.4&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>And the error message is very confusing: </p>

<blockquote>
<p>2017-03-23 00:24:03,717 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://sc2prod:8020]<br/>
2017-03-23 00:24:03,718 INFO [main] org.apache.hadoop.service.AbstractService: Service JobHistoryEventHandler failed in state INITED; cause: java.lang.IllegalArgumentException: Wrong FS: hdfs://sc2prod:8020/user/yu/.staging/job_1489616796019_14222, expected: hdfs://sc2prod<br/>
java.lang.IllegalArgumentException: Wrong FS: hdfs://sc2prod:8020/user/yu/.staging/job_1489616796019_14222, expected: hdfs://sc2prod<br/>
    at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)<br/>
    at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:465)<br/>
    at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceInit(JobHistoryEventHandler.java:143)<br/>
    at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)</p>
</blockquote>

<p>My guess is that for the wrong hadoop dependency, the jar can not find the correct hadoop config path/file for the configurations. Then it just uses the default, which is not right.</p>

<p>After I changed the <code>pom.xml</code> to below, it works fine.</p>

<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
  &lt;version&gt;2.5.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt;
  &lt;version&gt;2.5.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
  &lt;version&gt;2.5.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p><strong>Note</strong> The newly added <code>hadoop-client</code> is a must, otherwise will have error like follow:</p>

<blockquote>
<p>2017-03-23 06:22:00,622 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster<br/>
java.lang.NoSuchMethodError: org.apache.hadoop.ipc.RPC.getServer(Ljava/lang/Class;Ljava/lang/Object;Ljava/lang/String;IIZLorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/security/token/SecretManager;)Lorg/apache/hadoop/ipc/RPC$Server;<br/>
    at org.apache.hadoop.mapred.TaskAttemptListenerImpl.startRpcServer(TaskAttemptListenerImpl.java:121)</p>
</blockquote>

<p>As noted in <a href="http://stackoverflow.com/questions/39574202/java-lang-nosuchmethoderror-org-apache-hadoop-ipc-rpc-getproxy-while-creating-h">this stackoverflow post</a>, the <code>hadoop-client</code> dependency need be added.</p>

<h1 id="toc_19">CPU bounded computation</h1>

<p>Adding parallelism for computation intense algorithm like <code>FDC</code> need set the following two options:</p>

<ol>
<li><code>-ca giraph.numComputeThreads=${giraphComputeThreads}</code> (by default it is 1) </li>
<li><code>-ca mapreduce.map.cpu.vcores=${giraphComputeThreads}</code></li>
</ol>

<p>Note: the second line can not be set inside <code>oozie</code> and must be set inside giraph arguments.</p>

<p>This does <strong>NOT</strong> work:</p>

<pre><code class="language-xml">&lt;property&gt;
    &lt;name&gt;oozie.launcher.mapreduce.map.cpu.vcores&lt;/name&gt;
    &lt;value&gt;${giraphComputeThreads}&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h1 id="toc_20">Class implements Writable needs empty constructor</h1>

<p>Because the serialization and deserialization will call the empty constructor (<code>non-parameterized</code>) constructor, we should always write one!</p>

<p>I made the same bug a lot of times...</p>

<hr/>

<ul>
<li>
<a href="#toc_0">Kill the background job</a>
</li>
<li>
<a href="#toc_1">giraph Unit testing</a>
</li>
<li>
<a href="#toc_2">giraph edge value not Mutable in place</a>
</li>
<li>
<a href="#toc_3">giraph shell runner / command line</a>
</li>
<li>
<a href="#toc_4">Unsolved problem with the input string split in InputFormat class</a>
</li>
<li>
<a href="#toc_5">giraph log file memory info free/total/max</a>
</li>
<li>
<a href="#toc_6">Giraph Edge and Message memory optimization</a>
<ul>
<li>
<a href="#toc_7">Edge memory management</a>
</li>
<li>
<a href="#toc_8">Message memory management</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">Use MessageStore</a>
</li>
<li>
<a href="#toc_10">The memory cost for java classes</a>
</li>
<li>
<a href="#toc_11">Mutate Graph</a>
<ul>
<li>
<a href="#toc_12">When there is a conflict for the mutate request</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">The timeout parameter for waiting resources</a>
</li>
<li>
<a href="#toc_14">duplicated job tracker setting is a must</a>
</li>
<li>
<a href="#toc_15">Use both Edge Input and Vertex Value Input</a>
</li>
<li>
<a href="#toc_16">Output during computation</a>
</li>
<li>
<a href="#toc_17">A lot of supersteps cause counters.LimitExceededException</a>
</li>
<li>
<a href="#toc_18">giraph jar Hadoop building dependency</a>
</li>
<li>
<a href="#toc_19">CPU bounded computation</a>
</li>
<li>
<a href="#toc_20">Class implements Writable needs empty constructor</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[giraph with jython]]></title>
    <link href="www.echo-ohce.com/14742423250588.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250588.html</id>
    <content type="html"><![CDATA[
<p>This post specifically documents the key point of using giraph with jython. Once everything is done, I will write another post for a tutorial with giraph + jython</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Hadoop version</a>
</li>
<li>
<a href="#toc_1">Use <code>jython-standalone</code> dependency not <code>jython</code></a>
</li>
<li>
<a href="#toc_2">Run from jython REPL</a>
</li>
<li>
<a href="#toc_3">Config intellij for Jython and Java</a>
</li>
<li>
<a href="#toc_4">Run giraph jython from Hadoop Yarn</a>
</li>
<li>
<a href="#toc_5">pitfall of <code>TypeHolder</code> and <code>TypesHolder</code></a>
</li>
<li>
<a href="#toc_6">python and java data type</a>
</li>
<li>
<a href="#toc_7">debug print statement inside python file</a>
</li>
<li>
<a href="#toc_8">Jython type to java pitfall</a>
</li>
<li>
<a href="#toc_9">import python package or jython package</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Hadoop version</h1>

<p>In order to use Jython, in the maven pom.xml setting up, do <font color='red'><strong>NOT</strong></font> include other hadoop dependencies, but use the below specific version of hadoop:  </p>

<pre><code class="language-xml">&lt;dependencies&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.apache.giraph&lt;/groupId&gt;
       &lt;artifactId&gt;giraph-core&lt;/artifactId&gt;
       &lt;version&gt;1.1.0-hadoop2&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
       &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
       &lt;version&gt;2.5.0&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.python&lt;/groupId&gt;
       &lt;artifactId&gt;jython-standalone&lt;/artifactId&gt;
       &lt;version&gt;2.7.0&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;junit&lt;/groupId&gt;
       &lt;artifactId&gt;junit&lt;/artifactId&gt;
       &lt;version&gt;4.11&lt;/version&gt;
       &lt;scope&gt;test&lt;/scope&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.mockito&lt;/groupId&gt;
       &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;
       &lt;scope&gt;test&lt;/scope&gt;
       &lt;version&gt;1.9.5&lt;/version&gt;
   &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>

<p>Otherwise, there will be error message like:</p>

<blockquote>
<p>Exception in thread &quot;main&quot; java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected`  </p>
</blockquote>

<p><a href="http://stackoverflow.com/questions/22630323/hadoop-java-lang-incompatibleclasschangeerror-found-interface-org-apache-hadoo">Because Hadoop 1.0.3: JobContext is a class, but in Hadoop 2.0.0: JobContext is an interface</a></p>

<p>Most important points in this pom are:</p>

<ul>
<li>use giraph version of <code>1.1.0-hadoop2</code> because nowadays we are all using hadoop 2.</li>
<li>use <code>jython-standalone</code> not <code>jython</code> (see below)</li>
</ul>

<h1 id="toc_1">Use <code>jython-standalone</code> dependency not <code>jython</code></h1>

<p>Otherwise it will show error message like:  </p>

<blockquote>
<p>Exception in thread &quot;main&quot; ImportError: Cannot import site module and its dependencies: No module named site`  </p>
</blockquote>

<p>This is because <code>jython-standalone-2.7.0.jar</code> has <code>Lib</code> folder but <code>jython-2.7.0.jar</code> does not have that folder.  </p>

<p>Another possible issue about jython with error message like:  </p>

<blockquote>
<p>java.lang.IncompatibleClassChangeError: Found class com.kenai.jffi.InvocationBuffer, but interface was expected  </p>
</blockquote>

<p>is also about the jar of jython. Using <a href="https://github.com/scijava/jython-shaded"><code>jython-shade</code></a> will solve this issue.   </p>

<h1 id="toc_2">Run from jython REPL</h1>

<ul>
<li>For installing the jython, remeber to add the following path to <code>.bashrc</code><br/></li>
</ul>

<pre><code class="language-bash">export PATH=/Users/yugan/jython2.7.0/bin:$PATH
</code></pre>

<ul>
<li>Need import the following two pathes, in order to get the java <code>import</code> work.<br/></li>
</ul>

<pre><code class="language-python">import sys
sys.path.append(&quot;/Users/yugan/.m2/repository/org/apache/giraph/giraph-core/1.1.0/giraph-core-1.1.0-hadoop_2_6_dependencies.jar&quot;)
sys.path.append(&quot;/Users/yugan/.m2/repository/org/apache/giraph/giraph-core/1.1.0/giraph-core-1.1.0.jar&quot;)
sys.path.append(&quot;/Users/yugan/.m2/repository/org/apache/hadoop/hadoop-core/2.5.0/hadoop-core-2.5.0.jar&quot;)

from org.apache.giraph.jython import JythonJob
</code></pre>

<h1 id="toc_3">Config intellij for Jython and Java</h1>

<ol>
<li>using <code>command + ;</code> to call out the project setting.</li>
<li>project SDK should be java (to recognize java packages etc)</li>
<li>In the <code>Facets</code>, add Python framework, and config <code>python interpreter</code> as <code>jython</code></li>
</ol>

<h1 id="toc_4">Run giraph jython from Hadoop Yarn</h1>

<p>Got a working version using my own <code>graph-distance.py</code> example.  </p>

<ol>
<li>python file can be put anywhere, but I put it under the <code>resources</code> folder at:<br/>
<code>/src/main/resources/graph-distance.py</code><br/>
In this python file, it builds the python class <code>GraphDistance</code> which subclasses giraph&#39;s <code>JythonComputation</code>. There is no way that we can give the type parameters (generics), but we are going to handle it in a different way later.<br/></li>
<li>the graph input file can be anywhere, but I put it under the <code>resources</code> folder too.<br/>
It is put on hdfs.<br/></li>
<li>To specify the type parameters of <code>&lt;I, V, E, M, M&gt;</code>, I built the <code>JythonTypes</code> class which implements the <code>TypesHolder</code> interface.</li>
<li>I also created the corresponding <code>I V E M</code> classes if it is necessary.</li>
<li>Build the jar file which need include the custom build type classes, and also the original giraph classes. I used the <code>maven-assembly-plugin</code> to build the jar with all dependencies.</li>
<li>In the working folder on the cluster, we have the <u>python</u> file and the <u>jar</u> file (<code>giraph-jython-1.0-SNAPSHOT-jar-with-dependencies.jar</code>). Then run the shell script.</li>
</ol>

<p>The pom.xml file and the shell runner file:</p>

<pre><code class="language-xml">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;com.adsymp.dpp.giraph.jython&lt;/groupId&gt;
    &lt;artifactId&gt;giraph-jython&lt;/artifactId&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;giraph-jython&lt;/name&gt;

    &lt;parent&gt;
        &lt;groupId&gt;com.adsymp&lt;/groupId&gt;
        &lt;artifactId&gt;base-pom&lt;/artifactId&gt;
        &lt;version&gt;0.0.1&lt;/version&gt;
    &lt;/parent&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.giraph&lt;/groupId&gt;
            &lt;artifactId&gt;giraph-core&lt;/artifactId&gt;
            &lt;version&gt;1.1.0-hadoop2&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;2.5.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.python&lt;/groupId&gt;
            &lt;artifactId&gt;jython-standalone&lt;/artifactId&gt;
            &lt;version&gt;2.7.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.11&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.mockito&lt;/groupId&gt;
            &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
            &lt;version&gt;1.9.5&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;descriptorRefs&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre>

<p>Build the jar:</p>

<pre><code class="language-bash">mvn clean package assembly:single -DskipTests
</code></pre>

<p>run the shell script:</p>

<pre><code class="language-bash">hdfs dfs -rm -r giraph-jython/output
ZKLIST=sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181

yarn jar giraph-jython-1.0-SNAPSHOT-jar-with-dependencies.jar \
        com.adsymp.dpp.giraph.jython.GiraphJythonRunner \
        -Dmapred.job.queue.name=datascience \
        -Dgiraph.zkList=$ZKLIST \
        graph-distance.py \
        sc2-hmr101.drawbrid.ge:2181,sc2-hmr102.drawbrid.ge:2181,sc2-hmr103.drawbrid.ge:2181,sc2-hmr104.drawbrid.ge:2181,sc2-hmr105.drawbrid.ge:2181 \
        --jythonClass GraphDistance \
        --typesHolder com.adsymp.dpp.giraph.jython.JythonTypes \
        -eif org.apache.giraph.io.formats.IntNullTextEdgeInputFormat \
        -eip giraph-jython/input/tiny_graph \
        -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
        -op giraph-jython/output \
        -w 1 \
        -ca giraph.SplitMasterWorker=false
</code></pre>

<h1 id="toc_5">pitfall of <code>TypeHolder</code> and <code>TypesHolder</code></h1>

<p>Be careful, the <code>TypesHolder</code> interface (which is what we need supply to the cmd) is different than the <code>TypeHolder</code>.</p>

<ul>
<li>The first one is at <code>org.apache.giraph.conf.TypesHolder</code> - Interface for classes that are parameterized by all of the Giraph types.<br/></li>
<li>The second one is an inner class at <code>org.apache.giraph.jython.JythonJob</code></li>
</ul>

<h1 id="toc_6">python and java data type</h1>

<p>Converting python data type to java data type has a lot of limitations.</p>

<p>For now, data come from and come into java are all in the type of array(). To use it in python, convert it to list using array.tolist() method.<br/>
To pass it to java, convert it to list then use array(list, <code>typecode</code>) <a href="http://www.jython.org/jythonbook/en/1.0/DataTypes.html#jython-specific-collections">Typecode</a> refers to Table 2-8</p>

<pre><code class="language-python">from org.python.modules.jarray import array

</code></pre>

<h1 id="toc_7">debug print statement inside python file</h1>

<p>Very tricky that the standard python way of print will not work in the python file.</p>

<p><strong>Wrong</strong>:  </p>

<pre><code class="language-python">print &#39;vertext_id: {0:d} received {1:d} messages&#39;.format(int(v_id), count)
</code></pre>

<p><strong>Correct</strong></p>

<pre><code class="language-python">print &#39;vertext_id: %d received %d messages&#39; % (int(v_id), count)
</code></pre>

<h1 id="toc_8">Jython type to java pitfall</h1>

<p>One subtle bug (very hard to debug) is the <code>Writable</code> type. When directly calling giraph&#39;s java method, such as <code>sendMessage(vertexID, MessageOut)</code>, the type of <code>vertexID</code> should be <code>LongWritable</code>, but inside jython, the default type is just <code>long</code>, and the framework will not automatically wrap it for you.</p>

<p><strong>Wrong</strong>  </p>

<pre><code class="language-python">self.sendMessage(id, outgoing_Message)
</code></pre>

<p><strong>Correct</strong>  </p>

<pre><code class="language-python">from org.apache.hadoop.io import LongWritable

self.sendMessage(LongWritable(id), outgoing_Message)
</code></pre>

<p>Why it is so hard to debug? Because there is <strong>NO</strong> error message! try add some print statement inside python file to check the type.</p>

<h1 id="toc_9">import python package or jython package</h1>

<p>At least for my project setting (intellij with jython facets), import from python does not work well.</p>

<p><strong>Work</strong>  </p>

<pre><code class="language-python">from org.python.modules.itertools import itertools

for id1, id2 in itertools.combinations(newSeenIds, 2):
</code></pre>

<p><strong>Not work</strong>  </p>

<pre><code class="language-python">import itertools

for id1, id2 in itertools.combinations(newSeenIds, 2):
</code></pre>

<p><strong>Work</strong>  </p>

<pre><code class="language-python">from java.util import Random

@staticmethod
    def isSampledID(id):
        random = Random(id)
        return random.nextInt(GraphDistance.sample) == 0
</code></pre>

<p><strong>Not work</strong>  </p>

<pre><code class="language-python">import random

@staticmethod
    def isSampledID(id):
        
        return random.randint(1, GraphDistance.sample) == 1
</code></pre>

<p>In favor of import java modules. some python extended libraries is not included in jython. <a href="http://forums.parasoft.com/index.php?showtopic=2302#">refer to this post</a> A side note is that java&#39;s random is exclusive, but jython&#39;s random is inclusive.</p>

<hr/>

<ul>
<li>
<a href="#toc_0">Hadoop version</a>
</li>
<li>
<a href="#toc_1">Use <code>jython-standalone</code> dependency not <code>jython</code></a>
</li>
<li>
<a href="#toc_2">Run from jython REPL</a>
</li>
<li>
<a href="#toc_3">Config intellij for Jython and Java</a>
</li>
<li>
<a href="#toc_4">Run giraph jython from Hadoop Yarn</a>
</li>
<li>
<a href="#toc_5">pitfall of <code>TypeHolder</code> and <code>TypesHolder</code></a>
</li>
<li>
<a href="#toc_6">python and java data type</a>
</li>
<li>
<a href="#toc_7">debug print statement inside python file</a>
</li>
<li>
<a href="#toc_8">Jython type to java pitfall</a>
</li>
<li>
<a href="#toc_9">import python package or jython package</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[git]]></title>
    <link href="www.echo-ohce.com/14742423250638.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250638.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Back to the unaltered HEAD</a>
</li>
<li>
<a href="#toc_1">Messed up local master, come back to the origin/master</a>
</li>
<li>
<a href="#toc_2">Reset one file</a>
</li>
<li>
<a href="#toc_3">Save change in one branch and apply it (partially apply it) to another branch</a>
</li>
<li>
<a href="#toc_4">Add only modified changes and ignore untracked (newly added) files</a>
</li>
<li>
<a href="#toc_5">Syntax of <code>.gitignore</code></a>
</li>
<li>
<a href="#toc_6">Untracked directory and file</a>
</li>
<li>
<a href="#toc_7">discard unstaged changes in Git</a>
</li>
<li>
<a href="#toc_8">Oh Shit, git</a>
</li>
<li>
<a href="#toc_9">Exclude local unstaged changes from being tracked</a>
</li>
<li>
<a href="#toc_10">Locally ignore some unstaged changes</a>
</li>
<li>
<a href="#toc_11">git stash</a>
</li>
<li>
<a href="#toc_12">Set two separate GitLab (Corp) and GitHub (Personal) account</a>
</li>
<li>
<a href="#toc_13">Github page</a>
</li>
<li>
<a href="#toc_14">Quick way of adding untracked files into .gitignore:</a>
</li>
<li>
<a href="#toc_15">Git revert to a previous commit both locally and remotely</a>
</li>
<li>
<a href="#toc_16">Pick only some files from a commit</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Back to the unaltered HEAD</h1>

<p>Run these two in order:  </p>

<pre><code class="language-bash">git reset --hard HEAD
git clean -df
</code></pre>

<p><font color='red'><strong>Be careful of using <code>git clean -df</code>, it will delete all the test cases</strong></font></p>

<h1 id="toc_1">Messed up local master, come back to the origin/master</h1>

<pre><code class="language-bash">git checkout -B master origin/master
</code></pre>

<p>If <code>-B</code> is given, the new branch is created if it doesn’t exist; otherwise, it is reset.  </p>

<h1 id="toc_2">Reset one file</h1>

<pre><code class="language-bash">git checkout HEAD -- my-file.txt
</code></pre>

<p><a href="http://stackoverflow.com/questions/7147270/hard-reset-of-a-single-file">Reference post1</a><br/><br/>
<a href="http://stackoverflow.com/questions/6561142/difference-between-git-checkout-filename-and-git-checkout-filename/6561160#6561160">Reference post2</a></p>

<h1 id="toc_3">Save change in one branch and apply it (partially apply it) to another branch</h1>

<pre><code class="language-bash">git stash
git checkout branch2
git stash list       # to check the various stash made in different branch
git stash pop        # pop out the last stash, and it will be deleted from the stash
git stash apply stash@{0}    # to select the stash and apply to branch2, the stash still saved in the list
git stash -u   # to stash currenlty untracked (newly added) files
</code></pre>

<h1 id="toc_4">Add only modified changes and ignore untracked (newly added) files</h1>

<pre><code class="language-bash">git add .   # stage all changed / untracked (newly added), but not includes deleted
git add -u  # stage all changed / deleted, but not untracked (newly added)
git add -A  # doing both of above
</code></pre>

<h1 id="toc_5">Syntax of <code>.gitignore</code></h1>

<pre><code class="language-bash">**/foo # matches file or directory &quot;foo&quot; anywhere
foo # the same as above  
**/foo/bar # matches file or   directory &quot;bar&quot; anywhere that is directly under directory &quot;foo&quot;  
abc/** # matches all files inside directory &quot;abc&quot;, infinite depth  
abc/* # matches all files inside directory &quot;abc&quot;, but not its sub-directories.  
a/**/b # match 0 or more directories in between  
a/**/b # matches &quot;a/b&quot;, &quot;a/x/b&quot;, &quot;a/x/y/b&quot; and so on.
</code></pre>

<h1 id="toc_6">Untracked directory and file</h1>

<ol>
<li><p>File previously tracked (in git repository) need remove it from the repo but keep it at local. <em>This is called as delete the cache</em><br/><br/>
<code>git rm -r -f --cached mydirectory/myfile</code><br/><br/>
the <code>-r</code> is recursive, the <code>-f</code> is force (in case, this tracked file has uncommitted change).  </p></li>
<li><p>Only change the local repo&#39;s <code>.gitignore</code> without change the remote repo&#39;s tracking behavior. Add ignore into <code>.git/info/exclude</code> instead of the <code>.gitignore</code> file at root.<br/><br/>
<a href="http://stackoverflow.com/questions/767147/ignore-the-gitignore-file-itself">reference post</a></p></li>
</ol>

<h1 id="toc_7">discard unstaged changes in Git</h1>

<pre><code class="language-bash">git checkout -- .
</code></pre>

<p>Make sure to include the last <code>.</code></p>

<h1 id="toc_8">Oh Shit, git</h1>

<p><a href="http://ohshitgit.com">An interesting blog describe git scenarios in plain english</a></p>

<h1 id="toc_9">Exclude local unstaged changes from being tracked</h1>

<p>This <a href="http://stackoverflow.com/questions/13630849/git-difference-between-assume-unchanged-and-skip-worktree">post</a> described my need:</p>

<blockquote>
<p>I have local changes to a file that I don&#39;t want to commit to my repository. It is a configuration file for building the application on a server, but I want to build locally with different settings. Naturally, the file always shows up when i do &#39;git status&#39; as something to be staged. I would like to hide this particular change and not commit it. I won&#39;t make any other changes to the file.</p>
</blockquote>

<p>In order to do that we should use <code>--skip-worktree</code> option like follow:</p>

<pre><code class="language-bash"># untrack this file: dpp/graph/proguard.gradle
git update-index --skip-worktree dpp/graph/proguard.gradle
</code></pre>

<p>The difference between <code>--assume-unchanged</code> and <code>--skip-worktree</code> is described well <a href="http://stackoverflow.com/a/13631525/4229125">here</a></p>

<h1 id="toc_10">Locally ignore some unstaged changes</h1>

<p>This is actually the same thing as above. But this method is much easier and cleaner.</p>

<p>Update this file <code>.git/info/exclude</code>. This file has the same format as normally <code>.gitignore</code> but is not shared with others.</p>

<p>If after update this file, still shows the unstaged changes, do the above steps. But my own experience is update this file is sufficient.</p>

<p><a href="https://stackoverflow.com/a/1753078/4229125">post on stackoverflow</a>  </p>

<h1 id="toc_11">git stash</h1>

<pre><code class="language-bash"># before checkout another branch
git stash
# checkcout to another branch and do other jobs
# after done, checkout back to this branch
git stash list
# stash@{0}: WIP on ali2: 0832570 Merge branch &#39;ds-163&#39; into ali2 
git stash apply --index stash@{0}
git stash drop --index stash@{0}

# directly use stash pop
git stash pop
</code></pre>

<h1 id="toc_12">Set two separate GitLab (Corp) and GitHub (Personal) account</h1>

<p>The most important steps are:<br/>
1. Edit <code>~/.ssh/config</code> file and set up the two hosts.<br/>
2. Generate two separate ssh keys and put them in the folder <code>~/.ssh/</code><br/>
3. The main account is set by <code>git config --global</code> and the other account is set by locally <code>git config</code> overridden. </p>

<p>The main reference post is <a href="https://gist.github.com/rosswd/e1afd2b0b0d515517eac">here</a></p>

<h1 id="toc_13">Github page</h1>

<p>Nothing special, just first need first create an empty repository using the unique name required.</p>

<h1 id="toc_14">Quick way of adding untracked files into .gitignore:</h1>

<pre><code class="language-bash">git status --porcelain | grep &#39;^??&#39; | cut -c4- &gt;&gt; .gitignore
</code></pre>

<p>Or just hide it in the view for this time:</p>

<pre><code class="language-bash">git status -uno
</code></pre>

<p><a href="https://stackoverflow.com/a/15142010/4229125">stackoverflow post</a></p>

<h1 id="toc_15">Git revert to a previous commit both locally and remotely</h1>

<ul>
<li>first find the commit SHA by: <code>git log --all</code>, or <code>git l</code></li>
<li>check current branch is <code>ygdev</code></li>
<li>revert locally: <code>git reset --hard SHA</code></li>
<li>revert remotely: <code>git push -f upstream ygdev</code>. Need use <code>-f</code> to force update</li>
</ul>

<h1 id="toc_16">Pick only some files from a commit</h1>

<p>It is not <code>cherry-pick</code>, <code>cherry-pick</code> means pick some commits, not some files inside a commit.</p>

<ul>
<li><p>List the change of the files between the commit (we need pick from) and the current head</p>

<pre><code class="language-bash">git diff --name-only ef0e558f 62ba3f1b
</code></pre></li>
<li><p>Apply the files you need</p>

<pre><code class="language-bash">git checkout source_branch &lt;paths&gt;...
git checkout ygdev verticals/federated/src/main/java/com/linkedin/galene/federatedsearch/federated/search/blenders/rankers/FederatedSearchRankerFactory.java
</code></pre></li>
<li><p>Want see exactly what changed in one file between two commit</p>

<pre><code class="language-bash">git diff ef0e558f 62ba3f1b -- product-spec.json
</code></pre></li>
</ul>

<hr/>

<ul>
<li>
<a href="#toc_0">Back to the unaltered HEAD</a>
</li>
<li>
<a href="#toc_1">Messed up local master, come back to the origin/master</a>
</li>
<li>
<a href="#toc_2">Reset one file</a>
</li>
<li>
<a href="#toc_3">Save change in one branch and apply it (partially apply it) to another branch</a>
</li>
<li>
<a href="#toc_4">Add only modified changes and ignore untracked (newly added) files</a>
</li>
<li>
<a href="#toc_5">Syntax of <code>.gitignore</code></a>
</li>
<li>
<a href="#toc_6">Untracked directory and file</a>
</li>
<li>
<a href="#toc_7">discard unstaged changes in Git</a>
</li>
<li>
<a href="#toc_8">Oh Shit, git</a>
</li>
<li>
<a href="#toc_9">Exclude local unstaged changes from being tracked</a>
</li>
<li>
<a href="#toc_10">Locally ignore some unstaged changes</a>
</li>
<li>
<a href="#toc_11">git stash</a>
</li>
<li>
<a href="#toc_12">Set two separate GitLab (Corp) and GitHub (Personal) account</a>
</li>
<li>
<a href="#toc_13">Github page</a>
</li>
<li>
<a href="#toc_14">Quick way of adding untracked files into .gitignore:</a>
</li>
<li>
<a href="#toc_15">Git revert to a previous commit both locally and remotely</a>
</li>
<li>
<a href="#toc_16">Pick only some files from a commit</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[github Page, Hexo, MWeb, GoDaddy setting]]></title>
    <link href="www.echo-ohce.com/14742423250557.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250557.html</id>
    <content type="html"><![CDATA[
<p>I tried to use MWeb + Hexo to setup this blog, but it does not work out well. Give this up, turn back to MWeb only</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Set two separate GitLab (Corp) and GitHub (Personal) account</a>
</li>
<li>
<a href="#toc_1">Github page</a>
</li>
<li>
<a href="#toc_2">Hexo</a>
</li>
<li>
<a href="#toc_3">Freemind Hexo theme</a>
</li>
<li>
<a href="#toc_4">Mweb</a>
</li>
<li>
<a href="#toc_5">GoDaddy</a>
</li>
<li>
<a href="#toc_6">Hexo works with MathJax / LaTex</a>
</li>
<li>
<a href="#toc_7">Future work</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Set two separate GitLab (Corp) and GitHub (Personal) account</h1>

<p>The most important steps are:<br/>
1. Edit <code>~/.ssh/config</code> file and set up the two hosts.<br/>
2. Generate two separate ssh keys and put them in the folder <code>~/.ssh/</code><br/>
3. The main account is set by <code>git config --global</code> and the other account is set by locally <code>git config</code> overridden. </p>

<p>The main reference post is <a href="https://gist.github.com/rosswd/e1afd2b0b0d515517eac">here</a></p>

<h1 id="toc_1">Github page</h1>

<p>Nothing special, just first need first create an empty repository using the unique name required.</p>

<h1 id="toc_2">Hexo</h1>

<p>Hexo is used to build everything and managers everything. <br/>
It sets the local folder structures, manages the style / theme / plugin, generates the sites, publish to the github pages. Some important things about hexo</p>

<ol>
<li><a href="">hexo installation and integration with github pages</a></li>
<li><a href="https://hexo.io/docs/index.html">Hexo basic commands</a></li>
<li><a href="https://hexo.io/docs/front-matter.html">Hexo Front-matter</a>. The <code>freemind</code> theme that I am using provides extra Front-matter: <code>toc</code> and <code>top</code></li>
<li>[YGMARK] in <code>_config.yml</code> setting, after <code>key:</code> a space must be used, otherwise all sorts of bugs.</li>
</ol>

<h1 id="toc_3">Freemind Hexo theme</h1>

<ul>
<li><a href="https://github.com/wzpan/hexo-theme-freemind/">Installation Official</a></li>
<li><a href="http://baoxiehao.com/2014/05/17/Hexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96/">非常好的一篇博客，详细说明了Freemind Hexo的优化。有时间再仔细看</a></li>
<li><a href="http://masikkk.com/blog/hexo-4-usage-of-hexo-theme/">一篇中文的博客，更详细，并提到freemind search engine的设置</a></li>
</ul>

<h1 id="toc_4">Mweb</h1>

<p>Mweb is simply the Markdown writing program.<br/>
It has a very good full <a href="http://www.mweb.im/markdown-syntax-guide-full-version.html">Markdown syntax guide</a>.<br/>
打开MWeb, ​Command + E 切换到外部模式(使用cmd + L可以切回默认的library模式), 然后把hexo下的source目录拖到左边, 设置它的Display name为github blog(随意), 最重要一点是设置Media Save Path为Absolute, 这样就可以直接使用MWeb的粘贴图片功能.<br/><br/>
<img src="media/14742423250557/14742438643424.jpg" alt=""/></p>

<h1 id="toc_5">GoDaddy</h1>

<ul>
<li>Add a <code>CNAME</code> file in the <code>source</code> folder: <code>www.echo-ohce.com</code></li>
<li><p>Go to GoDaddy and config the domain name</p>

<ul>
<li><p>Add an &quot;A (Host)&quot; record with &quot;host&quot; = <code>@</code> and &quot;Points to&quot; = <code>192.30.252.153</code> or <code>192.30.252.154</code><br/>
<img src="media/14742423250557/14742439346348.png" alt=""/></p></li>
<li><p>Create a CNAME record<br/><br/>
<img src="media/14742423250557/14742439461628.png" alt=""/></p></li>
</ul></li>
<li><p><a href="https://help.github.com/articles/setting-up-an-apex-domain/">Official help page</a></p></li>
<li><p><a href="http://andrewsturges.com/blog/jekyll/tutorial/2014/11/06/github-and-godaddy.html">My reference post</a></p></li>
<li><p>Specical Notes: 在CNAME中的域名不需要<code>http://</code>。</p></li>
</ul>

<h1 id="toc_6">Hexo works with MathJax / LaTex</h1>

<ul>
<li>install plugin <a href="https://github.com/akfish/hexo-math">hexo-math</a> at the root folder of the blog <code>npm install hexo-math --save</code></li>
<li>Update the root <code>_config.xml</code> append the following:</li>
</ul>

<pre><code class="language-xml"># Math
math:
  engine: &#39;mathjax&#39;
  mathjax:
    src: &quot;//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;
    config:
      tex2jax:
        inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ]
        skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;, &#39;code&#39;]
        processEscaps: true
      TeX:
        equationNumbers:
          autoNumber: &quot;AMS&quot;
</code></pre>

<ul>
<li>Inline LaTex <code>$...$</code>: \(x \rightarrow y\)</li>
<li>Block <code>$$...$$</code>:  \[\cos 2\theta = \cos^2 \theta - \sin^2 \theta =  2 \cos^2 \theta - 1\]</li>
<li>Multiple lines <code>{% math %} ... {% endmath %}</code>:</li>
</ul>

<p>{% math %}<br/>
\begin{aligned}<br/>
\dot{x} &amp; = \sigma(y-x) \<br/>
\dot{y} &amp; = \rho x - y - xz \<br/>
\dot{z} &amp; = -\beta z + xy<br/>
\end{aligned}<br/>
{% endmath %}</p>

<h1 id="toc_7">Future work</h1>

<p>GitHub  \(\rightarrow\)  GitLab <br/>
1. <a href="https://yudachi.biz/2016/05/27/moving-to-gitlab-pages/">中文的 GitLab and Hexo</a><br/>
2. <a href="http://blog.vanjor.com/2016/05/hexo-github-blog-install.html">代码双线托管，域名DNS双线解析</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Set two separate GitLab (Corp) and GitHub (Personal) account</a>
</li>
<li>
<a href="#toc_1">Github page</a>
</li>
<li>
<a href="#toc_2">Hexo</a>
</li>
<li>
<a href="#toc_3">Freemind Hexo theme</a>
</li>
<li>
<a href="#toc_4">Mweb</a>
</li>
<li>
<a href="#toc_5">GoDaddy</a>
</li>
<li>
<a href="#toc_6">Hexo works with MathJax / LaTex</a>
</li>
<li>
<a href="#toc_7">Future work</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[graphviz Dot]]></title>
    <link href="www.echo-ohce.com/14773416332741.html"/>
    <updated>2016-10-24T13:40:33-07:00</updated>
    <id>www.echo-ohce.com/14773416332741.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Good Resources</a>
</li>
<li>
<a href="#toc_1">Fix the node locations after adding new edges</a>
</li>
<li>
<a href="#toc_2">Layout</a>
</li>
<li>
<a href="#toc_3">command line of dot</a>
</li>
<li>
<a href="#toc_4">Using of color</a>
</li>
<li>
<a href="#toc_5">Maybe better alternative</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Good Resources</h1>

<ol>
<li><a href="http://www.graphviz.org/doc/info/attrs.html">Node, Edge and Graph Attributes - API doc to find parameters to use</a></li>
<li><a href="http://www.tonyballantyne.com/graphs.html#sec-5-2">A good tutorial, very simple and easy</a></li>
<li><a href="http://graphs.grevian.org/reference">Another very quick reference site</a> Some graph showing on the website are not using the default <code>-Kdot</code> engine.</li>
</ol>

<h1 id="toc_1">Fix the node locations after adding new edges</h1>

<p>For example, I made a graph G1, and I want to show that after some steps new edges are added to the graph G2.</p>

<p>Use neato layout engine as following steps </p>

<ol>
<li>Using normal way to generate the graph G1 (normal .dot file)</li>
<li>Output the G1 in <code>gv</code> format<br/>
<code>dot -Tgv -Kdot tinygraphOriginal.dot -o original.gv</code> </li>
<li>Open the <code>gv</code> file and adding edges</li>
<li>Generate the new graph G2 by<br/>
<code>dot -Tpng -Kneato -n original.gv -o C3.png</code></li>
</ol>

<h1 id="toc_2">Layout</h1>

<p>Couple tricks to use: </p>

<ol>
<li>Using subgrap:
<code>clusterrank=local</code>
<code>subgraph cluster_***</code><br/></li>
<li>Using <code>[style=invis]</code> for invisible nodes and edges to align</li>
<li>Using <code>rank=same</code>, <code>group</code> etc</li>
<li>Using <code>weight</code> properties of edges</li>
</ol>

<h1 id="toc_3">command line of dot</h1>

<p>The <a href="http://graphviz.org/content/command-line-invocation">complete arguments</a> are listed here. But the most frequently used one is:</p>

<pre><code class="language-bash">dot -Tpng -Kdot tinygraphOriginal.dot -o original.png
</code></pre>

<h1 id="toc_4">Using of color</h1>

<ul>
<li>In the graph top level specify the color scheme</li>
</ul>

<pre><code class="language-dot">strict graph {
    label=&quot;Original C1 graph&quot;;
    nodesep=2.0
    splines=line;
    colorscheme=&quot;Brewer&quot;;
    ...
    
</code></pre>

<ul>
<li>use color and fontcolor, also use the form <code>/xxx/#</code> to specify the detailed palette and color.</li>
</ul>

<pre><code class="language-dot">1 -- 2 [ label=&quot;0.1&quot;, dir=both, color=&quot;/set312/1&quot;, fontcolor=&quot;/set312/1&quot;];
</code></pre>

<ul>
<li><a href="http://www.graphviz.org/doc/info/colors.html">List of available colors are here</a></li>
</ul>

<h1 id="toc_5">Maybe better alternative</h1>

<p>For small one time use graph, maybe use <code>Omnigraffle</code> is a much better choice.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[intellij]]></title>
    <link href="www.echo-ohce.com/14765768181414.html"/>
    <updated>2016-10-15T17:13:38-07:00</updated>
    <id>www.echo-ohce.com/14765768181414.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, shortcuts I learned. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">List of most frequently used shortcut:</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">List of most frequently used shortcut:</h1>

<ul>
<li>Jump to previous location (navigate back):

<ul>
<li>cmd + [</li>
<li>cmd + ]</li>
</ul></li>
<li>Go to the previous/next occurrence of the word. This feature is called <code>Find Word at Caret</code> <a href="http://stackoverflow.com/a/18404014/4229125">stackoverflow post</a>. I set it as <code>Ctrl+,</code></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[machine Learning]]></title>
    <link href="www.echo-ohce.com/14858223421956.html"/>
    <updated>2017-01-30T16:25:42-08:00</updated>
    <id>www.echo-ohce.com/14858223421956.html</id>
    <content type="html"><![CDATA[
<p>This documents my personal understandings about bayesian, keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">An interesting illustration</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">An interesting illustration</h1>

<p><img src="media/14858223421956/14858223844745.jpg" alt=""/></p>

<p><a href="http://radimrehurek.com/data_science_python/">From this post</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[maven]]></title>
    <link href="www.echo-ohce.com/14742423250660.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250660.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">build jar include dependencies</a>
</li>
<li>
<a href="#toc_1">hadoop-common, hadoop-core, hadoop-client</a>
<ul>
<li>
<a href="#toc_2">Another example just found from giraph jar</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Maven build skip Test compilation</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">build jar include dependencies</h1>

<p>For the <code>pom.xml</code> use the <code>maven-assembly-plugin</code>. The lines with respect to this plugin is like follow:  </p>

<pre><code class="language-xml">&lt;build&gt;
   &lt;plugins&gt;
       &lt;plugin&gt;
           &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
           &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
           &lt;configuration&gt;
               &lt;descriptorRefs&gt;
                   &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
               &lt;/descriptorRefs&gt;
           &lt;/configuration&gt;
       &lt;/plugin&gt;
   &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p>The import line is <code>&lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</code>. This means to include and pack all dependencies into the project jar.</p>

<p>the maven command to run is:  </p>

<pre><code class="language-bash">mvn clean package assembly:single -DskipTests
</code></pre>

<h1 id="toc_1">hadoop-common, hadoop-core, hadoop-client</h1>

<p>In order to build a Hadoop map-reduce application you need only hadoop client dependency. (Use new API). Dependencies like hadoop-hdfs,hadoop-common,hadoop-clientapp,hadoop-yarn-api are resolved from this.</p>

<p>To help provide some additional details regarding the differences between Hadoop-common, Hadoop-core and Hadoop-client, from a high-level perspective:</p>

<ul>
<li>Hadoop-common refers to the commonly used utilities and libraries that support the Hadoop modules.</li>
<li>Hadoop-core is the same as Hadoop-common; It was renamed to Hadoop-common in July 2009.</li>
<li>Hadoop-client refers to the client libraries used to communicate with Hadoop&#39;s common components (HDFS, MapReduce, YARN) including but not limited to logging and codecs for example.</li>
</ul>

<p>Generally speaking, for developers who build apps that submit to YARN, run a MR job, or access files from HDFS use Hadoop-client libraries.</p>

<p><a href="http://stackoverflow.com/a/34229580/4229125">stackoverflow post</a></p>

<h2 id="toc_2">Another example just found from giraph jar</h2>

<p>Error message like:</p>

<blockquote>
<p>java.lang.NoSuchMethodError: org.apache.hadoop.ipc.RPC.getServer</p>
</blockquote>

<p>This is because the <code>pom.xml</code> had <code>hadoop-common</code> dependency, should change to <code>hadoop-client</code></p>

<h1 id="toc_3">Maven build skip Test compilation</h1>

<p><code>-DskipTests</code> just skips the test execution: the tests are still compiled.<br/>
<code>-Dmaven.test.skip</code> skips both compilation and execution of the tests.</p>

<p>I need this because I have .gitignore of all the test folders.</p>

<p>This is the command to be used:</p>

<pre><code class="language-bash">mvn clean package assembly:single -Dmaven.test.skip
</code></pre>

<p><a href="http://stackoverflow.com/a/2593834/4229125">stackoverflow post</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">build jar include dependencies</a>
</li>
<li>
<a href="#toc_1">hadoop-common, hadoop-core, hadoop-client</a>
<ul>
<li>
<a href="#toc_2">Another example just found from giraph jar</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Maven build skip Test compilation</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[metrics]]></title>
    <link href="www.echo-ohce.com/14742423250680.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250680.html</id>
    <content type="html"><![CDATA[
<p>This document collects statistics metrics used in my work.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">precision</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">precision</h1>

<p><img src="media/14742423250680/14742450671711.jpg" alt=""/></p>

<hr/>

<ul>
<li>
<a href="#toc_0">precision</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mongodb]]></title>
    <link href="www.echo-ohce.com/14820465795454.html"/>
    <updated>2016-12-17T23:36:19-08:00</updated>
    <id>www.echo-ohce.com/14820465795454.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Pandas and Mongodb</a>
</li>
<li>
<a href="#toc_1">Auto Start MongoDB daemon (mongod) on the background</a>
</li>
<li>
<a href="#toc_2">Hitting the ulimit</a>
</li>
<li>
<a href="#toc_3">sort argument inside find_one method</a>
</li>
<li>
<a href="#toc_4">Indexing and sort in both directions</a>
</li>
<li>
<a href="#toc_5">mongodb locations</a>
</li>
<li>
<a href="#toc_6">check mongodb is running or not</a>
</li>
<li>
<a href="#toc_7">embedded documents</a>
</li>
<li>
<a href="#toc_8">delete a field</a>
</li>
<li>
<a href="#toc_9">sort</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Pandas and Mongodb</h1>

<ul>
<li>From pandas dataframe to mongodb: use <code>to_dict(&#39;records&#39;)</code>. <font color='red'>Must have the records parameters to get a list of dict, not a nested dict </font>. <a href="http://stackoverflow.com/a/33984472/4229125">Stackoverflow post</a></li>
</ul>

<pre><code class="language-python">db.timeline.insert_many(yahoo_df[::-1]      #yahoo_df is reversed order
                        .drop(&#39;Adj Close&#39;, axis=1)      #don&#39;t need the adj close
                        .to_dict(&#39;records&#39;))            #to mongodb
</code></pre>

<ul>
<li>From mongodb to pandas dataframe: use <code>list</code> of the pymonogo&#39;s <code>find</code>&#39;s cursor.  <a href="http://stackoverflow.com/a/17805626/4229125">Stackoverflow post</a></li>
</ul>

<pre><code class="language-python">from dateutil import parser as dateparser

start_date = dateparser.parse(&#39;2015-01-01&#39;)
end_date = dateparser.parse(&#39;2016-01-01&#39;)
mongo_df = pd.DataFrame(list(db.timeline.find({&#39;Date&#39;:{&#39;$lt&#39;:end_date, &#39;$gt&#39;:start_date}})))
</code></pre>

<ul>
<li>If it is pymongo&#39;s <code>find_one</code> output, it should convert to <code>pd.Series</code> instead of dataframe</li>
</ul>

<pre><code class="language-python">import pymongo
import pandas as pd
client = pymongo.MongoClient(host=&#39;localhost&#39;, port=27017)
db = client[&#39;personalTrader&#39;]
symbol_coll = db[&#39;symbol&#39;]
stock_info = pd.Series(symbol_coll.find_one({&#39;code&#39;:&#39;600000&#39;}))
</code></pre>

<h1 id="toc_1">Auto Start MongoDB daemon (mongod) on the background</h1>

<p><a href="http://serverfault.com/a/398366">Stackoverflow post</a></p>

<pre><code class="language-bash">mkdir -p ~/Library/LaunchAgents
cp /usr/local/Cellar/mongodb/3.2.10/homebrew.mxcl.mongodb.plist ~/Library/LaunchAgents
launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.mongodb.plist
</code></pre>

<p>This will launch mongod in the background now and every time you restart your computer.</p>

<p>The best way to get a program to run at startup on OS X is to create a LaunchDaemon (see <a href="http://developer.apple.com/mac/library/documentation/MacOSX/Conceptual/BPSystemStartup/Articles/LaunchOnDemandDaemons.html">Apple&#39;s docs</a>, and take a look at some of the Apple-supplied daemons in /System/Library/LaunchDaemons) and install it in /Library/LaunchDaemons. <a href="https://sourceforge.net/projects/lingon/files/">Lingon</a> can help you create the .plist file.<br/>
Note by Y.G <code>Lingon</code> is dead now.</p>

<p><font color='red'><strong>UPDATE</strong></font><br/>
A easier way is to use the app <code>MongoDB.prefPane</code> <a href="https://www.mongodb.com/blog/post/macosx-preferences-pane-for-mongodb">Official link</a><br/><br/>
After install this app, the <code>~/Library/LaunchAgents/homebrew.mxcl.mongodb.plist</code> will be replaced by <code>~/Library/LaunchAgents/com.remysaissy.mongodbprefspane.plist</code>. However the content is exactly the same.</p>

<h1 id="toc_2">Hitting the ulimit</h1>

<p><strong><a href="https://github.com/basho/basho_docs/issues/1402">A very good summary post about this</a></strong></p>

<p>mongod silently die, check the log at <code>/usr/local/var/log/mongodb/mongo.log</code> find the following error:</p>

<blockquote>
<p>2016-12-18T21:44:30.153-0800 W FTDC     [ftdc] Uncaught exception in &#39;FileNotOpen: Failed to open interim file /usr/local/var/mongodb/diagnostic.data/metrics.interim.temp&#39; <br/>
2016-12-18T21:44:34.930-0800 E STORAGE  [thread1] WiredTiger (24) [1482126274:929985][960:0x700000393000], file:WiredTiger.wt, WT_SESSION.checkpoint: /usr/local/var/mongodb/WiredTiger.turtle: handle-open: open: Too many open files<br/>
2016-12-18T21:44:34.934-0800 I -        [thread1] Fatal Assertion 28558</p>
</blockquote>

<p>This is because I have too many collections in the db and hitting the limit of open files.</p>

<ul>
<li><a href="https://docs.mongodb.com/manual/reference/ulimit/#ulimit">check the current limit</a><br/></li>
</ul>

<pre><code class="language-bash">ulimit -a  
</code></pre>

<ul>
<li><a href="https://docs.mongodb.com/manual/reference/ulimit/#recommended-ulimit-settings">recommended limit</a></li>
<li>temporary set a higher limit (which will expire when restart)</li>
</ul>

<pre><code class="language-bash">sudo launchctl limit maxfiles 65536 65536
sudo launchctl limit maxproc 2048 2048
ulimit -n 65536
sudo ulimit -u 2048
</code></pre>

<p>What I did in the end that works are exactly following the <a href="https://github.com/basho/basho_docs/issues/1402">aforementioned post</a></p>

<ol>
<li>Changed the limit from the <code>plist</code> file.</li>
<li>created the two file: <code>/Library/LaunchDaemons/limit.maxfiles.plist</code> and <code>/Library/LaunchDaemons/limit.maxproc.plis</code></li>
<li>Updated my <code>~/.bashrc</code></li>
<li>restart</li>
</ol>

<h1 id="toc_3">sort argument inside find_one method</h1>

<p>In <code>pymongo</code>, it is easy to make bug at the the sort syntax. The correct way of sort and take the first one. (The <code>[]</code> of the sort argument is a must, i.e. sort must be a <strong>list</strong> of <strong>tuple</strong>)</p>

<pre><code class="language-python">first_date = db[ncode].find_one(sort=[(&#39;Date_d&#39;, pymongo.ASCENDING)])[&#39;Date_d&#39;]
last_date = db[ncode].find_one(sort=[(&#39;Date_d&#39;, pymongo.DESCENDING)])[&#39;Date_d&#39;]
</code></pre>

<h1 id="toc_4">Indexing and sort in both directions</h1>

<p>MongoDB can scan an index from both directions. MongoDB may also traverse the index in either directions. As a result, for single-field indexes, ascending and descending indexes are interchangeable. This is not the case for compound indexes: in compound indexes, the direction of the sort order can have a greater impact on the results</p>

<h1 id="toc_5">mongodb locations</h1>

<p>After installing MongoDB with <code>Homebrew</code>:</p>

<ul>
<li>The databases are stored in the <code>/usr/local/var/mongodb/</code> directory</li>
<li>The <code>mongod.conf</code> file is here: <code>/usr/local/etc/mongod.conf</code></li>
<li>The mongo logs can be found at <code>/usr/local/var/log/mongodb/</code></li>
<li>The mongo binaries are here: <code>/usr/local/Cellar/mongodb/[version]/bin</code></li>
</ul>

<h1 id="toc_6">check mongodb is running or not</h1>

<pre><code class="language-bash">ps -ef | grep mongod | grep -v grep | wc -l | tr -d &#39; &#39;
</code></pre>

<h1 id="toc_7">embedded documents</h1>

<p>Filter or no filter,there is no difference on performance. See this <a href="http://stackoverflow.com/a/2140125/4229125">post</a>:  </p>

<blockquote>
<p>There&#39;s currently no way to filter on embedded docs in the way you&#39;re describing. Using the dot notation allows you to match on an embedded doc, but the entire document, parent and all, will still be returned. It&#39;s also possible to select which fields will be returned, but that doesn&#39;t really help your case, either.</p>
</blockquote>

<p>It doesn&#39;t make difference to filter it inside pandas DataFrame or filter it from the mongodb side.</p>

<p>However, I have some good working examples for the advanced usage of mongodb</p>

<ul>
<li>put a dataframe into a document as an array of embedded sub-documents</li>
</ul>

<pre><code class="language-python">code = &#39;600033&#39;
symbol_coll.update_one({&#39;code&#39;:code}, 
                       {&#39;$set&#39;: {&#39;fhpg&#39;: [dict(v.dropna()) for k, v in df.iterrows()]}}, 
                       upsert=False)
</code></pre>

<p>In the above code, <code>[dict(v.dropna()) for k, v in df.iterrows()]</code> is equivalent to <code>df.to_dict(&#39;records&#39;)</code>, but also skip the field with <code>nan</code> values when insert into the mongodb, i.e. make the database a little bit cleaner.</p>

<ul>
<li>get the dataframe out from an array of embedded sub-documents inside a document. Need use pipeline (listed two methods below)</li>
</ul>

<pre><code class="language-python">p_match = {&#39;$match&#39;:{&#39;code&#39;:code}}
p_project = {&#39;$project&#39;: {&#39;fhps&#39;:1, &#39;_id&#39;:0}}
pipeline=[p_match, p_project]

# note the use of [0], because the aggregate result is a list with one element
df = pd.DataFrame(list(symbol_coll.aggregate(pipeline))[0][&#39;fhps&#39;])

# use $unwind, do not need [0] anymore.
p_unwind = {&#39;$unwind&#39;: {&#39;path&#39;:&#39;$fhps&#39;}}
pipeline=[p_match, p_project, p_unwind]
df = pd.DataFrame([record[&#39;fhps&#39;] for record in symbol_coll.aggregate(pipeline)])
</code></pre>

<ul>
<li>get only one field (one column) out from an array of embedded sub-documents inside a document.</li>
</ul>

<p>The trick is inside the <code>$project</code>: using the <a href="https://docs.mongodb.com/manual/reference/glossary/#term-field-path">field path</a> syntax</p>

<pre><code class="language-python">p_match = {&#39;$match&#39;:{&#39;code&#39;:code}}
p_unwind = {&#39;$unwind&#39;: {&#39;path&#39;:&#39;$fhps&#39;}}
# here used field path syntax
p_project = {&#39;$project&#39;: {&#39;Date&#39;: &#39;$fhps.Date&#39;, &#39;_id&#39;:0}}
pipeline=[p_match, p_unwind, p_project]
df = pd.DataFrame(list(symbol_coll.aggregate(pipeline)))
</code></pre>

<ul>
<li>get some rows out from an array of embedded sub-documents inside a document.</li>
</ul>

<p>This is much easier to do once we have pandas dataframe. To do it in side mongo actually has no benefit. But document there as a future reference</p>

<p><strong>A pitfall : <code>$elemMatch</code> used with <code>$project</code> only return the <font color='red'>first</font> matching element.</strong> <a href="https://docs.mongodb.com/v3.2/reference/operator/projection/elemMatch/#definition">See reference here</a><br/>
<strong>The same thing holds true for <code>$</code>, i.e. the positional operator</strong> <a href="https://docs.mongodb.com/v3.2/reference/operator/projection/positional/#proj._S_">reference here</a></p>

<pre><code class="language-python"># only return the first match
symbol_coll.find_one({&#39;code&#39;:code, 
                      &#39;fhps.Date&#39;:{&#39;$lt&#39;:date_parser.parse(&#39;2012-06-25&#39;)}},
                     {&#39;fhps.$&#39;:1, &#39;_id&#39;:0})
# still only return the first match
symbol_coll.find_one({&#39;code&#39;: code}, {&#39;fhps&#39;: {&#39;$elemMatch&#39;: {&#39;Fh&#39;: {&#39;$gt&#39;: 0.01}}}, &#39;_id&#39;:0})    
</code></pre>

<p>Need use <code>$filter</code> method inside the <code>$project</code> pipeline stage (<font color='red'> <code>$filter</code> itself is not a pipeline stage</font>)</p>

<pre><code class="language-python">p_match = {&#39;$match&#39;:{&#39;code&#39;:code}}
p_project = {&#39;$project&#39;: {&#39;fhps&#39;:{&#39;$filter&#39;:{&#39;input&#39;:&#39;$fhps&#39;, 
                                             &#39;as&#39;:&#39;fhps_f&#39;,
                                             &#39;cond&#39;:{&#39;$gt&#39;: [&#39;$$fhps_f.Date&#39;, date_parser.parse(&#39;2012-06-25&#39;)]}}},
                          &#39;_id&#39;:0}}
pipeline=[p_match, p_project]
df = pd.DataFrame(list(symbol_coll.aggregate(pipeline))[0][&#39;fhps&#39;])
</code></pre>

<p><code>$$</code> in the above code is the syntax to refer a variable, as explained <a href="https://docs.mongodb.com/master/meta/aggregation-quick-reference/#field-path-and-system-variables">in the official document</a></p>

<h1 id="toc_8">delete a field</h1>

<p>I need remove <code>fhps</code> which is an array of embedded sub-documents:</p>

<pre><code class="language-python"># note the empty &#39;&#39; inside the unset
symbol_coll.update_one({&#39;code&#39;:code}, {&#39;$unset&#39;:{&#39;fhps&#39;:&#39;&#39;}})
</code></pre>

<h1 id="toc_9">sort</h1>

<hr/>

<ul>
<li>
<a href="#toc_0">Pandas and Mongodb</a>
</li>
<li>
<a href="#toc_1">Auto Start MongoDB daemon (mongod) on the background</a>
</li>
<li>
<a href="#toc_2">Hitting the ulimit</a>
</li>
<li>
<a href="#toc_3">sort argument inside find_one method</a>
</li>
<li>
<a href="#toc_4">Indexing and sort in both directions</a>
</li>
<li>
<a href="#toc_5">mongodb locations</a>
</li>
<li>
<a href="#toc_6">check mongodb is running or not</a>
</li>
<li>
<a href="#toc_7">embedded documents</a>
</li>
<li>
<a href="#toc_8">delete a field</a>
</li>
<li>
<a href="#toc_9">sort</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[oozie]]></title>
    <link href="www.echo-ohce.com/14867660612004.html"/>
    <updated>2017-02-10T14:34:21-08:00</updated>
    <id>www.echo-ohce.com/14867660612004.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Run python script by oozie</a>
</li>
<li>
<a href="#toc_1">checking oozie job, kill job</a>
</li>
<li>
<a href="#toc_2">oozie EL expression/function</a>
</li>
<li>
<a href="#toc_3">conflict of two libraries in two jars</a>
</li>
<li>
<a href="#toc_4">global configuration do NOT work with shell action</a>
</li>
<li>
<a href="#toc_5">Oozie memory setting</a>
</li>
<li>
<a href="#toc_6">Put multiple libpath in oozie job</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Run python script by oozie</h1>

<p>Need use distributed cache, to send the python script to all workers. The important workflow lines are:</p>

<pre><code class="language-xml">&lt;exec&gt;script.py&lt;/exec&gt;
&lt;argument&gt;arg1&lt;/argument&gt;
&lt;argument&gt;arg2&lt;/argument&gt;
...
&lt;file&gt;scripts/script.py#script.py&lt;/file&gt;
</code></pre>

<p>The <code>#</code> means distribute the hdfs script file to the workers&#39; local file system using the sepcified name.</p>

<p>The python script writes to the workers&#39; local disk, no in the hdfs. The workaround is inside the python script, first write to a local temp file, then using subprocess to put the file onto hdfs, then delete the local temp file. This is the method I used for the <code>LP_tunning_17</code> project.</p>

<ul>
<li><a href="https://community.hortonworks.com/articles/12927/oozie-python-workflow-example-walkthrough.html">Hortonworks post for an oozie-python example</a></li>
<li><a href="https://community.hortonworks.com/questions/24182/where-is-the-output-of-an-oozie-workflow-is-stored.html">The output location of shell/python in oozie workflow</a></li>
<li><a href="http://blog.cloudera.com/blog/2013/03/how-to-use-oozie-shell-and-java-actions/">How to use shell and java action by cloudera. Pretty good</a></li>
</ul>

<h1 id="toc_1">checking oozie job, kill job</h1>

<p><a href="http://stackoverflow.com/a/24552758/4229125">A list of oozie commands</a></p>

<pre><code class="language-bash">oozie jobs | grep yu
oozie job -kill oozie-job-id
</code></pre>

<h1 id="toc_2">oozie EL expression/function</h1>

<p>Oozie has some specific EL expression/function, which can be find from their <a href="https://oozie.apache.org/docs/3.2.0-incubating/WorkflowFunctionalSpec.html#a4.2_Expression_Language_Functions">documents</a>. For the general EL expression/function, it is using <strong>JSP Expression Language</strong> syntax.. Check the <code>JSP 2.0 specification</code> to get the syntax.</p>

<ul>
<li><a href="http://stackoverflow.com/a/41902598/4229125">See my answer to a stackoverflow question</a></li>
<li><a href="http://blog.cloudera.com/blog/2013/09/how-to-write-an-el-function-in-apache-oozie/">Some example from Cloudera blog</a></li>
</ul>

<h1 id="toc_3">conflict of two libraries in two jars</h1>

<p>I encounter this issue when deploy an oozie workflow include both a giraph action and a spark action. Both action needs custom build jars <code>dpp-giraph-0.0.1-with-giraph-core.jar</code>, <code>spark2.jar</code>. I put these two jars in the <code>lib\</code> folder in oozie standard way. But the workflow failed with strange error message as some library&#39;s versionId is not consistent.</p>

<p>It actually is because in these two custom built jar, we used the same library but with conflicting versions. The solution for this is <strong>NOT</strong> put the jar in the <code>lib\</code>folder, but copy the specific jar in each action stage using the <code>&lt;file&gt;</code> option.</p>

<p>Example:</p>

<pre><code class="language-xml">&lt;action name=&quot;preprocess&quot;&gt;
   &lt;java&gt;
       &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
       &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
       &lt;prepare&gt;
           &lt;delete path=&quot;${nameNode}${lpOutputPath}/sCut_${sCut}/labelPairs&quot;/&gt;
           &lt;delete path=&quot;${nameNode}${lpOutputPath}/sCut_${sCut}/sample&quot;/&gt;
           &lt;delete path=&quot;${nameNode}${lpOutputPath}/sCut_${sCut}/labelIds&quot;/&gt;
       &lt;/prepare&gt;
       &lt;configuration&gt;
           &lt;property&gt;
               &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
               &lt;value&gt;${queueName}&lt;/value&gt;
           &lt;/property&gt;
       &lt;/configuration&gt;
       &lt;main-class&gt;org.apache.spark.deploy.SparkSubmit&lt;/main-class&gt;
       &lt;arg&gt;--conf&lt;/arg&gt;
       &lt;arg&gt;spark.driver.maxResultSize=20g&lt;/arg&gt;
       &lt;arg&gt;--conf&lt;/arg&gt;
       &lt;arg&gt;spark.driver.memory=20g&lt;/arg&gt;
       &lt;arg&gt;--conf&lt;/arg&gt;
       &lt;arg&gt;spark.yarn.executor.memoryOverhead=10240&lt;/arg&gt;
...
       &lt;arg&gt;--driver-cores&lt;/arg&gt;
       &lt;arg&gt;4&lt;/arg&gt;
       &lt;arg&gt;--num-executors&lt;/arg&gt;
       &lt;arg&gt;30&lt;/arg&gt;
       &lt;arg&gt;--executor-memory&lt;/arg&gt;
       &lt;arg&gt;5g&lt;/arg&gt;
       &lt;arg&gt;--executor-cores&lt;/arg&gt;
       &lt;arg&gt;10&lt;/arg&gt;
       &lt;!-- specify code source jar --&gt;
       &lt;arg&gt;spark2.jar&lt;/arg&gt;
       &lt;!--input path--&gt;
       &lt;arg&gt;LP_tunning_17/spark_base/labelPairs&lt;/arg&gt;
       &lt;arg&gt;LP_tunning_17/spark_base/sample&lt;/arg&gt;
       &lt;arg&gt;LP_tunning_17/spark_base/labelIds&lt;/arg&gt;
       &lt;arg&gt;${sCut}&lt;/arg&gt;
       &lt;!--output path--&gt;
       &lt;arg&gt;${lpOutputPath}/sCut_${sCut}/labelPairs&lt;/arg&gt;
       &lt;arg&gt;${lpOutputPath}/sCut_${sCut}/sample&lt;/arg&gt;
       &lt;arg&gt;${lpOutputPath}/sCut_${sCut}/labelIds&lt;/arg&gt;
       &lt;!-- dependencies must be distributed cached and added to the job using &#39;jar&#39; option above --&gt;
       &lt;!-- this jar contains the code --&gt;
       &lt;file&gt;spark2.jar#spark2.jar&lt;/file&gt;
   &lt;/java&gt;
   &lt;ok to=&quot;lp&quot;/&gt;
   &lt;error to=&quot;email-error&quot;/&gt;
&lt;/action&gt;

&lt;action name=&quot;lp&quot;&gt;
   &lt;java&gt;
       &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
       &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
       &lt;prepare&gt;
           &lt;delete path=&quot;${nameNode}${lpOutputPath}/sCut_${sCut}/${LPVersion}/lp-output&quot;/&gt;
       &lt;/prepare&gt;
       &lt;configuration&gt;
           &lt;property&gt;
               &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
               &lt;value&gt;${queueName}&lt;/value&gt;
           &lt;/property&gt;
           &lt;property&gt;
               &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;
               &lt;value&gt;${lpMemoryMB}&lt;/value&gt;
           &lt;/property&gt;
           &lt;property&gt;
               &lt;name&gt;mapred.task.timeout&lt;/name&gt;
               &lt;value&gt;1800000&lt;/value&gt;
           &lt;/property&gt;
           &lt;property&gt;
               &lt;name&gt;mapreduce.task.timeout&lt;/name&gt;
               &lt;value&gt;1800000&lt;/value&gt;
           &lt;/property&gt;
           &lt;property&gt;
               &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;
               &lt;value&gt;-Xmx${lpHeap}m&lt;/value&gt;
           &lt;/property&gt;
       &lt;/configuration&gt;
       &lt;main-class&gt;com.adsymp.dpp.giraph.lp.LPRunner&lt;/main-class&gt;
       &lt;arg&gt;com.adsymp.dpp.giraph.lp.WeightedLPComputation&lt;/arg&gt;
       &lt;arg&gt;${lpZKList}&lt;/arg&gt;
       &lt;arg&gt;-eif&lt;/arg&gt;
       &lt;arg&gt;com.adsymp.dpp.giraph.LongFloatTextEdgeInputFormat&lt;/arg&gt;
       &lt;arg&gt;-eip&lt;/arg&gt;
       &lt;arg&gt;${lpOutputPath}/sCut_${sCut}/sample&lt;/arg&gt;
       &lt;arg&gt;-vof&lt;/arg&gt;
       &lt;arg&gt;org.apache.giraph.io.formats.IdWithValueTextOutputFormat&lt;/arg&gt;
       &lt;arg&gt;-op&lt;/arg&gt;
       &lt;arg&gt;${lpOutputPath}/sCut_${sCut}/${LPVersion}/lp-output&lt;/arg&gt;
       &lt;arg&gt;-w&lt;/arg&gt;
       &lt;arg&gt;${lpWorkers}&lt;/arg&gt;
   ...
       &lt;arg&gt;-ca&lt;/arg&gt;
       &lt;arg&gt;mapreduce.map.memory.mb=${lpMemoryMB}&lt;/arg&gt;
       &lt;arg&gt;-ca&lt;/arg&gt;
       &lt;arg&gt;mapreduce.map.java.opts=-Xmx${lpHeap}m&lt;/arg&gt;
       &lt;file&gt;dpp-giraph-0.0.1-with-giraph-core.jar#dpp-giraph-0.0.1-with-giraph-core.jar&lt;/file&gt;
   &lt;/java&gt;
   &lt;ok to=&quot;postprocess&quot;/&gt;
   &lt;error to=&quot;email-error&quot;/&gt;
&lt;/action&gt;
</code></pre>

<p>And in the folder: <code>/home/yu/LP_tunning_17/sparkWorkflow</code></p>

<pre><code class="language-bash">[yu@sc2-hive1 sparkWorkflow]$ ls
dpp-giraph-0.0.1-with-giraph-core.jar  job_properties_template  nohup.out        scripts     workflow.xml
job.properties                         lib                      oozie-runner.py  spark2.jar
</code></pre>

<h1 id="toc_4">global configuration do NOT work with shell action</h1>

<p><a href="https://oozie.apache.org/docs/3.3.0/WorkflowFunctionalSpec.html#a19_Global_Configurations">As given here</a> the <code>Global configuration</code> supposed to avoid repetition. However, if it is a shell action, you still need repeat <code>&lt;job-tracker&gt;</code> and <code>&lt;name-node&gt;</code></p>

<p>Wrong example</p>

<pre><code class="language-bash">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;alibaba&quot;&gt;

    &lt;global&gt;
        &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
        &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
        &lt;configuration&gt;
            &lt;property&gt;
                &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                &lt;value&gt;${queueName}&lt;/value&gt;
            &lt;/property&gt;
        &lt;/configuration&gt;
    &lt;/global&gt;

    &lt;start to=&quot;base&quot;/&gt;

    &lt;action name=&quot;base&quot;&gt;
        &lt;java&gt;
            &lt;prepare&gt;
                &lt;delete path=&quot;${nameNode}/${pairOut}/base&quot; /&gt;
            &lt;/prepare&gt;
            &lt;main-class&gt;ge.drawbrid.dpp.graph.miaozhen.pairing.Base&lt;/main-class&gt;
            &lt;arg&gt;mapred&lt;/arg&gt;
            &lt;arg&gt;${freqOut}/output/*&lt;/arg&gt;
            &lt;arg&gt;${pairOut}/&lt;/arg&gt;
            &lt;arg&gt;${ipdtThreshold}&lt;/arg&gt; &lt;!-- ipdt is always higher than ipwk --&gt;
            &lt;arg&gt;${numReducer}&lt;/arg&gt;
            &lt;arg&gt;${partnerId}&lt;/arg&gt;
            &lt;arg&gt;${partnerKey}&lt;/arg&gt;
            &lt;file&gt;${ipFile}#ip3-type.txt&lt;/file&gt;
        &lt;/java&gt;
        &lt;ok to=&quot;end&quot; /&gt;
        &lt;error to=&quot;email-error&quot; /&gt;
    &lt;/action&gt;
    
    &lt;action name=&#39;train-cd&#39;&gt;
        &lt;shell xmlns=&quot;uri:oozie:shell-action:0.1&quot;&gt;
            &lt;prepare&gt;
                &lt;delete path=&quot;${nameNode}/${modelOut}/${version}/CD&quot;/&gt;
                &lt;mkdir path=&quot;${nameNode}/${modelOut}/${version}/CD&quot;/&gt;
            &lt;/prepare&gt;
            &lt;configuration&gt;
                &lt;property&gt;
                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${queueName}&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.memory.mb&lt;/name&gt;
                    &lt;value&gt;32768&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.java.opts&lt;/name&gt;
                    &lt;value&gt;-Xmx32256m&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;mapreduce.task.timeout&lt;/name&gt;
                    &lt;value&gt;36000000&lt;/value&gt;
                &lt;/property&gt;
            &lt;/configuration&gt;
            &lt;exec&gt;train.py&lt;/exec&gt;
            &lt;argument&gt;${rankOut}/${version}/features-all/CD&lt;/argument&gt;
            &lt;argument&gt;${modelOut}/${version}/CD&lt;/argument&gt;
            &lt;env-var&gt;HADOOP_USER_NAME=${wf:user()}&lt;/env-var&gt;
            &lt;file&gt;${trainScript}#train.py&lt;/file&gt;
            &lt;capture-output/&gt;&lt;/shell&gt;
        &lt;ok to=&quot;pred-cd&quot; /&gt;
        &lt;error to=&quot;email-error&quot; /&gt;
    &lt;/action&gt;
</code></pre>

<p>The second action will fail, and will see error message like this:</p>

<blockquote>
<p>Error: E0701 : E0701: XML schema error, cvc-complex-type.2.4.a: Invalid content was found starting with element &#39;prepare&#39;. One of &#39;{&quot;uri:oozie:shell-action:0.1&quot;:job-tracker}&#39; is expected.</p>
</blockquote>

<p>For the <code>shell</code> action, we still need repeat <code>&lt;job-tracker&gt;</code> and <code>&lt;name-node&gt;</code>. Correct code:</p>

<pre><code class="language-bash">
    &lt;action name=&#39;train-cd&#39;&gt;
        &lt;shell xmlns=&quot;uri:oozie:shell-action:0.1&quot;&gt;
            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
            &lt;prepare&gt;
                &lt;delete path=&quot;${nameNode}/${modelOut}/${version}/CD&quot;/&gt;
                &lt;mkdir path=&quot;${nameNode}/${modelOut}/${version}/CD&quot;/&gt;
            &lt;/prepare&gt;
            &lt;configuration&gt;
                &lt;property&gt;
                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${queueName}&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.memory.mb&lt;/name&gt;
                    &lt;value&gt;32768&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.java.opts&lt;/name&gt;
                    &lt;value&gt;-Xmx32256m&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;mapreduce.task.timeout&lt;/name&gt;
                    &lt;value&gt;36000000&lt;/value&gt;
                &lt;/property&gt;
            &lt;/configuration&gt;
            &lt;exec&gt;train.py&lt;/exec&gt;
            &lt;argument&gt;${rankOut}/${version}/features-all/CD&lt;/argument&gt;
            &lt;argument&gt;${modelOut}/${version}/CD&lt;/argument&gt;
            &lt;env-var&gt;HADOOP_USER_NAME=${wf:user()}&lt;/env-var&gt;
            &lt;file&gt;${trainScript}#train.py&lt;/file&gt;
            &lt;capture-output/&gt;&lt;/shell&gt;
        &lt;ok to=&quot;pred-cd&quot; /&gt;
        &lt;error to=&quot;email-error&quot; /&gt;
    &lt;/action&gt;
</code></pre>

<p><a href="http://stackoverflow.com/questions/35344565/oozie-global-configurations">Here is the stackoverflow post</a></p>

<h1 id="toc_5">Oozie memory setting</h1>

<p><a href="http://www.openkb.info/2016/07/memory-allocation-for-oozie-launcher-job.html">Here is a good post</a></p>

<ul>
<li>oozie.launcher.mapreduce.map.memory.mb</li>
<li>oozie.launcher.mapreduce.map.java.opts</li>
<li>oozie.launcher.yarn.app.mapreduce.am.resource.mb</li>
<li>oozie.launcher.mapreduce.map.java.opts</li>
</ul>

<p>The above 4 parameters set in workflow.xml for each Oozie job controls oozie memory.</p>

<ul>
<li>This is the Oozie case<br/></li>
</ul>

<p><img src="media/14867660612004/14903082502623.png" alt=""/></p>

<ul>
<li>This is the normal case<br/></li>
</ul>

<p><img src="media/14867660612004/14903082595315.png" alt=""/></p>

<h1 id="toc_6">Put multiple libpath in oozie job</h1>

<p>Use <code>,</code> as delimiter.</p>

<p>in job.properties, set:</p>

<pre><code class="language-bash">oozie.libpath=/path/to/jars,another/path/to/jars
oozie.use.system.libpath=true
</code></pre>

<p><a href="http://stackoverflow.com/a/33732743/4229125">Stackoverflow post</a><br/>
<a href="http://blog.cloudera.com/blog/2014/05/how-to-use-the-sharelib-in-apache-oozie-cdh-5/">Original post</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Run python script by oozie</a>
</li>
<li>
<a href="#toc_1">checking oozie job, kill job</a>
</li>
<li>
<a href="#toc_2">oozie EL expression/function</a>
</li>
<li>
<a href="#toc_3">conflict of two libraries in two jars</a>
</li>
<li>
<a href="#toc_4">global configuration do NOT work with shell action</a>
</li>
<li>
<a href="#toc_5">Oozie memory setting</a>
</li>
<li>
<a href="#toc_6">Put multiple libpath in oozie job</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[pig]]></title>
    <link href="www.echo-ohce.com/14742423250700.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250700.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">File Name Pattern Substitution</a>
</li>
<li>
<a href="#toc_1"><code>ToDate(milliseconds)</code> gives wrong date</a>
</li>
<li>
<a href="#toc_2">DateTime SimpleDateFormat</a>
</li>
<li>
<a href="#toc_3">Parameter substitution Shell Runner subtle bug</a>
</li>
<li>
<a href="#toc_4">Becareful of <code>DateTime</code> data type in pig</a>
</li>
<li>
<a href="#toc_5">More pitfalls for <code>DateTime</code> data type</a>
</li>
<li>
<a href="#toc_6">Setting of memory</a>
</li>
<li>
<a href="#toc_7">Pig Unit Test <code>override()</code> method</a>
</li>
<li>
<a href="#toc_8">Error message for wrong reference operator</a>
</li>
<li>
<a href="#toc_9">Pig Java UDF Unit Test</a>
</li>
<li>
<a href="#toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></a>
</li>
<li>
<a href="#toc_11">specify function output data type</a>
</li>
<li>
<a href="#toc_12">using <code>u0001</code> as the delimiter</a>
</li>
<li>
<a href="#toc_13">Error with SUBSTRING and SIZE function</a>
</li>
<li>
<a href="#toc_14">oneline to order two fields</a>
</li>
<li>
<a href="#toc_15">sort with duplicates</a>
</li>
<li>
<a href="#toc_16">input and output are reserved keyword in pig</a>
</li>
<li>
<a href="#toc_17">Mystery ERROR 2999</a>
</li>
<li>
<a href="#toc_18">Cumsum</a>
</li>
<li>
<a href="#toc_19">A pretty good graph cluster to pair Pig example code</a>
</li>
<li>
<a href="#toc_20">A pretty good Score bucketize and cumsum precision Pig example code</a>
</li>
<li>
<a href="#toc_21">Parameter Substitution in Pig</a>
</li>
<li>
<a href="#toc_22">SUBSTRING function</a>
</li>
<li>
<a href="#toc_23">Nested FOREACH</a>
</li>
<li>
<a href="#toc_24">Function are allowed inside FOREACH</a>
</li>
<li>
<a href="#toc_25">Use Star Expression and Project-Range Expression</a>
</li>
<li>
<a href="#toc_26">Strange bug ERROR 2116 related with compression</a>
</li>
<li>
<a href="#toc_27">Strange Bug with Split of two logic expressions</a>
</li>
<li>
<a href="#toc_28">Use <code>AvroStorage()</code> inside pig</a>
</li>
<li>
<a href="#toc_29">Load Parquet data to pig</a>
</li>
<li>
<a href="#toc_30">A side-by-side datatype comparison among avro, parquet, and pig</a>
</li>
<li>
<a href="#toc_31">More about the parquet</a>
</li>
<li>
<a href="#toc_32">From one relation, put two fields into one bag by FLATTEN</a>
</li>
<li>
<a href="#toc_33">Learning from piggybank <code>HashFNV</code></a>
</li>
<li>
<a href="#toc_34">Pig type and Avro complex datatype</a>
</li>
<li>
<a href="#toc_35">Specify complex data type using schema declaration</a>
</li>
<li>
<a href="#toc_36">Pig local mode load local file system data</a>
</li>
<li>
<a href="#toc_37">Pig, Avro, Python UDF <code>float</code> and <code>double</code> type</a>
</li>
<li>
<a href="#toc_38">Config memory of Mapper &amp; Reducer in Yarn</a>
</li>
<li>
<a href="#toc_39">Pass a specific hadoop config parameter to Pig</a>
</li>
<li>
<a href="#toc_40">Pig Set command</a>
</li>
<li>
<a href="#toc_41">Pig Spill</a>
</li>
<li>
<a href="#toc_42">Pig operator causes reduce phase and use of <code>parallel</code> clause</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">File Name Pattern Substitution</h1>

<p>Pig is using hadoop file <code>glob</code> utilities to process the file name pattern. It is <font color=red>NOT</font> using shell&#39;s <code>glob</code>. Hadoop&#39;s <code>glob</code> are documented <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html#globStatus%28org.apache.hadoop.fs.Path%29">here</a>. Specially, it does not support <code>..</code> operator for a range.</p>

<p><a href="http://stackoverflow.com/questions/3515481/pig-latin-load-multiple-files-from-a-date-range-part-of-the-directory-structur">reference post</a></p>

<h1 id="toc_1"><code>ToDate(milliseconds)</code> gives wrong date</h1>

<p>Bug looks like always return date of some time at <code>1970-01-17</code>. This is because the input is 10 digits in seconds, and input should be in milliseconds. Need \( \times 1000\) on the input to get the correct DateTime.</p>

<h1 id="toc_2">DateTime SimpleDateFormat</h1>

<p>Pig&#39;s <code>DateTime</code> type conforms to the following format:</p>

<pre><code class="language-java">SimpleDateFormat dateFormat = 
    new SimpleDateFormat(&quot;yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSSXXX&quot;);
</code></pre>

<p>The string format is: <code>2001-07-04T12:08:56.235-07:00</code><br/>
<a href="https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html">Java official example table</a></p>

<p>[YGMARK] <code>-07:00</code> timezone format is only supported after JDK 7.</p>

<h1 id="toc_3">Parameter substitution Shell Runner subtle bug</h1>

<p>I like to define <code>_</code> as the output file separator. I usually have parameter <code>SEP= &#39;_&#39;</code> in my pig script and shell runner. However, if this parameter is defined inside shell runner, <em>it has to be escaped</em>, like: <code>SEP=&#39;\_&#39;</code>. Otherwise, this mislead error message will show:  </p>

<blockquote>
<p>ERROR org.apache.pig.Main - ERROR 1000: Error during parsing. Lexical error at line 1, column 6.  Encountered: <EOF> after : &quot;&quot;  </p>
</blockquote>

<p>It does not need be escaped if only defined inside pig script.<br/>
Another side note for seeing the above message. It is very likely caused by unbalanced bracket, quote, <font color='red'>extra space between the bash line separator <code>\</code> and the new line</font> etc.</p>

<h1 id="toc_4">Becareful of <code>DateTime</code> data type in pig</h1>

<p>It is convenient to get the time duration of difference by using the <code>DateTime</code> data type. However, in pig it is a know bug that <a href="https://issues.apache.org/jira/browse/PIG-3953">it can not be compared correctly.</a> Therefore ordering, grouping type of work should use either <code>ToUnixTime()</code> or <code>ToString()</code> before ordering.</p>

<blockquote>
<p>Error: org.joda.time.DateTime.compareTo ...</p>
</blockquote>

<h1 id="toc_5">More pitfalls for <code>DateTime</code> data type</h1>

<ul>
<li>The 2nd argument of <code>ToString(datetime [, format string])</code> is not optional, but a must for pig 0.12 (<a href="https://issues.apache.org/jira/browse/PIG-3805">Bug fixed on 0.13</a>)</li>
<li>For my case, <code>ToString(date, &#39;yyyyMMdd&#39;)</code> works.</li>
<li>All capital function name is not accepted, like <code>TOSTRING()</code> fails.</li>
<li>Also, there is no way to directly read in the <code>DateTime</code> type to pig. It still need be read in as <code>chararray</code>, otherwise it will be just null. (I tested it). <a href="http://stackoverflow.com/a/22052965">Stackoverflow post</a></li>
<li><code>GetMilliSecond(DateTime datetime)</code> is not the function to get the millisecond from epoch. It ... en, as the name indicates ... just get the millisecond of the time. To get the <strong>second</strong> use <code>ToUnixTime(DateTime datetime)</code>. To get the <strong>millisecond</strong> use <code>ToMilliSeconds(DateTime datetime)</code>.</li>
</ul>

<h1 id="toc_6">Setting of memory</h1>

<p>In side pig script or grunt:  </p>

<pre><code class="language-bash">SET mapreduce.map.memory.mb 4096;
SET mapreduce.reduce.memory.mb 8192;
</code></pre>

<h1 id="toc_7">Pig Unit Test <code>override()</code> method</h1>

<p>Code normally like:  </p>

<pre><code class="language-java">PigTest test = new PigTest(pigScript, parameters);
test.override(&quot;alias&quot;, &quot;alias = LOAD ...&quot;);
</code></pre>

<p>Inside the override, can not use variable substitution, otherwise will see error like:  </p>

<blockquote>
<p>org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias  </p>
</blockquote>

<p>Wrong:  </p>

<pre><code class="language-java">test.override(&quot;alias&quot;, &quot;alias = LOAD &#39;$INPUT&#39; USING PigStorage() AS (...);&quot;);
</code></pre>

<p>Should always use actual file path:</p>

<pre><code class="language-java">test.override(&quot;alias&quot;, &quot;alias = LOAD &#39;input.txt&#39; USING PigStorage() AS (...);&quot;);
</code></pre>

<h1 id="toc_8">Error message for wrong reference operator</h1>

<p>If receive following error message:  </p>

<blockquote>
<p>Error: Scalar has more than one row in the output.  </p>
</blockquote>

<p>It is very likely that I got a dot <code>Alias.field</code> where you need a double semi-colon <code>Alias::field</code>.<br/><br/>
<a href="http://stackoverflow.com/questions/22522155/pig-scalar-has-more-than-one-row-in-the-output">stackoverflow post</a>  </p>

<h1 id="toc_9">Pig Java UDF Unit Test</h1>

<p>Find a good <a href="http://www.crackinghadoop.com/unit-test-java-udfs/">post</a> specifically for Java UDF unit test.<br/><br/>
The main point is to use <code>DefaultTuple</code> to inject test cases into the UDF.  </p>

<pre><code class="language-java">String inputText = &quot;09/15/2014&quot;;
DefaultTuple input = new DefaultTuple();
input.append(inputText);
</code></pre>

<h1 id="toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></h1>

<ol>
<li><code>join</code> (more specifically, inner join) acts on more than two relations:<br/>
<code>X = JOIN A BY fieldA, B BY fieldB, C BY fieldC;</code> </li>
<li><p><code>join</code> on duplicates gives cross product on the two relations.  </p>

<pre><code class="language-sql">A = LOAD &#39;data1&#39; AS (a1:int,a2:int,a3:int);
DUMP A;   
(1,2,3)
(4,2,1)
(4,3,3)

B = LOAD &#39;data2&#39; AS (b1:int,b2:int);
DUMP B;
(4,6)
(4,9)

X = JOIN A BY a1, B BY b1;
DUMP X;
(4,2,1,4,6)
(4,3,3,4,6)
(4,2,1,4,9)
(4,3,3,4,9)  
</code></pre></li>
<li><p><code>join</code> on null disregards (filters out) null values. See the <a href="https://pig.apache.org/docs/r0.11.1/basic.html#nulls_join">official document</a>.  </p></li>
<li><p><code>cogroup</code> on duplicates gives nested set of tuples for the two relations (a bag for each side of the relation). If <code>flatten</code> is then used, will generate cross product.  </p></li>
<li><p><code>cogroup</code> or <code>group</code> for <code>null</code> key, will be grouped together per relation. <a href="https://pig.apache.org/docs/r0.11.1/basic.html#nulls_group">official document</a>  </p></li>
<li><p><code>cogroup</code> or <code>group</code> for <code>null</code> value, will just keep the empty bag, unless <code>inner</code> keywords is used.  </p></li>
<li><p><code>flatten</code> of an empty bag will <font color='red'><strong>remove</strong></font> the whole record (for the cross product case of <code>flatten</code> two bags) because <code>flatten</code> an empty bag produces no output. <a href="http://datafu.incubator.apache.org/docs/datafu/guide/more-tips-and-tricks.html">Refer to this post</a> to under this further and a trick (<em>generating a bag with a null tuple</em>) to avoid this behavior for getting &#39;outer join&#39; effect.</p></li>
</ol>

<h1 id="toc_11">specify function output data type</h1>

<p>No matter whether it is builtin function or UDF, it is always better to specify this function&#39;s output data type.  </p>

<p>This is wrong, because the CONCAT output type is <code>bytearray</code>:    </p>

<pre><code class="language-sql">ip_db = FOREACH ip_db_m3 GENERATE MD5(CONCAT(ip_raw, &#39;umpdmp&#39;)) AS ip;   
</code></pre>

<p>This is right by specifying clearly that the output data type is <code>chararray</code>:  </p>

<pre><code class="language-sql">ip_db_m3 = FOREACH ip_db_m2 GENERATE CONCAT(ip_raw, &#39;umpdmp&#39;) AS ip:chararray;
ip_db = FOREACH ip_db_m3 GENERATE MD5(ip) AS ip;   
</code></pre>

<h1 id="toc_12">using <code>\u0001</code> as the delimiter</h1>

<ul>
<li>If want to use <code>LzoPigStorage</code> need be careful about in pig, to specify the parameter of the function, you cannot do it at the code where calls this function, but need do it in the <code>DEFINE</code> statement at the beginning, like below:<br/></li>
</ul>

<pre><code class="language-sql">DEFINE LzoPigStorage com.twitter.elephantbird.pig.store.LzoPigStorage(&#39;\u0001&#39;);   

store OUT into &#39;$OUTPUT&#39; using LzoPigStorage();   
</code></pre>

<ul>
<li>Note that <code>&#39;^A&#39;</code> will <strong>NOT</strong> work, Only <code>&#39;\u0001&#39;</code> works.<br/></li>
<li>If use <code>PigStorage</code> just specify it inside the function like below:<br/></li>
</ul>

<pre><code class="language-sql">A = LOAD &#39;input.txt&#39; USING PigStorage(&#39;,&#39;);   
STORE A INTO &#39;out&#39; USING PigStorage(&#39;\u0001&#39;);   
</code></pre>

<h1 id="toc_13">Error with SUBSTRING and SIZE function</h1>

<p>I was trying to remove the leading <code>.</code> of a ip string, and used <code>SUBSTRING(ip, 1, SIZE(ip)) AS ip_fixed:chararray</code>. The error message reads:  </p>

<blockquote>
<p>ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.SUBSTRING as multiple or none of them fit. Please use an explicit cast  </p>
</blockquote>

<p>The reason is because <code>SIZE()</code> function returns <code>long</code> type, but <code>SUBSTRING()</code> function needs <code>int</code> type. The following code passes: <code>SUBSTRING(ip, 1, (int) SIZE(ip)) AS ip_fixed:chararray</code>  </p>

<h1 id="toc_14">oneline to order two fields</h1>

<pre><code class="language-sql">CC_raw = LOAD &#39;$INPUT_CC&#39; USING PigStorage()
                   AS (id1:chararray, id2:chararray, score:double);
CC_m0 = FOREACH CC_raw GENERATE 
            FLATTEN(((id1 &lt; id2) ? (id1, id2) : (id2, id1))) AS (smallID, largeID),
            score AS score;
</code></pre>

<p>order the two fields <code>id1</code> and <code>id2</code>.</p>

<h1 id="toc_15">sort with duplicates</h1>

<p>sort with heavily duplicated field sometimes generate highly skewed partitions (they are partitioned based on bins, records with the same value of the sorting field will be inside the same bin). This will reduce the performance dramatically.</p>

<p>Solution is to sort with combined fields like <code>BY (f1, f2)</code> in which, <code>f1</code> is the actual field that need be sorted but with a lot of duplicates, <code>f2</code> is another field that is much more evenly distributed to help reduce the skew.</p>

<p>Thanks for MingYan :)  </p>

<h1 id="toc_16">input and output are reserved keyword in pig</h1>

<p>Do not use <code>input</code> or <code>output</code> as relations name, because they are reserved keyword. Error message is like:  </p>

<blockquote>
<p>ERROR org.apache.pig.PigServer - exception during parsing: Error during parsing. <file clean_vertex_result.pig, line 3, column 0>  mismatched input &#39;input&#39; expecting EOF</p>
</blockquote>

<h1 id="toc_17">Mystery ERROR 2999</h1>

<blockquote>
<p>ERROR 2999: Unexpected internal error. null</p>
</blockquote>

<p>One case (I wasted one full day on this!) is I used the same name for both a relation and a field!</p>

<p>Solution: add <code>_r</code> for all relation names</p>

<h1 id="toc_18">Cumsum</h1>

<p>use <code>Over, Stitch, ORDER BY</code></p>

<pre><code class="language-sql">REGISTER /home/adsymp/lib/piggybank.jar;

DEFINE OVER org.apache.pig.piggybank.evaluation.Over(&#39;long&#39;);
DEFINE STITCH org.apache.pig.piggybank.evaluation.Stitch();

output_to_LP_m0 = FOREACH (GROUP pairs_bucketized ALL) {
                        sorted = ORDER pairs_bucketized BY score_bucket DESC;
                        GENERATE FLATTEN(STITCH(sorted, OVER(sorted.tp_inbucket, &#39;sum(long)&#39;), OVER(sorted.fp_inbucket, &#39;sum(long)&#39;)));};
</code></pre>

<ul>
<li><code>STITCH</code> take bags as input, and its output cannot be directly assigned to a relation, i.e. it has to be inside a <code>FOREACH</code> statement <a href="http://stackoverflow.com/a/27798007">Stackoverflow post</a> </li>
<li><code>ORDER</code> makes the rows sorted for the <code>cumsum</code></li>
<li><code>OVER</code> does the <code>cumsum</code> <a href="https://pig.apache.org/docs/r0.12.0/api/org/apache/pig/piggybank/evaluation/Over.html">Official document</a></li>
<li><code>STITCH</code> make the <code>cumsum</code> one-on-one appending with the correct row.</li>
<li>When I tried <code>OVER</code> through <code>grunt</code>, I checked the output schema is <code>null</code>. This is not a bug! In fact, if we didn&#39;t input the string of type in the <code>Over()</code> constructor, this <code>piggybank</code> function does not know what is the output schema.</li>
</ul>

<p>One lesson learned: If it is the logic planning stage error, checking the alias.</p>

<h1 id="toc_19">A pretty good graph cluster to pair Pig example code</h1>

<p><code>/Users/yugan/repo/drawbridge/dpp/workflow-new/miaozhen-scripts/graph/Stats.pig</code></p>

<p>In this code:</p>

<ul>
<li>use of <code>TOKENIZE</code>, <code>LAST_INDEX_OF</code></li>
<li>The way of exploding graph <code>cluster</code> into pairs and get the TP/FP with label data is pretty clean and neat.</li>
<li>use <code>HashString</code> to sample</li>
</ul>

<h1 id="toc_20">A pretty good Score bucketize and cumsum precision Pig example code</h1>

<p><code>/Users/yugan/repo/drawbridge/dpp/workflow-new/miaozhen-scripts/rank/ToPrec2.pig</code></p>

<p>In this code:</p>

<ul>
<li>use <code>$SCORE_FACTOR</code> converting double to long for bucketize.</li>
<li>use of three UDF <code>GetScoreBuckets</code>, <code>AnalyzeScoreBucket</code>, and <code>GetPrecision</code>.</li>
<li>use <code>HashString</code> to sample</li>
<li>use <code>replicated</code> mode for the JOIN</li>
</ul>

<p>But I have also pretty good code using <code>Over</code>, <code>Stitch</code>, <code>ORDER BY</code> to implement cumsum as shown above.</p>

<h1 id="toc_21">Parameter Substitution in Pig</h1>

<p>Official documentation <a href="http://wiki.apache.org/pig/ParameterSubstitution">here</a></p>

<h1 id="toc_22">SUBSTRING function</h1>

<p>It is does not include the second index position<br/>
<code>SUBSTRING(string, startIndex, stopIndex)</code>, <code>startIndex</code> starts from <code>0</code>, and <code>stopIndex</code> is not included.<br/><br/>
Given a field named <code>alpha</code> whose value is <code>ABCDEF</code>, to return substring <code>BCD</code> use this statement: <code>SUBSTRING(alpha,1,4)</code>. Note that <code>1</code> is the index of <code>B</code> (the first character of the substring) and <code>4</code> is the index of <code>E</code> (the character <strong>following</strong> the last character of the substring).</p>

<p>Note that the description is different in the official documents both for Pig 0.12:<br/>
* <a href="https://pig.apache.org/docs/r0.12.0/func.html#substring">Correct official document</a> <br/>
* <a href="https://pig.apache.org/docs/r0.12.0/api/org/apache/pig/builtin/SUBSTRING.html">Wrong official document</a></p>

<h1 id="toc_23">Nested FOREACH</h1>

<ol>
<li>Allowed operation are <code>CROSS</code>, <code>DISTINCT</code>, <code>FILTER</code>, <code>FOREACH</code>, <code>LIMIT</code>, <code>ORDER BY</code>, and Project operation. <a href="http://pig.apache.org/docs/r0.15.0/basic.html#foreach">Official Document</a></li>
<li><code>SPLIT</code> does not work inside nested FOREACH</li>
</ol>

<h1 id="toc_24">Function are allowed inside FOREACH</h1>

<p><a href="http://stackoverflow.com/a/26996221/4229125">an example of using <code>SUM</code> inside FOREACH from stackoverflow</a>  </p>

<pre><code class="language-sql">file = LOAD &#39;input.txt&#39; USING PigStorage() AS (type: chararray, year: chararray,
match_count: float, volume_count: float);
grouped = GROUP file BY type;
group_operat = FOREACH grouped {
                                 sum_m = SUM(file.match_count);
                                 sum_v = SUM(file.volume_count);
                                 GENERATE group,(float)(sum_m/sum_v) as sum_mv;
                                }
</code></pre>

<p>Lesson learned: Do not use 2 levels of <code>FOREACH</code> if it is not absolutely necessary.</p>

<p>Wrong </p>

<pre><code class="language-sql">acookie_log_input = LOAD &#39;$INPUT_ACOOKIE&#39; USING LzoPigStorage()
                       AS (acookie:chararray, ip:chararray, time_stamp:DateTime, useragent:chararray, browser:chararray,
                           os:chararray, url:chararray, province:chararray, acookie_daily_tag:chararray,
                           url_catelevel1:chararray, url_catelevel2:chararray, date:DateTime);
acookie_log_input_m0 = FOREACH acookie_log_input GENERATE acookie AS acookie, SUBSTRING(province, 0, 2) AS province:chararray;
acookie_log_input_m1 = FILTER acookie_log_input_m0 BY province IS NOT NULL;
acookie_log_input_m2 = FOREACH (GROUP acookie_log_input_m1 BY (acookie, province)) GENERATE FLATTEN(group) AS (acookie, province),
                        COUNT(acookie_log_input_m1) AS acookie_province_cnt;
-- Does not work from here
acookie_province = FOREACH (GROUP acookie_log_input_m2 BY acookie) {
                    total_cnt = SUM(acookie_log_input_m2.acookie_province_cnt);
                    normalized = FOREACH acookie_log_input_m2 GENERATE (int) (acookie_province_cnt * 100 / total_cnt) AS hist:int;
                    sorted = ORDER normalized BY hist DESC;
                    GENERATE group AS acookie, BagToString(sorted.hist) AS hist:chararray;};
</code></pre>

<p>Correct</p>

<pre><code class="language-sql">acookie_province_m0 = FOREACH (GROUP acookie_log_input_m2 BY acookie) {
                    total_cnt = SUM(acookie_log_input_m2.acookie_province_cnt);
                    GENERATE group AS acookie, FLATTEN(acookie_log_input_m2.acookie_province_cnt) AS (acookie_province_cnt),
                    total_cnt AS total_cnt;};
acookie_province_m1 = FOREACH acookie_province_m0 GENERATE acookie AS acookie, (int) (acookie_province_cnt*100/total_cnt) AS hist:int;
acookie_province = FOREACH (GROUP acookie_province_m1 BY acookie) {
                    sorted = ORDER acookie_province_m1 BY hist DESC;
                    GENERATE group AS acookie, BagToString(sorted.hist) AS hist:chararray;};
</code></pre>

<h1 id="toc_25">Use Star Expression and Project-Range Expression</h1>

<p>to avoid copy all the fields again and again.<br/>
<a href="https://pig.apache.org/docs/r0.12.0/basic.html#expressions">0.12 official documentation</a></p>

<p>Star Expression: <code>*</code><br/>
Project-Range Expression: <code>..</code></p>

<p>A good example:</p>

<pre><code class="language-sql">
lkp_input = LOAD &#39;$INPUT_PAIR&#39; USING PigStorage() AS (id1:long, id2:long, c2NumPath:int,
            c2SumTot:float, c2MinTot:float, c2MaxTot:float, c2SumS1:float, c2MinS1:float, 
            c2MaxS1:float, c2SumS2:float, c2MinS2:float, c2MaxS2:float, c3NumPath:int, 
            c3NumNoRedundantPath:int, c3NumDistinctID:int, c3SumTot:float, c3MinTot:float, 
            c3MaxTot:float, c3SumS1:float, c3MinS1:float, c3MaxS1:float, c3SumS2:float, 
            c3MinS2:float, c3MaxS2:float, c3SumS3:float, c3MinS3:float, c3MaxS3:float);

-- avoid using duplicated field names!
lkp_index_table = LOAD &#39;$INPUT_INDEX_TABLE&#39; USING LzoPigStorage() 
                    AS (id:long, str_id:chararray);

lkp_str_l = FOREACH (JOIN lkp_input BY id1, lkp_index_table BY id) GENERATE
            str_id AS strId1, id2..c3MaxS3;
lkp_str = FOREACH (JOIN lkp_str_l BY id2, lkp_index_table BY id) GENERATE
            strId1 AS strId1, str_id AS strId2, c2NumPath..c3MaxS3;

</code></pre>

<h1 id="toc_26">Strange bug ERROR 2116 related with compression</h1>

<p>Error code like:</p>

<blockquote>
<p>ERROR 2116:<br/>
<file lkpAddLabelNoLzo.pig, line 38, column 0> Output Location Validation Failed for: &#39;hdfs://sc2prod/user/yu/linkprediction/processed_full/0.05_0.08_0.005_11/pairsNoLzo</p>

<p>org.apache.pig.impl.plan.VisitorException: ERROR 2116:<br/>
<file lkpAddLabelNoLzo.pig, line 38, column 0> Output Location Validation Failed for: &#39;hdfs://sc2prod/user/yu/linkprediction/processed_full/0.05_0.08_0.005_11/pairsNoLzo<br/>
        at org.apache.pig.newplan.logical.rules.InputOutputFileValidator$InputOutputFileVisitor.visit(InputOutputFileValidator.java:75)</p>
</blockquote>

<p>This error has nothing to do with the output location, but because the wrong compression setting</p>

<p>The wrong one caused the bug</p>

<pre><code class="language-sql">SET output.compression.enabled true;
</code></pre>

<p>The correct one:</p>

<pre><code class="language-sql">SET mapreduce.output.fileoutputformat.compress true;
</code></pre>

<h1 id="toc_27">Strange Bug with Split of two logic expressions</h1>

<p>The error message looks like this:</p>

<blockquote>
<p>2016-11-10 03:38:16,010 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: Split - scope-1576 Operator Key: scope-1576): org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error while executing ForEach at [null[-1,-1]]<br/>
    at ....PhysicalOperator.processInput(PhysicalOperator.java:289)<br/>
    at ....physicalLayer.relationalOperators.POSplit.getNextTuple(POSplit.java:214)<br/>
    at ....relationalOperators.POSplit.runPipeline(POSplit.java:255)<br/>
    ...<br/>
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error while executing ForEach at [null[-1,-1]]<br/>
    at ....physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:306)<br/>
    at ....PhysicalOperator.processInput(PhysicalOperator.java:281)<br/>
    ... 19 more<br/>
Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Float<br/>
    at java.lang.Float.compareTo(Float.java:50)<br/>
    at ....expressionOperators.NotEqualToExpr.doComparison(NotEqualToExpr.java:113)<br/>
    at ....expressionOperators.NotEqualToExpr.getNextBoolean(NotEqualToExpr.java:84)<br/>
    at ....expressionOperators.POAnd.getNextBoolean(POAnd.java:90)</p>
</blockquote>

<p>This error message is <strong><font color='red'>misleading</font></strong>. There is not <code>ClassCastException</code> issue, it is caused by the <code>Split</code>. I don&#39;t understand how this is happened, but <code>split</code> is merely a syntax sugar, we can just use the <code>filter</code> with the same efficiency.</p>

<p>Problem code:</p>

<pre><code class="language-sql">lkp_input = LOAD &#39;$INPUT_PAIR&#39; USING LzoPigStorage() AS (strID1:chararray, strID2:chararray, c2NumPath:int, c2SumTot:float,
                    c2MinTot:float, c2MaxTot:float, c2SumS1:float, c2MinS1:float, c2MaxS1:float, c2SumS2:float,
                    c2MinS2:float, c2MaxS2:float, c3NumPath:int, c3NumNoRedundantPath:int, c3NumDistinctID:int,
                    c3SumTot:float, c3MinTot:float, c3MaxTot:float, c3SumS1:float, c3MinS1:float, c3MaxS1:float,
                    c3SumS2:float, c3MinS2:float, c3MaxS2:float, c3SumS3:float, c3MinS3:float, c3MaxS3:float, label:int);
SPLIT lkp_input INTO
    c2_only_pairs IF ((c2NumPath != -1) AND (c3NumPath == -1)),
    c3_only_pairs IF ((c2NumPath == -1) AND (c3NumPath != -1)),
    c2_c3_pairs OTHERWISE;

c2_all_pairs = FILTER lkp_input BY (c2NumPath != -1);
c3_all_pairs = FILTER lkp_input BY (c3NumPath != -1);
</code></pre>

<p>Correct code:</p>

<pre><code class="language-sql">lkp_input = LOAD &#39;$INPUT_PAIR&#39; USING LzoPigStorage() AS (strID1:chararray, strID2:chararray, c2NumPath:int, c2SumTot:float,
                    c2MinTot:float, c2MaxTot:float, c2SumS1:float, c2MinS1:float, c2MaxS1:float, c2SumS2:float,
                    c2MinS2:float, c2MaxS2:float, c3NumPath:int, c3NumNoRedundantPath:int, c3NumDistinctID:int,
                    c3SumTot:float, c3MinTot:float, c3MaxTot:float, c3SumS1:float, c3MinS1:float, c3MaxS1:float,
                    c3SumS2:float, c3MinS2:float, c3MaxS2:float, c3SumS3:float, c3MinS3:float, c3MaxS3:float, label:int);

c2_only_pairs = FILTER lkp_input BY ((c2NumPath != -1) AND (c3NumPath == -1));
c3_only_pairs = FILTER lkp_input BY ((c2NumPath == -1) AND (c3NumPath != -1));
c2_c3_pairs = FILTER lkp_input BY ((c2NumPath != -1) AND (c3NumPath != -1));;
    
c2_all_pairs = FILTER lkp_input BY (c2NumPath != -1);
c3_all_pairs = FILTER lkp_input BY (c3NumPath != -1);
</code></pre>

<h1 id="toc_28">Use <code>AvroStorage()</code> inside pig</h1>

<p>Need register the following jars</p>

<pre><code class="language-sql">REGISTER /home/adsymp/lib/piggybank.jar;
REGISTER /home/adsymp/lib/avro-1.7.6.jar;
REGISTER /home/adsymp/lib/jackson-core-asl-1.7.3.jar;
REGISTER /home/adsymp/lib/jackson-mapper-asl-1.7.3.jar;
REGISTER /home/adsymp/lib/json-simple-1.1.jar;

DEFINE AvroStorage org.apache.pig.piggybank.storage.avro.AvroStorage(&#39;no_schema_check&#39;);
</code></pre>

<p>Also, don&#39;t mess with the <code>float</code> and <code>double</code> types with the AvroStorage input.</p>

<h1 id="toc_29">Load Parquet data to pig</h1>

<p>Need specify the schema string in the constructor</p>

<p>for example the following file:<br/>
<code>/user/oozie/pairing-modeling-gpairing/2016-12-23/cookie-cookie/mob-mob/output/part-r-00000-0eb9c76c-f170-4d4a-a20b-aa9f72d6654c.snappy.parquet</code></p>

<p>The schema of this parquet data is specified in the source code:<br/>
<code>/Users/yugan/repo/drawbridge/dpp/spark2/src/main/scala/ge/drawbrid/dpp/spark2/pairing/PairingFormat.scala</code></p>

<pre><code class="language-scala">case class PairData(id1: String, id2: String, id1Type: String, id2Type: String, pairingVersion: String,
                    id1TotalFrequency: Long, id2TotalFreqency: Long, precision: Double, rfc: Double, scores: scala.collection.Map[String, Double])
</code></pre>

<p>In order to read this data into pig we should do:</p>

<pre><code class="language-sql">A = LOAD &#39;/user/oozie/pairing-modeling-gpairing/2016-12-23/cookie-cookie/mob-mob/output/part-r-00000-0eb9c76c-f170-4d4a-a20b-aa9f72d6654c.snappy.parquet&#39; USING parquet.pig.ParquetLoader(&#39;id1:chararray, id2:chararray, id1Type:chararray, id2Type:chararray, pairingVersion:chararray, id1TotalFrequency:long, id2TotalFrequency:long, precision:double, rfc:double, scores:map[double]&#39;);
</code></pre>

<p>Don&#39;t mess up the field names and the float/double datatypes. Also pay attention to the <code>:map[double]</code> type specification (refer to the <a href="http://pig.apache.org/docs/r0.12.0/basic.html#schema-complex">pig schema documentation here</a>).</p>

<p><font color='red'><strong>The most important advantage is reading some columns out only</strong></font></p>

<pre><code class="language-sql">A = LOAD &#39;/user/oozie/pairing-modeling-gpairing/2016-12-23/cookie-cookie/mob-mob/output/part-r-00000-0eb9c76c-f170-4d4a-a20b-aa9f72d6654c.snappy.parquet&#39; USING parquet.pig.ParquetLoader(&#39;id1:chararray, id2:chararray, precision:double&#39;);
</code></pre>

<ul>
<li><strong>There is another way to load by column position</strong>
<a href="http://stackoverflow.com/a/32276119/4229125">stackoverflow post</a></li>
</ul>

<p>As specified in the <a href="https://github.com/Parquet/parquet-mr/blob/master/parquet-pig/src/main/java/parquet/pig/ParquetLoader.java#L119">source code</a>: just add a boolean string as the 2nd argument (I did not try it)</p>

<pre><code class="language-java">ParquetLoader(String requestedSchemaStr, String columnIndexAccess)

ParquetLoader(&#39;n1:int, n2:float, n3:double, n4:long&#39;, &#39;true&#39;)
</code></pre>

<ul>
<li><font color='red'><strong>read in the metadata in the pig mr-planning phase</strong></font></li>
</ul>

<p>when load parquet data into pig, all parquet files&#39; footer, i.e. metadata need be read in first (which in my case takes quite some time) <br/>
<a href="https://forums.databricks.com/questions/1097/stall-on-loading-many-parquet-files-on-s3.html">Very good reference</a><br/>
<a href="https://drill.apache.org/docs/optimizing-parquet-metadata-reading/">reference1</a><br/>
<a href="https://issues.apache.org/jira/browse/SPARK-9347">reference2</a><br/>
<a href="https://groups.google.com/forum/#!topic/parquet-dev/aQU9Q8f0bPY">reference3</a></p>

<blockquote>
<p>I might have an explanation. It appears to be because the current Spark/Hadoop versions are not designed to read thousands of tables (a handful of large/small tables instead). Before any job is actually submitted to the cluster, the driver will go through the parquet files one-by-one, reading the footers from each parquet part. If your parts are small, this is tantamount to the driver serially reading all your data from S3 before starting your job.<br/>
The driver log contains these lines:<br/>
Jun 27, 2015 8:34:23 PM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5 Jun 27, 2015 8:34:23 PM INFO: parquet.hadoop.SplitStrategy: Using Client Side Metadata Split Strategy Jun 27, 2015 8:34:24 PM INFO: parquet.hadoop.ClientSideMetadataSplitStrategy: There were no row groups that could be dropped due to filter predicates Jun 27, 2015 8:34:24 PM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 14<br/>
In addition, it appears that opening up parquet files through the newAPIHadoopFile creates a broadcast variable per file: SparkContext: Created broadcast 653 from newAPIHadoopFile at ADAMContext.scala:150<br/>
The solution would seem to be to both produce parquet files with larger/fewer parts, and to aggregate your parquet files together. Of course doing so means you might have to pay this penalty to simply run the job to aggregate to a single logical parquet file.<br/>
A _metadata files contains a copy of all the footers of all the files in same directory. To produce efficient splits and do projection push down we read the metadata on the client side. Consolidating the metadata per directory reduces the number of files to open and speeds up that process.If the _metadata file is absent we just fall back to the files footers, also if there are files are missing, we just ignore the information in the _metadata file so you can delete part files or move them around and it still works.</p>
</blockquote>

<ul>
<li><strong>Improve the speed of read in the metadata files</strong><br/>
Currently I have no good ways to generate the common metadata, the only option that I know is to increase the parquet.metadata.read.parallelism<br/></li>
</ul>

<p><a href="http://stackoverflow.com/questions/33726400/what-is-parquet-read-parallelism">stackoverflow</a></p>

<blockquote>
<p>increase ParquetFileReader parallelism from default 5 to 30 - with setting &quot;parquet.metadata.read.parallelism&quot;: &quot;30&quot; in newAPIHadoopFile config</p>
</blockquote>

<p>The way of setting it is like follow:</p>

<pre><code class="language-sql">%default BUCKET_SIZE 981

SET mapred.job.queue.name &#39;datascience&#39;;
SET parquet.metadata.read.parallelism 50;

REGISTER /home/adsymp/lib/dpp-pig-udf-0.0.1-SNAPSHOT.jar;
...
</code></pre>

<p><strong>reference</strong></p>

<ul>
<li><a href="https://groups.google.com/forum/#!topic/parquet-dev/qyqRrJ3bMew">A code snippet in a google-group&#39;s post</a></li>
</ul>

<pre><code class="language-sql">REGISTER /xxtech/scenarios/parquet-pig-bundle-1.2.4.jar
REGISTER $xxtechLocalRepo/xxtechPigExtension.jar
REGISTER $xxtechLocalRepo/lib/piggybank.jar
REGISTER $xxtechLocalRepo/lib/joda-time-1.6.jar

SET job.name parquet-store-$record-$parquet.pig;

dataImport = LOAD ‚/xxtech/$record/{%DATE%}/{%HOUR%}/{*}.avro&#39; USING com.xxtech.repo.pig.udf.storage.avro.ExtendedxxtechAvroStorage(&#39;$timestamp&#39;, &#39;-7200&#39;, &#39;missing_input_dir&#39;);

SET parquet.compression snappy
SET parquet.enable.dictionary true
SET parquet.block.size 536870912

store dataImport into &#39;/$parquet/xxtech/$record/20131020/10/$record.par&#39; using parquet.pig.ParquetStorer();

parquet.pig:

REGISTER /xxtech/scenarios/parquet-pig-bundle-1.2.4.jar

data = LOAD &#39;/parquet/xxtech/rawlogs/20131020/10/rawlogs.par/part-m-00000.snappy.parquet&#39; USING parquet.pig.ParquetLoader(&#39;SGSHeader: (VersionID: int,SequenceID: long,RecordType: int,TimeStamp: int,GMTOffset: int)&#39;);

a = GROUP data ALL;
describe a;


b = foreach a generate SUM(a.data.RecordType);
describe b;
dump b;
</code></pre>

<ul>
<li><a href="https://hadoopified.wordpress.com/2013/10/18/parquet-columnar-storage-hadoop/">A blog. By the way this is a good hadoop blog</a></li>
</ul>

<h1 id="toc_30">A side-by-side datatype comparison among avro, parquet, and pig</h1>

<p><a href="http://alvincjin.blogspot.com/2014/06/convert-same-pig-and-avro-output-schema.html">Not that useful, just a quick reference</a></p>

<h1 id="toc_31">More about the parquet</h1>

<p><strong>some useful command line tools</strong></p>

<pre><code class="language-bash"># showing schema
hadoop parquet.tools.Main schema /user/oozie/pairing-modeling-gpairing/2016-12-23/dev-cookie/dev-mob/output/part-r-28207-74d4715e-3418-4f39-9519-109c7364f939.snappy.parquet

# Check meta
hadoop parquet.tools.Main meta ...

# check head
 hadoop parquet.tools.Main head ...
</code></pre>

<p>More <a href="https://www.cloudera.com/documentation/enterprise/5-8-x/topics/cdh_ig_parquet.html#parquet_files">here</a></p>

<ul>
<li>A visual representation of parquet data strcuture</li>
</ul>

<p><img src="media/14742423250700/14827763637843.jpg" alt=""/></p>

<p><a href="http://grepalex.com/2014/05/13/parquet-file-format-and-object-model/">Original post</a></p>

<h1 id="toc_32">From one relation, put two fields into one bag by FLATTEN</h1>

<p>{A:(id1:chararray, id2:chararray)} → {B:(id:chararray)}, in which B::id is the collection of A::id1 and A::id2.</p>

<p>The following is the code that I tested and works (in hlab &gt; pig-book &gt; script &gt;  collect_one_bag.pig</p>

<pre><code>four_field = LOAD &#39;data_flatten&#39; AS (one:INT, two:INT, three:INT, four:INT);
one_field = FOREACH four_field GENERATE FLATTEN(TOBAG(one, two, three, four));
DESCRIBE one_field;
</code></pre>

<h1 id="toc_33">Learning from piggybank <code>HashFNV</code></h1>

<p>It used a technique to support different function implementations according to the input schema</p>

<p><code>HashFNV</code> has two subclasses <code>HashFNV1</code> and <code>HashFNV2</code>, in which, <code>HashFNV1</code> is used when input only has one argument, i.e. the string need be hashed, and <code>HashFNV2</code> is used when input has two arguments, i.e. the string need be hashed and the integer for the mod operation.</p>

<p>The key implementation method that is called in both the two subclasses <code>HashFNV1</code> and <code>HashFNV2</code> is actually inside <code>HashFNV</code>, and is called <code>hashFnv32(String s)</code></p>

<p>This technique of have different implementations is by overriding method <code>public List&lt;FuncSpec&gt; getArgToFuncMapping()</code>. Official document is at <a href="https://github.com/apache/pig/blob/branch-0.12/src/org/apache/pig/EvalFunc.java#L279">here</a>. And <code>HashFNV</code> is a good example of <a href="http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/string/HashFNV.java?view=markup#l77">using it</a> </p>

<p>For my case, in Spark, there is already an implementation I can use at <code>ge.drawbrid.dpp.spark2.util.Util#hashFnv32</code></p>

<p>Source code:<br/>
* <code>HashFNV</code> is at <a href="http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/string/HashFNV.java?view=markup">here</a><br/>
* <code>HashFNV1</code> is at <a href="http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/string/HashFNV1.java?view=markup">here</a><br/>
* <code>HashFNV2</code> is at <a href="http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/string/HashFNV2.java?view=markup">here</a></p>

<h1 id="toc_34">Pig type and Avro complex datatype</h1>

<p>The most frequently used Avro complex datatype in pig are:<br/>
* <code>records</code><br/>
* <code>arrays</code> </p>

<h1 id="toc_35">Specify complex data type using schema declaration</h1>

<p><img src="media/14742423250700/15080203565504.jpg" alt=""/></p>

<p>reference is from this <a href="http://anutoshdatta.blogspot.com/2013/03/data-types-and-schema-in-pig.html">post</a></p>

<h1 id="toc_36">Pig local mode load local file system data</h1>

<ol>
<li>run in local mode: <code>pig -x local</code></li>
<li>load local data, adding <code>file:///</code> and use the absolute path;</li>
</ol>

<p>If not so, possible error is:</p>

<blockquote>
<p>Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]</p>
</blockquote>

<p>Example:</p>

<pre><code class="language-sql">old = load &#39;file:///export/home/ygan/li-spark-shell/data/avroTemp/part-r-00001.avro&#39; using LiAvroStorage();
</code></pre>

<h1 id="toc_37">Pig, Avro, Python UDF <code>float</code> and <code>double</code> type</h1>

<p>The root cause is that in python, there is no <code>java</code> equivalent <code>float</code> type. Any python UDF returned <code>float</code> value, is actually <code>double</code> in Pig</p>

<p>Refer to this <a href="https://issues.apache.org/jira/browse/PIG-3357">post</a><br/>
<code>java<br/>
} else if (pyObject instanceof PyFloat) {<br/>
   // J(P)ython is loosely typed, supports only float type, <br/>
   // hence we convert everything to double to save precision<br/>
   javaObj = pyObject.__tojava__(Double.class);<br/>
}<br/>
</code></p>

<p>So, in order to get the <code>float</code> type inside pig or saved in avro format, we need do explicit cast <code>(float)</code> inside pig.</p>

<p>Cast inside complex type by redefine the datatype doest not work in pig:</p>

<p><strong><font color='red'>not working</font></strong></p>

<pre><code class="language-sql">history_features_temp0 = foreach (group cnt_type_member by memberId) generate group as memberId, 
                            intentionUtils.getMemberHistoryFeatures(cnt_type_member.(historyType, cntPerTypePerMember)) 
                                as memberHistoryFeatures : bag{t:(featureName: chararray, featureValue: float)};

</code></pre>

<p>in which the output of python udf: <code>intentionUtils.getMemberHistoryFeatures</code>:</p>

<pre><code class="language-python">@outputSchema(&#39;memberHistoryFeatures:{t:(featureName:chararray, featureValue:float)}&#39;)`

</code></pre>

<p><strong><font color='red'>working</font></strong></p>

<pre><code class="language-sql">history_features_temp0 = foreach (group cnt_type_member by memberId) generate group as memberId,
                            flatten(intentionUtils.getMemberHistoryFeatures(
                              cnt_type_member.(historyType, cntPerTypePerMember)))
                              as (featureName: chararray, featureValue: double);
history_features = foreach history_features_temp0 generate memberId as memberId, featureName as featureName,
                      (float) featureValue as featureValue: float;

</code></pre>

<p>Need un-nest the complex dataType then explicit cast.</p>

<h1 id="toc_38">Config memory of Mapper &amp; Reducer in Yarn</h1>

<p>Based on a really good post <a href="https://documentation.altiscale.com/heapsize-for-mappers-and-reducers">here</a></p>

<p>There are three memory related parameters:<br/>
1. <code>yarn.scheduler.maximum-allocation-mb</code>: The total memory allocated for yarn by Node Manager. Normally it is the majority part of the machine&#39;s RAM. eg. 244GB<br/>
2. <code>mapreduce.{map|reduce}.memory.mb</code>: Used by YARN to set the memory size of the container being used to run the map or reduce task. If the task grows beyond this limit, YARN will kill the container<br/>
3. <code>mapreduce.{map|reduce}.java.opts</code>: To execute the actual map or reduce task, YARN will run a JVM within the container. This parameter is intended to pass options to this JVM. This can include <code>­Xmx</code> to set max heap size of the JVM. However, it is limited by the actual size of the container as set by <code>mapreduce.{map|reduce}.memory.mb</code>.<br/>
4. A good rule of thumb is to set the jave heap size to be 10% less than the container size<br/>
    <code>mapreduce.{map|reduce}.java.opts = mapreduce.{map|reduce}.memory.mb x 0.9</code><br/>
5. At here, we see that default value: <code>mapreduce.map.memory.mb=2048</code>, and <code>mapreduce.map.java.opts=-Xmx1G</code></p>

<p>Note: The two properties <code>yarn.nodemanager.resource.memory-mb</code> and <code>yarn.scheduler.maximum-allocation-mb</code> normally cannot be set by customers. </p>

<p><img src="media/14742423250700/15083447009513.jpg" alt=""/></p>

<p>Example of setting:</p>

<pre><code class="language-bash">hadoop jar &lt;jarName&gt; -Dmapreduce.map.memory.mb=4096 -Dmapreduce.map.java.opts=-Xmx3686m
</code></pre>

<h1 id="toc_39">Pass a specific hadoop config parameter to Pig</h1>

<p>There are multiple places you can pass hadoop configuration parameter to Pig. Here is a list from high priority to low priority (configuration in high priority will override the configuration in low priority):<br/>
1. set command<br/>
2. -P properties_file<br/>
3. pig.properties<br/>
4. java system property/environmental variable<br/>
5. Hadoop configuration file: hadoop-site.xml/core-site.xml/hdfs-site.xml/mapred-site.xml, or Pig specific hadoop configuration file: pig-cluster-hadoop-site.xml)</p>

<p>Both 3 and 5 require the configuration file in classpath.</p>

<p><a href="https://cwiki.apache.org/confluence/display/PIG/FAQ#FAQ-Q:HowcanIpassaspecifichadoopconfigurationparametertoPig?">Reference from the official doc</a></p>

<h1 id="toc_40">Pig Set command</h1>

<p><a href="https://pig.apache.org/docs/r0.15.0/cmds.html#set">Official doc</a><br/>
All Pig and Hadoop properties can be set, either in the Pig script or via the Grunt command line.</p>

<p>Example of correct setting inside pig script:</p>

<pre><code class="language-sql">-- need this to increase 3rd application&#39;s mapper time (too short)
-- this should also help increase the reducer time (too short)
set pig.maxCombinedSplitSize 536870912;
-- need this to reduce Mapper Spill
set mapreduce.task.io.sort.mb &#39;1024&#39;;
set mapreduce.map.memory.mb &#39;4096&#39;;
set mapreduce.map.java.opts &#39;-Xmx3686m&#39;;
</code></pre>

<h1 id="toc_41">Pig Spill</h1>

<p>Two methods<br/>
1. Increase the size of in-memory sort buffer <code>mapreduce.task.io.sort.mb</code> (set as above), default 100M. But <strong><font color='red'>NOTE: it can only go up to 2047MB</font></strong>. <code>mapreduce.task.io.sort.mb</code> is the total amount of buffer memory to use while sorting files, in megabytes. Reference: <a href="https://stackoverflow.com/a/45370437">stackoverflow</a><br/>
2. Compress mapper output (set <code>mapreduce.map.output.compress</code> and <code>mapreduce.map.output.compress.codec</code>). Example of my setting in the gradle Hadoop DSL:</p>

<pre><code class="language-groovy">propertySet(&#39;pig-common&#39;) {
  set properties: [
    &#39;pig.additional.jars&#39;: [
      &#39;./lib/daily-member-feature-pipeline*.jar&#39;
    ].join(&#39;:&#39;),
  &#39;pig.home&#39; : &#39;/export/apps/pig/linkedin-pig-h2-0.15.0.li23-1&#39;,
  ]
  set jvmProperties: [&#39;fs.permissions.umask-mode&#39;                       : &#39;022&#39;,
                      &#39;mapreduce.map.env&#39;                               : &#39;JYTHONPATH=job.jar/Lib&#39;,
                      &#39;mapreduce.reduce.env&#39;                            : &#39;JYTHONPATH=job.jar/Lib&#39;,
                      &#39;mapreduce.map.java.opts&#39;                         : &#39;-Xmx1G&#39;,
                      &#39;mapreduce.reduce.java.opts&#39;                      : &#39;-Xmx1G&#39;,
                      &#39;java.io.tmpdir&#39;                                  : &#39;/grid/a/mapred/tmp&#39;,
                      &#39;mapreduce.map.output.compress&#39;                   : true,
                      &#39;mapreduce.output.fileoutputformat.compress&#39;      : true,
                      &#39;mapreduce.output.fileoutputformat.compress.type&#39; : &#39;BLOCK&#39;,
                      &#39;mapreduce.output.fileoutputformat.compress.codec&#39;: &#39;org.apache.hadoop.io.compress.DefaultCodec&#39;,
                      &#39;avro.mapred.deflate.level&#39;                       : 6,
                      &#39;pig.tmpfilecompression&#39;                          : true,
                      &#39;pig.tmpfilecompression.codec&#39;                    : &#39;gz&#39;,]
}
</code></pre>

<h1 id="toc_42">Pig operator causes reduce phase and use of <code>parallel</code> clause</h1>

<p>The operations starts a reduce phase:<br/>
COGROUP, CROSS, DISTINCT, GROUP, JOIN (inner), JOIN (outer), and ORDER.<br/>
See reference <a href="https://pig.apache.org/docs/r0.7.0/cookbook.html#Use+the+PARALLEL+Clause">here</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">File Name Pattern Substitution</a>
</li>
<li>
<a href="#toc_1"><code>ToDate(milliseconds)</code> gives wrong date</a>
</li>
<li>
<a href="#toc_2">DateTime SimpleDateFormat</a>
</li>
<li>
<a href="#toc_3">Parameter substitution Shell Runner subtle bug</a>
</li>
<li>
<a href="#toc_4">Becareful of <code>DateTime</code> data type in pig</a>
</li>
<li>
<a href="#toc_5">More pitfalls for <code>DateTime</code> data type</a>
</li>
<li>
<a href="#toc_6">Setting of memory</a>
</li>
<li>
<a href="#toc_7">Pig Unit Test <code>override()</code> method</a>
</li>
<li>
<a href="#toc_8">Error message for wrong reference operator</a>
</li>
<li>
<a href="#toc_9">Pig Java UDF Unit Test</a>
</li>
<li>
<a href="#toc_10">Duplicates and <code>null</code> behavior for <code>cogroup + flatten</code> and <code>join</code></a>
</li>
<li>
<a href="#toc_11">specify function output data type</a>
</li>
<li>
<a href="#toc_12">using <code>u0001</code> as the delimiter</a>
</li>
<li>
<a href="#toc_13">Error with SUBSTRING and SIZE function</a>
</li>
<li>
<a href="#toc_14">oneline to order two fields</a>
</li>
<li>
<a href="#toc_15">sort with duplicates</a>
</li>
<li>
<a href="#toc_16">input and output are reserved keyword in pig</a>
</li>
<li>
<a href="#toc_17">Mystery ERROR 2999</a>
</li>
<li>
<a href="#toc_18">Cumsum</a>
</li>
<li>
<a href="#toc_19">A pretty good graph cluster to pair Pig example code</a>
</li>
<li>
<a href="#toc_20">A pretty good Score bucketize and cumsum precision Pig example code</a>
</li>
<li>
<a href="#toc_21">Parameter Substitution in Pig</a>
</li>
<li>
<a href="#toc_22">SUBSTRING function</a>
</li>
<li>
<a href="#toc_23">Nested FOREACH</a>
</li>
<li>
<a href="#toc_24">Function are allowed inside FOREACH</a>
</li>
<li>
<a href="#toc_25">Use Star Expression and Project-Range Expression</a>
</li>
<li>
<a href="#toc_26">Strange bug ERROR 2116 related with compression</a>
</li>
<li>
<a href="#toc_27">Strange Bug with Split of two logic expressions</a>
</li>
<li>
<a href="#toc_28">Use <code>AvroStorage()</code> inside pig</a>
</li>
<li>
<a href="#toc_29">Load Parquet data to pig</a>
</li>
<li>
<a href="#toc_30">A side-by-side datatype comparison among avro, parquet, and pig</a>
</li>
<li>
<a href="#toc_31">More about the parquet</a>
</li>
<li>
<a href="#toc_32">From one relation, put two fields into one bag by FLATTEN</a>
</li>
<li>
<a href="#toc_33">Learning from piggybank <code>HashFNV</code></a>
</li>
<li>
<a href="#toc_34">Pig type and Avro complex datatype</a>
</li>
<li>
<a href="#toc_35">Specify complex data type using schema declaration</a>
</li>
<li>
<a href="#toc_36">Pig local mode load local file system data</a>
</li>
<li>
<a href="#toc_37">Pig, Avro, Python UDF <code>float</code> and <code>double</code> type</a>
</li>
<li>
<a href="#toc_38">Config memory of Mapper &amp; Reducer in Yarn</a>
</li>
<li>
<a href="#toc_39">Pass a specific hadoop config parameter to Pig</a>
</li>
<li>
<a href="#toc_40">Pig Set command</a>
</li>
<li>
<a href="#toc_41">Pig Spill</a>
</li>
<li>
<a href="#toc_42">Pig operator causes reduce phase and use of <code>parallel</code> clause</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[python]]></title>
    <link href="www.echo-ohce.com/14742423250719.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250719.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">shell expansion insdie python with  <code>subprocess.call</code></a>
</li>
<li>
<a href="#toc_1">plotly using LaTex</a>
</li>
<li>
<a href="#toc_2">plotly cufflinks pandas dataframe customize the plot</a>
</li>
<li>
<a href="#toc_3">timezone</a>
</li>
<li>
<a href="#toc_4">From String to timestamp to epoch unix time</a>
</li>
<li>
<a href="#toc_5">datetime(python), pd.Timestamp(pandas), np.datetime64(numpy)</a>
</li>
<li>
<a href="#toc_6">To make a linspace of pd.Timestamp</a>
</li>
<li>
<a href="#toc_7">plotly tick setting: string format, number of ticks and location of ticks.</a>
</li>
<li>
<a href="#toc_8">plotly datetime type tick setting</a>
</li>
<li>
<a href="#toc_9">plotly change the padding size between subplots</a>
</li>
<li>
<a href="#toc_10">read in gz / gzip file</a>
</li>
<li>
<a href="#toc_11">plotly bar chart, use whether value is negative or positive to determine its color</a>
</li>
<li>
<a href="#toc_12">plotly subplot shared axis setting</a>
</li>
<li>
<a href="#toc_13">plotly remove missed dates from time series plot</a>
</li>
<li>
<a href="#toc_14">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</a>
</li>
<li>
<a href="#toc_15">Use <code>:</code> or <code>slice(None)</code></a>
</li>
<li>
<a href="#toc_16">String for pandas datetime period and frequency</a>
</li>
<li>
<a href="#toc_17">Python Library for Probabilistic Graphical Models</a>
</li>
<li>
<a href="#toc_18">Python blocking and non-blocking subprocess calls</a>
</li>
<li>
<a href="#toc_19">nohup python that includes a nohup shell script</a>
</li>
<li>
<a href="#toc_20">ipython jupyter notebook pandas showing options</a>
</li>
<li>
<a href="#toc_21">matplotlib style and colors</a>
</li>
<li>
<a href="#toc_22">matplotlib backend choices</a>
</li>
<li>
<a href="#toc_23">matplotlib with mpld3 <code>twinx</code> two side axes issue</a>
</li>
<li>
<a href="#toc_24">Conda install python package CondaHTTPError Error</a>
</li>
<li>
<a href="#toc_25">utf8 unicode in string formatter &amp; file read/write</a>
</li>
<li>
<a href="#toc_26">Use a relative path in a python module</a>
</li>
<li>
<a href="#toc_27">Mysterious Error pandas groupby error</a>
</li>
<li>
<a href="#toc_28">matplotlib add axes (subplot) on a made figure and sharex</a>
</li>
<li>
<a href="#toc_29">python parse webpage static content</a>
</li>
<li>
<a href="#toc_30">python get and parse webpage dynamic content</a>
</li>
<li>
<a href="#toc_31">python with statement and context manager type</a>
</li>
<li>
<a href="#toc_32">python selenium selector find element by multiple class names</a>
</li>
<li>
<a href="#toc_33">Speed up selenium and Time out</a>
</li>
<li>
<a href="#toc_34">An example of using regex parse table</a>
</li>
<li>
<a href="#toc_35">start and shutdown the selenium-server</a>
</li>
<li>
<a href="#toc_36">phantomJS options</a>
</li>
<li>
<a href="#toc_37">Pycharm unittest not finding the module path</a>
</li>
<li>
<a href="#toc_38">Conda rename environment</a>
</li>
<li>
<a href="#toc_39">Deploy an egg package to distributed nodes on hadoop</a>
</li>
<li>
<a href="#toc_40">numpy broadcasting</a>
</li>
<li>
<a href="#toc_41">Install matplotlib on Mac</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">shell expansion insdie python with  <code>subprocess.call</code></h1>

<p>For python older than 2.7 (like the default on on DB cluster, but we can change the shebang to specify 2.7), we can only use <code>subprocess.call()</code>.<br/><br/>
when using shell expansion with this command, need have <code>shell=True</code>. Better way to have shell expansion inside python is to use module <code>glob</code><br/><br/>
But the following works, because it is not the shell expansion but the <code>imagemagick</code> does the expansion.  </p>

<pre><code class="language-python">subprocess.call([&#39;convert&#39;, &#39;-delay&#39;, &#39;10&#39;, &#39;-loop&#39;, &#39;10&#39;, &#39;temp_1*.png&#39;, &#39;cluster_size_anim.gif&#39;])
</code></pre>

<h1 id="toc_1">plotly using LaTex</h1>

<p>It is not possible to use LaTex in offline mode. (don&#39;t waste time to figure out a way to do that anymore, maybe wait for newer update)<br/><br/>
It works when using the online mode.</p>

<h1 id="toc_2">plotly cufflinks pandas dataframe customize the plot</h1>

<ol>
<li>using the <code>asFigure=True</code> option to get the handle of the figure</li>
<li>using <code>update</code> method to update the layout python dictionary</li>
<li>using <code>po.iplot()</code> to plot the figure</li>
</ol>

<p>Example:  </p>

<pre><code class="language-python">figure = data.iplot(kind=&#39;scatter&#39;, fill=True, asFigure=True)
figure.layout.update(xaxis=dict(nticks=len(data.asfreq(&#39;M&#39;).index)))
po.iplot(figure)
</code></pre>

<h1 id="toc_3">timezone</h1>

<p>A classical scenario is get timestamp in UTC, want to convert to local timezone. Panda can do it for time series</p>

<pre><code class="language-python">ts2 = ts1.
        tz_localize(&#39;UTC&#39;).
        tz_convert(&#39;US/Pacific&#39;)
</code></pre>

<p>the <code>tz_localize(&#39;UTC&#39;)</code> make it timezone aware, the <code>tz_convert(&#39;US/Pacific&#39;)</code> convert it to local timezone.</p>

<h1 id="toc_4">From String to timestamp to epoch unix time</h1>

<p>Use <code>pd.Timestamp()</code> to convert string into pandas timestamp, then use <code>value</code> to epoch unix time in nanosecond</p>

<pre><code class="language-python">pd.Timestamp(&#39;2002-01-01&#39;).value/1e6
# this convert to epoch microseconds
</code></pre>

<h1 id="toc_5">datetime(python), pd.Timestamp(pandas), np.datetime64(numpy)</h1>

<p>At least between python and pandas is easy:</p>

<pre><code class="language-python">from dateutil import parser as dateparser
from datetime import datetime

dt = dateparser.parse(&#39;2015-01-01&#39;) # python datetime
dt2 = datetime(2015, 1, 1)          # Also python datetime
ts = pd.to_datetime(dt)             # pandas Timestamp
dt = ts.to_pydatetime()             # back to python datetime
</code></pre>

<p>A very good graph to show the converting relationship:</p>

<p><img src="media/14742423250719/14757939209171.jpg" alt=""/></p>

<h1 id="toc_6">To make a linspace of pd.Timestamp</h1>

<p>The <code>pd.date_range(start=None, end=None, periods=None, freq=&#39;D&#39;)</code> method will not work. This method is for different purpose.</p>

<pre><code class="language-python">rng = pd.date_range(&#39;1/1/2011&#39;, periods=72, freq=&#39;H&#39;)

# the same as pd.date_range(start, end, freq=&#39;D&#39;)
rng = pd.date_range(start, end)
</code></pre>

<p>To make the linespace of pd.Timestamp:<br/>
1. convert pd.Timestamp to long type<br/>
2. use np.linspace to make the array of long<br/>
3. convert back to pd.Timestamp array<br/>
4. Special note: If the original timestamp has timezone info, the step 3 need use different method to restore the timezone info</p>

<pre><code class="language-python"># the index of memory_df dataframe is of type pd.Timestamp
# the ticks_array is what we want linspace array of pd.Timestamp
# this is for cases without timezone information
ticks_array = pd.to_datetime(np.linspace(start=memory_df.index[0].value, 
                                         stop=memory_df.index[-1].value, 
                                         num=n_ticks, 
                                         endpoint=True))
                                         
# if need keep timezone info, to_datetime() method is not good anymore.
# for example the index of memory_df dataframe has timezone info
memory_df.index = memory_df.index.tz_localize(&#39;UTC&#39;).tz_convert(&#39;US/Pacific&#39;)

ticks_array = [pd.Timestamp(timetick, tz=memory_df.index.tz) for timetick in 
               (np.linspace(start=memory_df.index[0].value,
                            stop=memory_df.index[-1].value,
                            num=n_ticks, endpoint=True))]
</code></pre>

<h1 id="toc_7">plotly tick setting: string format, number of ticks and location of ticks.</h1>

<ul>
<li>The easiest way to set <code>nticks</code> and <code>tickformat</code>. Like the following example:</li>
</ul>

<pre><code class="language-python">plot_layout = go.Layout(xaxis=dict(nticks=5,
                                   tickformat=&#39;.2f&#39;))
</code></pre>

<ul>
<li>The hard way is to set <code>tickmode=&#39;array&#39;, ticktext=[...], tickvals=[...]</code>. </li>
</ul>

<h1 id="toc_8">plotly datetime type tick setting</h1>

<p>directly use <code>datetime</code> datatype and the plotly tick setting options have subtle bugs, especially when dealing with timezone awared data.</p>

<p>The workaround:<br/>
1. convert the index to string using <code>strftime</code> (these string need to have high enough resolution like <code>&#39;%H:%M:%S:%f&#39;</code>, otherwise, the plot will loose resolution because of having the same xaxis values)<br/>
2. generating <code>ticktext</code> and <code>tickvals</code>,specifing any format you like for <code>ticktext</code>.  </p>

<p>Look the below example:  </p>

<pre><code class="language-python">def visualization(job_id, basepath=&#39;.&#39;):
    memory_file = os.sep.join([basepath, &#39;memory_usage_&#39;+job_id+&#39;.csv&#39;])
    
    memory_df = pd.read_csv(memory_file)
    timestamp = pd.to_datetime(memory_df.timestamp, infer_datetime_format=True)
    memory_df = memory_df.set_index(timestamp).drop(&#39;timestamp&#39;, axis=1)
    memory_df=memory_df/float(1024*1024*1024*1024)
    memory_df.index = memory_df.index.tz_localize(&#39;UTC&#39;).tz_convert(&#39;US/Pacific&#39;)
    
    # change the index to string for plotting purpose
    memory_plot_df = memory_df.copy()
    memory_plot_df.index = pd.Series(data=[ts.strftime(&#39;%H:%M:%S:%f&#39;) for ts in memory_plot_df.index],name=&#39;time&#39;)
    mem_fig = memory_plot_df.iplot(asFigure=True)
    
    # set the xaxis ticks
    n_ticks = 10
    picked_tick_index = ((np.linspace(start=0, stop=len(memory_plot_df.index)-1, num=n_ticks, endpoint=True))
                        .astype(int))
    ticktext=[&#39;:&#39;.join(ts.split(&#39;:&#39;)[:2]) for ts in memory_plot_df.index[picked_tick_index]] # only take hour and mins
    tickvals=memory_plot_df.index[picked_tick_index]
    mem_fig.layout.update(title=&#39;Memory usage for &#39;+job_id,
                          yaxis=dict(title=&#39;TB&#39;),
                          xaxis=dict(title=&#39;time&#39;,
                                     tickmode=&#39;array&#39;,
                                     ticktext=ticktext,
                                     tickvals=tickvals))

    po.iplot(mem_fig, show_link=False)
</code></pre>

<h1 id="toc_9">plotly change the padding size between subplots</h1>

<p>Use <code>specs</code> and <code>vertical_spacing</code> or <code>horizontal_spacing</code> inside <code>tls.make_subplots()</code></p>

<p>Example:  </p>

<pre><code class="language-python">import plotly.tools as tls

figure = tls.make_subplots(rows=2, cols=1, shared_xaxes=True,
                           specs=[[{}], [{}]],
                           vertical_spacing=0.01)
</code></pre>

<h1 id="toc_10">read in gz / gzip file</h1>

<pre><code class="language-python">import gzip

with gzip.open(file_path, &#39;rb&#39;) as f:
    for line in f:
</code></pre>

<h1 id="toc_11">plotly bar chart, use whether value is negative or positive to determine its color</h1>

<pre><code class="language-python"># here sh_MACD.ch is a time series, with values can be positive or negative.
# want to generate the bar chart that is red if its value is positive, or green if negative

colors=(sh_MACD.ch&gt;0).map({True:&#39;red&#39;, False:&#39;green&#39;}).values
trace_macd = go.Bar(x=sh_MACD.index, y=sh_MACD.ch, name = &#39;MACD&#39;,
                    marker=dict(color=colors))
</code></pre>

<p>Key points:<br/>
* use map to convert logical array to the color literals<br/>
* use values to get np.ndarray not pd.Series (because plotly need np.ndarray)<br/>
* use <code>marker</code> filed</p>

<h1 id="toc_12">plotly subplot shared axis setting</h1>

<p>For example, if the subplot is 2 row 1 col, with shared x axis, the axis&#39; names are: [(1,1) x1,y1 ], [(2,1) x1,y2 ]. To set the axis properties in the <code>layout</code> dictionary, using <code>xaxis1</code> instead of <code>xaxis</code>.</p>

<p>The following example is from <code>01-Project/01-Study/PersonalProject/EDA.ipynb</code></p>

<pre><code class="language-python">overlay_plot[&#39;layout&#39;].update(xaxis1=dict(type=&#39;category&#39;))
</code></pre>

<h1 id="toc_13">plotly remove missed dates from time series plot</h1>

<p>change the xaxis&#39; type to <code>category</code>. (refer to the example above)</p>

<h1 id="toc_14">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</h1>

<pre><code>In [65]: df
Out[65]: 
                     A         B         C
first second                              
bar   one     0.895717  0.410835 -1.413681
      two     0.805244  0.813850  1.607920
baz   one    -1.206412  0.132003  1.024180
      two     2.565646 -0.827317  0.569605
foo   one     1.431256 -0.076467  0.875906
      two     1.340309 -1.187678 -2.211372
qux   one    -1.170299  1.130127  0.974466
      two    -0.226169 -1.436737 -2.006747
</code></pre>

<p>select all <code>two</code> data. Two methods:</p>

<ul>
<li>MultiIndex is tuple, using <code>loc</code> and tuple to select, and use <code>slice(None)</code> inside tuple to represent <code>:</code> that we normally use (because <code>:</code> is not recognized inside tuple). <font color='red'> The dataframe MultiIndex must be sorted first.</font></li>
</ul>

<pre><code class="language-python">df=df.sort_index()
selection = df.loc[(slice(None), &#39;two&#39;), :]
</code></pre>

<ul>
<li>Use <code>xs</code> method and use <code>level</code> argument to specify the level. (note that <code>xs</code> by default works on rows, if want to use it on columns, use the <code>axis=1</code> argument)<br/></li>
</ul>

<pre><code class="language-python">selection = df.xs(&#39;two&#39;, level=&#39;second&#39;)
</code></pre>

<h1 id="toc_15">Use <code>:</code> or <code>slice(None)</code></h1>

<p>They mean the same thing, normally in <code>[ ]</code>, we just use <code>:</code>, like <code>[5, :]</code><br/>
However, inside tuple, <code>:</code> is not recognized, we have to use <code>slice(None)</code> instead.</p>

<h1 id="toc_16">String for pandas datetime period and frequency</h1>

<p>Useful official documents</p>

<ul>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases">frequency strings</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offsets">Anchored offset frequency strings</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offset-semantics">Rolling forward or backward to snap on the anchors</a></li>
</ul>

<h1 id="toc_17">Python Library for Probabilistic Graphical Models</h1>

<p><a href="https://github.com/pgmpy/pgmpy">github link</a></p>

<h1 id="toc_18">Python blocking and non-blocking subprocess calls</h1>

<p><code>Popen</code> is nonblocking; <code>call</code> and <code>check_call</code> are blocking.<br/><br/>
<a href="http://stackoverflow.com/a/21936682/4229125">A good stackoverflow post explaining this</a></p>

<h1 id="toc_19">nohup python that includes a nohup shell script</h1>

<ul>
<li>when using nohup with python, the output will be buffered, in order to see the output in real time using <code>python -u</code> option to turn off the output buffering</li>
</ul>

<pre><code class="language-bash"># calling the nohup python script like this:
nohup python -u subprocess_test.py 1&gt;py_nohup.out 2&gt;&amp;1 &amp;
</code></pre>

<ul>
<li>Do not use <code>nohup</code> inside the python script for calling a shell script. Instead using the <code>Popen</code> with <code>shell=True</code> option, and redirects the stdout to a file.</li>
</ul>

<p>The following example is located at <code>Dropbox/01-Project/01-Study/playground/py_subprocess_call</code></p>

<p>The python script <code>subprocess_test.py</code>:</p>

<pre><code class="language-python">#!/usr/local/bin/python

from subprocess import Popen;
from time import sleep, strftime, localtime;

Popen(&#39;./keep_logging.sh 1&gt;shell_logging_out 2&gt;&amp;1&#39;, shell=True)

for i in range(20):
    print &#39;python logging {:d} time&#39;.format(i)
    print strftime(&quot;%a %b %d %H:%M:%S PDT %Y&quot;, localtime())
    sleep(3)
</code></pre>

<p>The called shell script inside python is <code>keep_logging.sh</code>: </p>

<pre><code class="language-bash">for ((c=1;c&lt;=20 ;c++ ))
do
        echo &quot;shell logging $c times&quot;
        date
        sleep 3
done
</code></pre>

<p>The command of running the python script (in turn running the shell script) is:</p>

<pre><code class="language-bash">nohup python -u subprocess_test.py 1&gt;py_nohup.out 2&gt;&amp;1 &amp;
</code></pre>

<p>Using <code>tail -f</code> for both logging file <code>shell_logging_out</code> and <code>py_nohup.out</code> shows they are <strong>non-blocking</strong> and the output are in real time.</p>

<h1 id="toc_20">ipython jupyter notebook pandas showing options</h1>

<p><a href="http://pandas.pydata.org/pandas-docs/stable/options.html">official documents</a></p>

<ul>
<li>set jupyter notebook show float in dataframe with 2 digits precision (globally)<br/>
<code>pd.options.display.float_format = &#39;{:.2f}&#39;.format</code> <a href="http://stackoverflow.com/a/31671975/4229125">stackoverflow reference</a></li>
<li>get/set number of dataframe rows to show in notebook<br/>
<code>pd.get_option(&quot;display.max_rows&quot;)</code><br/>
<code>pd.set_option(&quot;display.max_rows&quot;,999)</code><br/></li>
</ul>

<h1 id="toc_21">matplotlib style and colors</h1>

<ul>
<li><a href="https://tonysyu.github.io/raw_content/matplotlib-style-gallery/gallery.html">Here</a> is a good post for showing how all the matplotlib styles look like.</li>
<li>The recommended way of choosing color is through <code>seaborn</code>. <a href="http://seaborn.pydata.org/tutorial/color_palettes.html">The office document</a> which is very useful.</li>
<li>To temporarily change the matplotlib&#39;s default <a href="http://seaborn.pydata.org/tutorial/color_palettes.html#palette-contexts">color palette</a> and <a href="http://matplotlib.org/users/style_sheets.html#temporary-styling">style</a>, use <code>with</code>:<br/></li>
</ul>

<pre><code class="language-python">
with sns.color_palette(&quot;PuBuGn_d&quot;):
    # generate some plots with this color_palette
    
with plt.style.context((&#39;ggplot&#39;)):
    # generate some plots using the ggplot style  
    
</code></pre>

<h1 id="toc_22">matplotlib backend choices</h1>

<p>If a separate figure window needed (not the jupyter embedded window) use <code>Qt5Agg</code> as:</p>

<pre><code class="language-python">from matplotlib import pyplot as plt

plt.switch_backend(&#39;Qt5Agg&#39;)
</code></pre>

<p>If want to see the figure embedded inside the jupyter notebook, use magic line will be good enough:</p>

<pre><code class="language-python">%matplotlib notebook
</code></pre>

<h1 id="toc_23">matplotlib with mpld3 <code>twinx</code> two side axes issue</h1>

<p><code>mpld3</code> can correctly convert matplotlib twinx figs, plotly cannot</p>

<pre><code class="language-python">import matplotlib as mpl
from matplotlib import pyplot as plt
import mpld3
import plotly.tools as tls
import plotly.offline as po

%matplotlib notebook

fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot([1,3,2], &#39;b-&#39;)
ax2.plot([16, 4, 1], &#39;r:&#39;)
ax.set_ylabel(&#39;y1&#39;)
ax2.set_ylabel(&#39;y2&#39;)
ax2.patch.set_alpha(0.0) # This is the main trick for showing two axes
ax.set_xlabel(&#39;x&#39;)
fig.show()

###
mpld3.display(fig)

###
po.init_notebook_mode()
plotly_fig = tls.mpl_to_plotly(fig)
po.iplot(plotly_fig, show_link=False)
</code></pre>

<h1 id="toc_24">Conda install python package CondaHTTPError Error</h1>

<p>Mysterious error like:</p>

<blockquote>
<p>CondaHTTPError: HTTP 404 None<br/>
for url &lt;None&gt;<br/>
The remote server could not find the channel you requested.</p>
</blockquote>

<p>The reason is that some package installed previous changed the <code>~/.condarc</code> setting and added a channel, which <a href="http://conda.pydata.org/docs/config.html?highlight=channel%20url#channel-locations-channels">&quot;Listing channel locations in the .condarc file will override conda defaults&quot;</a></p>

<p>the modified <code>~/.condarc</code> that cause problem:  </p>

<pre><code class="language-bash">channels:
  - https://repo.continuum.io/pkgs/
  - defaults
</code></pre>

<p>just need delete the extra line, make it look like</p>

<pre><code class="language-bash">channels:
  - defaults
</code></pre>

<p>Problem solved.</p>

<h1 id="toc_25">utf8 unicode in string formatter &amp; file read/write</h1>

<ul>
<li>Use the <code>io</code> package for utf8 file read/write</li>
</ul>

<pre><code class="language-python">import io
with io.open(filename,&#39;r&#39;,encoding=&#39;utf8&#39;) as f:
    text = f.read()
# process Unicode text
with io.open(filename,&#39;w&#39;,encoding=&#39;utf8&#39;) as f:
    f.write(text)
</code></pre>

<p><a href="http://stackoverflow.com/a/19591815/4229125">Stackoverflow post</a></p>

<ul>
<li>Use <code>u</code> for both strings in string formatter</li>
</ul>

<pre><code class="language-python">s = u&#39;\u2265&#39;

# UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\u2265&#39; in position 0...
print &#39;{0}&#39;.format(s)

# Correct
print u&#39;{0}&#39;.format(s)
</code></pre>

<ul>
<li>To make the code itself utf8 (such as including chinese characters inside the code), add this line at the beginning of the python file</li>
</ul>

<pre><code class="language-python"># -*- coding: utf-8 -*-
</code></pre>

<h1 id="toc_26">Use a relative path in a python module</h1>

<p>Use case: need refer to my personal key which is located in another folder <code>resources</code></p>

<p>Store the absolute path to the module directory at the very beginning of the module:</p>

<pre><code class="language-python">package_directory = os.path.dirname(os.path.abspath(__file__))
with open(os.path.join(package_directory, &#39;../resources/tk.key&#39;), &#39;r&#39;) as f:
    token = f.readline()
    ts.set_token(token)
</code></pre>

<p><a href="http://stackoverflow.com/a/4187345/4229125">stackoverflow post</a></p>

<p>The same trick can be used for import a relative path module. <a href="http://stackoverflow.com/a/7506029/4229125">This neat trick is from stackoverflow here</a></p>

<p>At the head of the python file:</p>

<pre><code class="language-python"># -*- coding: utf-8 -*-
import sys, os
import pymongo

try:
    import fromTDX
    from indicators import commonIndicator
    from Const import Const
except Exception:
    sys.path.append(os.path.join(os.path.dirname(__file__), &#39;..&#39;))
    import fromTDX
    from indicators import commonIndicator
    from Const import Const
</code></pre>

<h1 id="toc_27">Mysterious Error pandas groupby error</h1>

<p>For some version of Pandas actually this error will not occur. (pandas v.18.1 will have error, but v.19 has no problem)  </p>

<ul>
<li>Symptom:</li>
</ul>

<blockquote>
<p>lib/python2.7/site-packages/pandas/core/groupby.pyc in reset_identity(values)<br/>
AttributeError: &#39;NoneType&#39; object has no attribute &#39;_get_axis&#39;</p>
</blockquote>

<ul>
<li><p>Cause:<br/><br/>
The apply function return <code>None</code>. When groupby then apply this func, this will cause issue.<br/><br/>
<a href="http://stackoverflow.com/a/20225276/4229125">A similar discussion on stackoverflow</a></p></li>
<li><p>The fix:</p></li>
</ul>

<p>The apply func should return empty <code>pd.DataFrame()</code> or empty <code>pd.Series()</code>.   </p>

<p>Wrong:</p>

<pre><code class="language-python">def get_buy_in_grp(self, grp):
   cond_1_m0 = (grp[&#39;MA_2_&#39; + self.base_period] &gt; grp[&#39;MA_5_&#39; + self.base_period])
   cond_1 = cond_1_m0 &amp; (~cond_1_m0.shift().astype(bool))
   cond_2 = (grp[&#39;MA_5_&#39; + self.base_period] &gt;= self.MA5_p * grp[&#39;MA_5_&#39; + self.base_period].shift())
   cond = cond_1 &amp; cond_2

   if ~(cond.any()):
       return
   return grp[cond]
</code></pre>

<p>Correct:</p>

<pre><code class="language-python">def get_buy_in_grp(self, grp):
   cond_1_m0 = (grp[&#39;MA_2_&#39; + self.base_period] &gt; grp[&#39;MA_5_&#39; + self.base_period])
   cond_1 = cond_1_m0 &amp; (~cond_1_m0.shift().astype(bool))
   cond_2 = (grp[&#39;MA_5_&#39; + self.base_period] &gt;= self.MA5_p * grp[&#39;MA_5_&#39; + self.base_period].shift())
   cond = cond_1 &amp; cond_2

   if ~(cond.any()):
       return pd.DataFrame() # return None may cause issues for the groupby().apply() action
   return grp[cond]     
</code></pre>

<h1 id="toc_28">matplotlib add axes (subplot) on a made figure and sharex</h1>

<p>Use the <code>fig.add_axes()</code> method <a href="http://matplotlib.org/api/figure_api.html#matplotlib.figure.Figure.add_axes">official document</a></p>

<p>To sharex, just <code>sharex</code> parameter in the method. Note that if <code>adjustable</code> parameter is set, in order to use <code>sharex</code>, <code>adjustable = ‘datalim’</code></p>

<pre><code class="language-python">
# the fig and ax_close etc are returned by another method

ax_close_pos = ax_close.get_position()
ax_macd_pos = [ax_close_pos.x0, ax_close_pos.y0, ax_close_pos.width, ax_close_pos.height/5.0]
ax_macd = fig.add_axes(ax_macd_pos, frameon=True, sharex=ax_close)
macd_df = (test_record.df[[col_name for col_name in test_record.df.columns
                           if col_name[-1]==&#39;m&#39;]]
           .drop_duplicates()
           .set_index(&#39;Date_m&#39;))
ax_macd.bar(macd_df[macd_df.MACD_m&gt;0].index.values, 
            macd_df[macd_df.MACD_m&gt;0].MACD_m.values,
            width=15, color=&#39;salmon&#39;)
ax_macd.bar(macd_df[macd_df.MACD_m&lt;0].index.values, 
            macd_df[macd_df.MACD_m&lt;0].MACD_m.values,
            width=15, color=&#39;steelblue&#39;)
ax_macd.grid(True)
mpld3.display()
</code></pre>

<h1 id="toc_29">python parse webpage static content</h1>

<p>use <code>requests</code> to get the response from webpage, use <code>BeautifulSoup</code> to parse the returned content</p>

<p>For some webpage, like <code>http://xueqiu.com/</code>, it will check the header of the connect and will reject the connection if no appropriate header specified. The following is the one that I used to access <code>http://xueqiu.com/</code></p>

<pre><code class="language-python">import requests
import pandas as pd
import io

hdr = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11&#39;,
       &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
       &#39;Accept-Charset&#39;: &#39;ISO-8859-1,utf-8;q=0.7,*;q=0.3&#39;,
       &#39;Accept-Encoding&#39;: &#39;none&#39;,
       &#39;Accept-Language&#39;: &#39;en-US,en;q=0.8&#39;,
       &#39;Connection&#39;: &#39;keep-alive&#39;}
url = &#39;http://xueqiu.com/S/SH600033/historical.csv&#39;

with requests.Session() as s:
   s.headers.update(hdr)
   r = s.get(url)
   if r.status_code == 200: # 200 is of type int NOT type string
       csv = r.content.decode(&#39;utf8&#39;)  # the decode is a must
       # Must use io.StringIO() to wrap it into a string buffer
       df = pd.read_csv(io.StringIO(csv), parse_dates=[&#39;date&#39;], infer_datetime_format=True)
       df = df.drop(&#39;symbol&#39;, axis=1)
       df.columns = [name.capitalize() for name in df.columns]
       df = df.set_index(&#39;Date&#39;)
       return df
   else:
       return None
</code></pre>

<h1 id="toc_30">python get and parse webpage dynamic content</h1>

<p><font color='red'><strong>SELENIUM</strong></font></p>

<p><a href="http://www.seleniumhq.org/docs/03_webdriver.jsp#introducing-the-selenium-webdriver-api-by-example">The best official documentation with python java examples</a></p>

<hr/>

<p>Some webpage&#39;s table is dynamically generated by js, like <code>https://xueqiu.com/S/SH600652/FHPS</code>.<br/>
Like in this <a href="http://stackoverflow.com/a/25062761/4229125">stackoverflow post</a><br/>
(Note: this answer has an issue, instead of <code>table.get_attribute(&#39;innerHTML&#39;)</code>, it should be <code>table.get_attribute(&#39;outerHTML&#39;)</code>. Because pandas need the <code>&lt;table&gt; &lt;/table&gt;</code> to understand that it is a table)</p>

<p>So the best solution is to use <code>selenium</code> <a href="http://selenium-python.readthedocs.io/api.html#webdriver-api">WebDriver</a><br/>
Using conda install like:   </p>

<pre><code class="language-bash">conda install -c clinicalgraphics selenium
</code></pre>

<p><a href="https://docs.google.com/a/adsymptotic.com/presentation/d/18lrHBU7RhyFBJnSmjlB2gBmzMeP2K4Vo53C1OjRm4D4/edit?usp=sharing">Here is a ppt about using selenium in java</a></p>

<ul>
<li>To use chrome WebDriver (which is very easy to use) need install ChromeDriver. <a href="http://stackoverflow.com/a/38087347/4229125">Two different ways to get the ChromeDriver</a>,  I used this one:</li>
</ul>

<pre><code class="language-bash">brew install chromedriver
</code></pre>

<ul>
<li>To use firefox WebDriver, need install two things: Firefox and geckodriver. I used this:</li>
</ul>

<pre><code class="language-bash">brew install Caskroom/cask/firefo
brew install geckodriver
</code></pre>

<p>My complete example:</p>

<pre><code class="language-python">import pandas as pd
from pandas.io import html as pd_html

from contextlib import contextmanager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium import webdriver



@contextmanager
def wait_for_page_load(driver, timeout=10):
    # I used chrome to get this xpath
    check_ele = driver.find_element_by_xpath(&#39;//*[@id=&quot;center&quot;]/div[2]/div[2]/div[2]/div&#39;)
    check_text = check_ele.text
    if check_text == u&#39;暂无数据&#39;:
        old_td = None
    else:
        old_td = driver.find_element_by_xpath(&#39;//*[@id=&quot;center&quot;]/div[2]/div[2]/div[2]/div[1]/table/tbody/tr[1]/td[2]&#39;)
    yield 
    # yield nothing, just want keep the current state of old_td 
    # when exit the with wait_for_page_load block, the next line
    # make sure that the old_td will be changed, or &#39;no next page&#39; element showing up
    if old_td:
        WebDriverWait(driver, timeout=timeout).until(EC.staleness_of(old_td))
    

@contextmanager
def open_driver():
    driver = webdriver.Chrome() # ChromeDriver need be installed by Homebrew
    driver.implicitly_wait(10) # This line will cause it to wait upto 10 seconds if an element is not there
    yield driver
    driver.quit()


with open_driver() as driver:
    url_test = &#39;https://xueqiu.com/S/SZ000547/FHPS&#39;
    driver.get(url_test)

    dividen_dfs = []
    while True:
        with wait_for_page_load(driver):
            check_ele = driver.find_element_by_xpath(&#39;//*[@id=&quot;center&quot;]/div[2]/div[2]/div[2]/div&#39;)
            check_text = check_ele.text
            if check_text == u&#39;暂无数据&#39;:
                break
            table = driver.find_element_by_xpath(&#39;//table[@class=&quot;dataTable table table-bordered&quot;]&#39;)
            table_html = table.get_attribute(&#39;outerHTML&#39;)
            df = pd_html.read_html(table_html, na_values = &#39;-&#39;)

            # below is just some normal dataframe munging
            processed_df = df[0].T
            processed_df.columns = processed_df.iloc[0,:]
            processed_df = processed_df.drop(0, axis=0)
            processed_df = processed_df.reset_index(drop=True)
            dividen_dfs.append(processed_df)

            # get the link and click on the link
            link = driver.find_element_by_link_text(u&#39;下一页&#39;)
            link.click()
        
dividen_df = pd.concat(dividen_dfs).drop_duplicates().reset_index(drop=True)
dividen_df

</code></pre>

<p><font color='red'><strong>Some special notes:</strong></font></p>

<ul>
<li>Why we need wait?  Because if there is asynchronous content like JS modifying the DOM after the initial page load is complete, the driver.get() may miss the target, since the target is loaded later <a href="http://stackoverflow.com/a/16740004/4229125">Stackoverflow post</a>. </li>
<li>Implicitly_wait(): An implicit wait is to tell WebDriver to poll the DOM for a certain amount of time when trying to find an element or elements if they are not immediately available. The default setting is 0. Once set, the implicit wait is set for the life of the WebDriver object instance.</li>
<li>Remember to close the <code>driver</code>, otherwise it will have resource leaking</li>
<li>Chrome has a very cool feature, under <code>inspect</code> mode, use right click you can directly copy <font color='red'><strong>XPath, Selection, Element etc.</strong></font><br/></li>
</ul>

<p><font color='red'><strong>Updated notes</strong></font></p>

<ul>
<li>I heavily referred to <a href="http://www.obeythetestinggoat.com/how-to-get-selenium-to-wait-for-page-load-after-a-click.html">this post</a> for the much improved version of the code. The central idea is by checking <code>EC.staleness_of(old_td)</code> to make sure that the new content has been generated in the <code>wait_for_page_load</code> method.</li>
<li>In the python binding of <code>selenium</code>, to get the text of the page element, actually need go through the property as <code>.text</code> instead of using method <code>.getText()</code> which is different than the Java binding</li>
<li>It doesn&#39;t show in the code anymore, but the way of using <code>Locator</code> is construct a tuple, which is kind of confusing. Look at the below code snippet and pay attention to the double bracket</li>
</ul>

<pre><code class="language-python">from selenium.webdriver.common.by import By

EC.presence_of_element_located((By.ID, &quot;myDynamicElement&quot;))
</code></pre>

<h1 id="toc_31">python with statement and context manager type</h1>

<p><a href="https://docs.python.org/2.7/library/stdtypes.html#context-manager-types">Official document for Context Manager</a><br/>
<a href="https://docs.python.org/2.7/reference/datamodel.html#with-statement-context-managers">Official document for with statement Context Manager</a></p>

<p>In the above code snippet, I used the <code>@contextmanager</code> decorator to build the <code>with open_driver() as driver:</code> so that when the block of code exit, the driver will be automatically closed and preventing resource leaking.</p>

<p><a href="http://preshing.com/20110920/the-python-with-statement-by-example/">A good blog post about this</a></p>

<h1 id="toc_32">python selenium selector find element by multiple class names</h1>

<p>Imagine we have few elements as the followings:</p>

<ol>
<li><code>&lt;div class=&quot;value test&quot; /&gt;</code></li>
<li><code>&lt;div class=&quot;value test &quot; /&gt;</code></li>
<li><code>&lt;div class=&quot;first value test last&quot; /&gt;</code></li>
<li><code>&lt;div class=&quot;test value&quot; /&gt;</code></li>
</ol>

<p>How XPath matches</p>

<ul>
<li><p>Match only 1 (exact match)</p>

<pre><code>driver.findElement(By.xpath(&quot;//div[@class=&#39;value test&#39;]&quot;));
</code></pre></li>
<li><p>Match 1, 2 and 3 (match class contains <code>value test</code>, class order matters)</p>

<pre><code>driver.findElement(By.xpath(&quot;//div[contains(@class, &#39;value test&#39;)]&quot;));
</code></pre></li>
<li><p>Match 1, 2, 3 and 4 (as long as elements have class <code>value</code> and <code>test</code>)</p>

<pre><code>driver.findElement(By.xpath(&quot;//div[contains(@class, &#39;value&#39;) and contains(@class, &#39;test&#39;)]&quot;));
</code></pre></li>
</ul>

<p>Also, in cases like this, Css Selector is always in favor of XPath (fast, concise, native).</p>

<ul>
<li><p>Match 1</p>

<pre><code>driver.findElement(By.cssSelector(&quot;div[class=&#39;value test&#39;]&quot;));
</code></pre></li>
<li><p>Match 1, 2 and 3</p>

<pre><code>driver.findElement(By.cssSelector(&quot;div[class*=&#39;value test&#39;]&quot;));
</code></pre></li>
<li><p>Match 1, 2, 3 and 4</p>

<pre><code>driver.findElement(By.cssSelector(&quot;div.value.test&quot;));
</code></pre></li>
</ul>

<p><a href="http://stackoverflow.com/a/21714006/4229125">stackoverflow post</a></p>

<h1 id="toc_33">Speed up selenium and Time out</h1>

<p>I only tried with Firefox. According to a lot of post, the solution should be using the <code>FirefoxProfile</code> by <code>set_preference(&#39;webdriver.load.strategy&#39;, &#39;unstable&#39;)</code> as shown in <a href="http://stackoverflow.com/a/11455428/4229125">this stackoverflow post</a>. However, as I tested with firefox version 50.x, it does not work. According to <a href="https://github.com/SeleniumHQ/selenium/issues/2873">this issue</a>, it only work for firefox <font color='red'><strong>less than v. 46</strong></font></p>

<p>This still not completely solved. I did find a way to make it a little bit better.</p>

<p>Check the available firefox configuration parameters: open firefox and input <code>about:config</code> in the url address field.</p>

<p>Also there is <a href="http://kb.mozillazine.org/About:config_entries">the official documents for the full list and explainations</a></p>

<p>The useful parameters that I found are:  </p>

<ul>
<li><code>permissions.default.image, 2</code>, which make the loading of the page much faster</li>
<li><strong><code>network.http.connection-timeout, 5</code></strong>, this one is very cool. It force the connect to break after the specified amount of time, so you don&#39;t have to wait till the full page to load if you can already find what you need. <a href="http://stackoverflow.com/a/1343963/4229125">Reference to this post, then checked with firefox&#39;s about:config</a></li>
</ul>

<p>To figure out what is the current firefox profile that the WebDriver is using:</p>

<pre><code class="language-python">firefox_p = webdriver.FirefoxProfile()
firefox_p.default_preferences # this is a dictionary
</code></pre>

<p>To do the improvement setting for firefox is:</p>

<pre><code class="language-python">@contextmanager
def open_fast_driver():
    firefox_p = webdriver.FirefoxProfile()
    firefox_p.set_preference(&#39;permissions.default.image&#39;, 2)
    firefox_p.set_preference(&#39;network.http.connection-timeout&#39;, 1)
    driver = webdriver.Firefox(firefox_profile=firefox_p)
    yield driver
    driver.quit()
</code></pre>

<p><font color='red'><strong>UPDATE</strong></font><br/>
I decided to downgrade the firefox to v.46 and see how it goes.</p>

<pre><code class="language-bash">brew cask uninstall firefox # uninstall the previous version
brew tap goldcaddy77/firefox
brew cask install firefox-46
</code></pre>

<p>After install this version of Firefox, need add the following lines when use firefox WebDirver:</p>

<pre><code class="language-python">from selenium.webdriver.firefox.firefox_binary import FirefoxBinary

firefox_b = FirefoxBinary(r&#39;/Applications/Firefox-46.app/Contents/MacOS/firefox&#39;)
driver = webdriver.Firefox(firefox_binary=firefox_b)
</code></pre>

<p>But, hey! You guess what!, it still not working even with Firefox V.46. And it breaks the little trick that I found to disable images too.</p>

<p>I uninstall the V46, and come back to what I did earlier.</p>

<pre><code class="language-bash">cd ~
brew cask uninstall firefox-4
brew untap goldcaddy77/firefo
brew install Caskroom/cask/firefox
</code></pre>

<p><font color='red'><strong>UPDATE, UPDATE</strong></font></p>

<p>Tried to use Firefox add-on <code>KillSpinners</code> to stop loading the page after some time-out. Not successful. But learned how to use add-on/extension of selenium</p>

<p>For firefox extension on mac, it is located here: <code>~/Library/Application Support/Firefox/Profiles/[profile name]/extensions</code>. For my Mac it is here: </p>

<pre><code class="language-python">firefox_KillSpinners_ext_path = &#39;/Users/yugan/Library/Application Support/Firefox/Profiles/1qvac6rq.default/extensions/killspinners@byo.co.il.xpi&#39;
</code></pre>

<p>The python code to use this extension is:</p>

<pre><code class="language-python">firefox_p = webdriver.FirefoxProfile()
firefox_p.add_extension(firefox_KillSpinners_ext_path)
firefox_p.set_preference(&#39;permissions.default.image&#39;, 2)
firefox_p.set_preference(&#39;extensions.killspinners.timeout&#39;, 10)
firefox_p.set_preference(&#39;extensions.killspinners.disablenotify&#39;, True)
</code></pre>

<p>The two parameters are found in the firefox <code>about:config</code> page.<br/>
The reason it failed is because this add-on blocked the thread.</p>

<p><font color='red'><strong>UPDATE, UPDATE, UPDATE</strong></font></p>

<p>The best update so far.</p>

<p>The solution is changing the WebDriver. No matter whether it is <code>ChromeDriver</code> or <code>FirefoxDriver</code>, they are too heavy and slow. We should use <font color='red'><strong>non-GUI drivers</strong></font> to speed it up. selenium has excellent support of using the following two non-GUI WebDriver. Check <a href="http://www.seleniumhq.org/docs/03_webdriver.jsp#driver-specifics-and-tradeoffs">the official documents for Selenium-WebDriver support</a></p>

<ul>
<li>HtmlUnit - <a href="http://www.seleniumhq.org/docs/03_webdriver.jsp#javascript-in-the-htmlunit-driver">need be careful for the javascript handling</a> </li>
</ul>

<p>In order to use this WebDriver, need install and run Selenium Server:</p>

<pre><code class="language-bash">brew install selenium-server-standalone
selenium-server -port 4444
</code></pre>

<p>Then in python code, specify Using Selenium with <font color='red'><strong>remote</strong></font> WebDriver (<a href="http://selenium-python.readthedocs.io/getting-started.html#using-selenium-with-remote-webdriver">See the remote webdriver here</a>), which should connect to: <code>http://127.0.0.1:4444/wd/hub</code>. Specify the desired_capabilities to <code>HTMLUNITWITHJS</code>. The <a href="http://selenium-python.readthedocs.io/api.html#desired-capabilities">desired_capabilities</a> includes all broswers like (chrome, firefox, safari, phantomJS... etc)</p>

<blockquote>
<p>Note: Always use <code>.copy()</code> on the DesiredCapabilities object to avoid the side effects of altering the Global class instance </p>
</blockquote>

<p>Also, checking the cmd line output can help find some useful parameters for the WebDrivers</p>

<ul>
<li><font color='red'>phatomJS</font>- This so far is the best one. It respond the javascript pretty well. I find some configuration parameters which can further speed up the response </li>
</ul>

<p>install:</p>

<pre><code class="language-bash">brew install phantomjs
</code></pre>

<p>python code:</p>

<pre><code class="language-python">@contextmanager
def open_phantomJS_driver():
    
    capabilities = webdriver.DesiredCapabilities.PHANTOMJS.copy()
    capabilities[&#39;phantomjs.page.settings.loadImages&#39;] = False
    capabilities[&#39;phantomjs.page.settings.webSecurityEnabled&#39;] = False
    capabilities[&#39;phantomjs.page.settings.javascriptCanOpenWindows&#39;] = False
    capabilities[&#39;phantomjs.page.settings.javascriptCanCloseWindows&#39;] = False
    capabilities[&#39;phantomjs.page.settings.userAgent&#39;] = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/538.1 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/538.1&#39;

    driver = webdriver.Remote(&quot;http://localhost:4444/wd/hub&quot;, capabilities)
    yield driver
    driver.quit()
</code></pre>

<p><a href="https://realpython.com/blog/python/headless-selenium-testing-with-python-and-phantomjs/">A blog for phantomJS</a><br/>
<strong><a href="https://github.com/SeleniumHQ/selenium/wiki/DesiredCapabilities">For the Browser Specific DesiredCapabilities, there is an official documents (not exhaustive though)</a></strong><br/>
<strong><a href="https://sites.google.com/a/chromium.org/chromedriver/capabilities">ChromeDriver available Capabilities and options</a></strong><br/>
<a href="http://peter.sh/experiments/chromium-command-line-switches/#timeout">List of chrome command line switches, but seems outdated. At least the <code>--timeout</code> switch does not work anymore</a></p>

<p><font color='red'><strong>I tested some config for the HtmlUnit and ChromeOptions, using the above links, but most of them are either not working or not relevant to what I am doing. Just keep these as a record if I need use them in the future</strong></font></p>

<h1 id="toc_34">An example of using regex parse table</h1>

<p>This table is not well formed for pandas to read. Combining use both the <code>split()</code> and regex make it much easier</p>

<pre><code class="language-python">import re
from dateutil import parser as date_parser
import pandas as pd
from contextlib import contextmanager
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

@contextmanager
def open_fast_driver():
    firefox_p = webdriver.FirefoxProfile()
    firefox_p.set_preference(&#39;permissions.default.image&#39;, 2)
    firefox_p.set_preference(&#39;network.http.connection-timeout&#39;, 1)
    driver = webdriver.Firefox(firefox_profile=firefox_p)
    yield driver
    driver.quit()
    

zg_re = re.compile(ur&#39;转增(\d+)股&#39;)
sg_re = re.compile(ur&#39;送(\d+)股&#39;)
fh_re = re.compile(ur&#39;派息(.+)元&#39;)
date_re = re.compile(ur&#39;(\d{4}-\d{2}-\d{2})&#39;)


base_url = &#39;http://q.stock.sohu.com/cn/{}/fhsp.shtml&#39;
code = &#39;600589&#39;
url = base_url.format(code)
table_xpath = &#39;/html/body/div[4]/div[2]/div[2]/div[2]/div/div[2]/table&#39;
table = None
with open_fast_driver() as driver:
    driver.get(url)
    d_table = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH,table_xpath)))
    table = d_table.text

if table:
    line_iter = iter(table.split(&#39;\n&#39;))
    records = []
    while True:
        record = dict()
        date_m = None
        try:
            line = line_iter.next()
            tokens = line.strip().split()
            if len(tokens) == 1:
                continue
            if tokens[0] != u&#39;除权除息日&#39;:
                continue
            if len(tokens) == 2:
                next_line = line_iter.next()
                date_m = date_re.search(next_line)
            if not date_m:
                date_m = date_re.search(line)
            zg_m = zg_re.search(line)
            sg_m = sg_re.search(line)
            fh_m = fh_re.search(line)
            record[&#39;Date&#39;] = date_parser.parse(date_m.group(1))
            if zg_m:
                record[&#39;Zg&#39;] = float(zg_m.group(1))/10.
            if sg_m:
                record[&#39;Sg&#39;] = float(sg_m.group(1))/10.
            if fh_m:
                record[&#39;Fh&#39;] = float(fh_m.group(1))/10.
            records.append(record)
        except StopIteration:
            break
    fhps_df = pd.DataFrame(records)
    for field in [&#39;Fh&#39;, &#39;Zg&#39;, &#39;Sg&#39;]:
        if field not in fhps_df:
            fhps_df.loc[:, field] = 0.0
    fhps_df.loc[:, [&#39;Fh&#39;, &#39;Zg&#39;, &#39;Sg&#39;]] = fhps_df.loc[:, [&#39;Fh&#39;, &#39;Zg&#39;, &#39;Sg&#39;]].fillna(0.0)
    fhps_df.loc[:, &#39;Ps&#39;] = fhps_df[&#39;Zg&#39;]+fhps_df[&#39;Sg&#39;]
    fhps_df = fhps_df.sort_values(&#39;Date&#39;)

fhps_df[[&#39;Date&#39;, &#39;Fh&#39;, &#39;Ps&#39;]]
</code></pre>

<h1 id="toc_35">start and shutdown the selenium-server</h1>

<pre><code class="language-python">import subprocess
import time
from contextlib import contextmanager


@contextmanager
def open_selenium_server():
    null_io = open(&#39;/dev/null&#39;)
    server_proc = subprocess.Popen([&#39;selenium-server&#39;, &#39;-port&#39;, &#39;4444&#39;], stdout=null_io, stderr=null_io)
    time.sleep(0.5)
    yield
    server_proc.terminate()
</code></pre>

<p><a href="http://stackoverflow.com/a/6883164/4229125">referred to this post</a></p>

<h1 id="toc_36">phantomJS options</h1>

<p>In the readme page of <code>ghostdriver</code> project, it gave a section about how to set the webdriver properties of phantomJS. <a href="https://github.com/detro/ghostdriver#what-extra-webdriver-capabilities-ghostdriver-offers">It is a good reference</a></p>

<p>specificially, besides the setting that I figured out randomly before, the following setting seems pretty good:</p>

<pre><code class="language-python"># new settings
capabilities[&#39;phantomjs.page.settings.resourceTimeout&#39;] = 5000 # 5000 milliseconds
</code></pre>

<p><a href="http://phantomjs.org/api/webpage/property/settings.html">official list of all properties / settings</a></p>

<h1 id="toc_37">Pycharm unittest not finding the module path</h1>

<p>This is something wrong with Pycharm itself</p>

<p>Error message looks like:</p>

<blockquote>
<p>RuntimeWarning: Parent module &#39;tests&#39; not found while handling absolute import<br/>
  import unittest</p>
</blockquote>

<p>Fix:<br/>
download the <code>utrunner.py</code> file from <a href="https://youtrack.jetbrains.com/_persistent/utrunner.py?file=74-332199&amp;c=true">here</a><br/>
copy it to <code>/Applications/PyCharm CE.app/Contents/helpers/pycharm/</code> and overwrite the original <code>utrunner.py</code> file</p>

<p><a href="http://stackoverflow.com/a/38724508/4229125">stackoverflow post</a></p>

<h1 id="toc_38">Conda rename environment</h1>

<p>Not specific command to do that. Just copy and remove like follow</p>

<pre><code class="language-bash">conda create --name personalTrader --clone bktrader
conda remove --name bktrader --all
</code></pre>

<p>also need manually remove the list entries inside this file: <code>~/.conda/environments.txt</code>, as referred <a href="https://github.com/conda/conda/issues/2704">here</a></p>

<p><a href="https://github.com/conda/conda/issues/3097">reference post</a></p>

<h1 id="toc_39">Deploy an egg package to distributed nodes on hadoop</h1>

<p>In <code>oozie</code> workflow, use <code>&lt;file&gt;</code> action node to &#39;ship&#39; the egg to distributed nodes</p>

<pre><code class="language-xml">    &lt;action name=&#39;train-cd&#39;&gt;
        &lt;shell xmlns=&quot;uri:oozie:shell-action:0.1&quot;&gt;
            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
            &lt;prepare&gt;
                &lt;delete path=&quot;${nameNode}${modelOut}/CD&quot;/&gt;
                &lt;mkdir path=&quot;${nameNode}${modelOut}/CD&quot;/&gt;
            &lt;/prepare&gt;
            &lt;configuration&gt;
                &lt;property&gt;
                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${queueName}&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.memory.mb&lt;/name&gt;
                    &lt;value&gt;204800&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;oozie.launcher.mapreduce.map.java.opts&lt;/name&gt;
                    &lt;value&gt;-Xmx174080m&lt;/value&gt;
                &lt;/property&gt;
                &lt;property&gt;
                    &lt;name&gt;mapreduce.task.timeout&lt;/name&gt;
                    &lt;value&gt;36000000&lt;/value&gt;
                &lt;/property&gt;
            &lt;/configuration&gt;
            &lt;exec&gt;train.py&lt;/exec&gt;
            &lt;argument&gt;${rankOut}/features-all/CD&lt;/argument&gt;
            &lt;argument&gt;${modelOut}/CD&lt;/argument&gt;
            &lt;env-var&gt;HADOOP_USER_NAME=${wf:user()}&lt;/env-var&gt;
            &lt;file&gt;${trainScript}#train.py&lt;/file&gt;
            &lt;file&gt;${lightgbmegg}#lightgbm-0.1-py2.7.egg&lt;/file&gt;
            &lt;capture-output/&gt;&lt;/shell&gt;
        &lt;ok to=&quot;pred-cd&quot; /&gt;
        &lt;error to=&quot;email-error&quot; /&gt;
    &lt;/action&gt;
</code></pre>

<p>In python need do two things:</p>

<ol>
<li>unzip the egg file</li>
<li>append the <code>cwd</code> to the system path</li>
</ol>

<pre><code class="language-python">#!/usr/local/bin/python2.7

import sys
import os
import pandas as pd
import cPickle as pickle
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.grid_search import GridSearchCV
from sklearn.utils import shuffle
from sklearn.metrics import auc, precision_recall_curve
from subprocess import call
call(&#39;unzip lightgbm-0.1-py2.7.egg&#39;, shell=True)

cwd = os.getcwd()
sys.path.append(cwd)

from lightgbm import LGBMClassifier
</code></pre>

<h1 id="toc_40">numpy broadcasting</h1>

<p>The purpose of doing broadcasting is to make two numpy array with different shape converting to the <strong>same</strong> shape, so that element-wise operation can be implemented.</p>

<p>Functions that does element-wise operation and supports broadcasting are known as universal functions <code>ufunc</code></p>

<p>Broadcasting two arrays together follows these rules:</p>

<ul>
<li>If the arrays do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.</li>
<li>The two arrays are said to be compatible in a dimension if they have the same size in the dimension, or if one of the arrays has size 1 in that dimension.</li>
<li>The arrays can be broadcast together if they are compatible in all dimensions.</li>
<li>After broadcasting, each array behaves as if it had shape equal to the elementwise maximum of shapes of the two input arrays.</li>
<li>In any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension</li>
</ul>

<p><a href="http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc">Array Broadcasting in numpy blog</a><br/>
<a href="http://cs231n.github.io/python-numpy-tutorial/#numpy-broadcasting">A very good python tutorial from Stanford</a></p>

<h1 id="toc_41">Install matplotlib on Mac</h1>

<p>I had this issue when installing <code>matplotlib</code> (using <code>pip install matplotlib</code>) in the virtualenv for LNKD spark-notebook. The error message is like this</p>

<blockquote>
<p>The following required packages can not be built: * freetype * Try installing freetype with <code>brew install freetype</code> * libpng * </p>
</blockquote>

<p>However, after I <code>brew install freetype</code> still matplotlib could not find it</p>

<p>Solution: <code>brew install pkg-config</code></p>

<p>After installed this, re-run <code>pip install matplotlib</code>. no more issue.<br/>
<a href="https://stackoverflow.com/a/24685596/4229125">reference post</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">shell expansion insdie python with  <code>subprocess.call</code></a>
</li>
<li>
<a href="#toc_1">plotly using LaTex</a>
</li>
<li>
<a href="#toc_2">plotly cufflinks pandas dataframe customize the plot</a>
</li>
<li>
<a href="#toc_3">timezone</a>
</li>
<li>
<a href="#toc_4">From String to timestamp to epoch unix time</a>
</li>
<li>
<a href="#toc_5">datetime(python), pd.Timestamp(pandas), np.datetime64(numpy)</a>
</li>
<li>
<a href="#toc_6">To make a linspace of pd.Timestamp</a>
</li>
<li>
<a href="#toc_7">plotly tick setting: string format, number of ticks and location of ticks.</a>
</li>
<li>
<a href="#toc_8">plotly datetime type tick setting</a>
</li>
<li>
<a href="#toc_9">plotly change the padding size between subplots</a>
</li>
<li>
<a href="#toc_10">read in gz / gzip file</a>
</li>
<li>
<a href="#toc_11">plotly bar chart, use whether value is negative or positive to determine its color</a>
</li>
<li>
<a href="#toc_12">plotly subplot shared axis setting</a>
</li>
<li>
<a href="#toc_13">plotly remove missed dates from time series plot</a>
</li>
<li>
<a href="#toc_14">MultiIndex Dataframe select a particular (like the 2nd) level of the MultiIndex</a>
</li>
<li>
<a href="#toc_15">Use <code>:</code> or <code>slice(None)</code></a>
</li>
<li>
<a href="#toc_16">String for pandas datetime period and frequency</a>
</li>
<li>
<a href="#toc_17">Python Library for Probabilistic Graphical Models</a>
</li>
<li>
<a href="#toc_18">Python blocking and non-blocking subprocess calls</a>
</li>
<li>
<a href="#toc_19">nohup python that includes a nohup shell script</a>
</li>
<li>
<a href="#toc_20">ipython jupyter notebook pandas showing options</a>
</li>
<li>
<a href="#toc_21">matplotlib style and colors</a>
</li>
<li>
<a href="#toc_22">matplotlib backend choices</a>
</li>
<li>
<a href="#toc_23">matplotlib with mpld3 <code>twinx</code> two side axes issue</a>
</li>
<li>
<a href="#toc_24">Conda install python package CondaHTTPError Error</a>
</li>
<li>
<a href="#toc_25">utf8 unicode in string formatter &amp; file read/write</a>
</li>
<li>
<a href="#toc_26">Use a relative path in a python module</a>
</li>
<li>
<a href="#toc_27">Mysterious Error pandas groupby error</a>
</li>
<li>
<a href="#toc_28">matplotlib add axes (subplot) on a made figure and sharex</a>
</li>
<li>
<a href="#toc_29">python parse webpage static content</a>
</li>
<li>
<a href="#toc_30">python get and parse webpage dynamic content</a>
</li>
<li>
<a href="#toc_31">python with statement and context manager type</a>
</li>
<li>
<a href="#toc_32">python selenium selector find element by multiple class names</a>
</li>
<li>
<a href="#toc_33">Speed up selenium and Time out</a>
</li>
<li>
<a href="#toc_34">An example of using regex parse table</a>
</li>
<li>
<a href="#toc_35">start and shutdown the selenium-server</a>
</li>
<li>
<a href="#toc_36">phantomJS options</a>
</li>
<li>
<a href="#toc_37">Pycharm unittest not finding the module path</a>
</li>
<li>
<a href="#toc_38">Conda rename environment</a>
</li>
<li>
<a href="#toc_39">Deploy an egg package to distributed nodes on hadoop</a>
</li>
<li>
<a href="#toc_40">numpy broadcasting</a>
</li>
<li>
<a href="#toc_41">Install matplotlib on Mac</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[scala]]></title>
    <link href="www.echo-ohce.com/14767220898143.html"/>
    <updated>2016-10-17T09:34:49-07:00</updated>
    <id>www.echo-ohce.com/14767220898143.html</id>
    <content type="html"><![CDATA[
<p>This documents all the tips, pitfalls I experienced along the way. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Convert between regular collection and parallel collection</a>
</li>
<li>
<a href="#toc_1">function / method handler</a>
</li>
<li>
<a href="#toc_2">Get the type of variable in REPL</a>
</li>
<li>
<a href="#toc_3">Span method</a>
</li>
<li>
<a href="#toc_4">flatMap and Option</a>
</li>
<li>
<a href="#toc_5">Patter Matching with default</a>
</li>
<li>
<a href="#toc_6">Pattern matching in array assignment, variable name must start with lower case letter</a>
</li>
<li>
<a href="#toc_7">read and write to file</a>
</li>
<li>
<a href="#toc_8">string formatter</a>
</li>
<li>
<a href="#toc_9">regex option</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">Convert between regular collection and parallel collection</h1>

<p>Using Seq as example. Use <code>.par</code> to convert from regular to parallel, and use <code>.seq</code> to convert back from parallel to regular.</p>

<p>Special note that <code>.toSeq</code>, <code>toMap</code> or <code>toSet</code> are converting between various collection but stay in the same parallel or regular domain.</p>

<p><a href="http://stackoverflow.com/a/12023096/4229125">stackoverflow post</a></p>

<h1 id="toc_1">function / method handler</h1>

<p>look at the following example:</p>

<pre><code class="language-scala">val test = Array(&quot;1.0&quot;, &quot;2.5&quot;, &quot;3.14159&quot;)

//Wrong
test map toDouble
// Because this is wrong
toDouble(test(2))
//correct
test map (_.toDouble)
// Because this is correct
test(2).toDouble

//Wrong
test map parseDouble
//correct
test map java.lang.Double.parseDouble
// Because this is correct
java.lang.Double.parseDouble(test(2))
</code></pre>

<h1 id="toc_2">Get the type of variable in REPL</h1>

<p><code>variableName.getClass.getSimpleName</code></p>

<h1 id="toc_3">Span method</h1>

<p><a href="https://www.garysieling.com/blog/scala-span-example">A good post</a><br/>
The span method lets you split a stream into two parts, by providing a function that detects the dividing line where you want the split to occur. What this doesn’t let you do is to sift through the stream and move each record into the a or b side – you can’t use this to separate even and odd numbers, for instance.</p>

<p>Example:</p>

<pre><code class="language-scala">var (a, b) = Stream.from(1).span(_ &lt; 10)
a: scala.collection.immutable.Stream[Int] = Stream(1, ?)
b: scala.collection.immutable.Stream[Int] = Stream(10, ?)
</code></pre>

<p>From this, it appears that it has actually run through 10 elements immediately, not waiting until you actually need one.</p>

<p>We can see it returns what we’d expect:</p>

<pre><code class="language-scala">scala&gt; a.take(10).toList
res26: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9)

scala&gt; b.take(10).toList
res27: List[Int] = List(10, 11, 12, 13, 14, 15, 16, 17, 18, 19)
</code></pre>

<p>To show that the even/odd filtering doesn’t work, see the example below. Since the first element is “1” (odd) it automatically goes to the “b” side, and you get an empty list for “a”.</p>

<pre><code class="language-scala">scala&gt; var (a, b) = Stream.from(1).span(_ % 2 == 0)
a: scala.collection.immutable.Stream[Int] = Stream()
b: scala.collection.immutable.Stream[Int] = Stream(1, ?)
</code></pre>

<h1 id="toc_4">flatMap and Option</h1>

<p><code>flatMap</code> on <code>Seq[Option]</code> effectively filter out the <code>None</code>s.<br/>
The keypoint of understanding Option is consider it as a Collection with 1 or 0 element. When using the <code>flatMap</code> it explode the inner collection and put all elements from the inner collection into the one big outer collection.</p>

<p><a href="http://danielwestheide.com/blog/2012/12/19/the-neophytes-guide-to-scala-part-5-the-option-type.html">A good reference post</a></p>

<h1 id="toc_5">Patter Matching with default</h1>

<pre><code class="language-scala">something match {
    case &quot;val&quot; =&gt; &quot;default&quot;
    case default =&gt; somefunction(default)
}
</code></pre>

<p>It is not a keyword, just an alias, so this will work as well:</p>

<pre><code class="language-scala">something match {
    case &quot;val&quot; =&gt; &quot;default&quot;
    case everythingElse =&gt; somefunction(everythingElse)
}
</code></pre>

<p>Or using <code>_</code></p>

<pre><code class="language-scala">something match {
    case &quot;val&quot; =&gt; &quot;default&quot;
    case _ =&gt; doSomething()
}
</code></pre>

<h1 id="toc_6">Pattern matching in array assignment, variable name must start with lower case letter</h1>

<p><a href="http://stackoverflow.com/a/8204231/4229125">stackoverflow post</a></p>

<p>If the variable name starts with capital letter, Scala treat it as a Constant</p>

<p>Wrong:</p>

<pre><code class="language-scala">val Array(trainData, CVData) = cleanedUserArtistData.randomSplit(Array(0.9, 0.1))
</code></pre>

<p>Correct:</p>

<pre><code class="language-scala">val Array(trainData, cvData) = cleanedUserArtistData.randomSplit(Array(0.9, 0.1))
</code></pre>

<h1 id="toc_7">read and write to file</h1>

<p>Scala doesn’t offer any special file writing capability, so fall back and use the Java <code>PrintWriter</code> or <code>FileWriter</code> approaches</p>

<pre><code class="language-scala">// PrintWriter
import java.io._
val pw = new PrintWriter(new File(&quot;hello.txt&quot; ))
pw.write(&quot;Hello, world&quot;)
pw.close

// FileWriter
val file = new File(canonicalFilename)
val bw = new BufferedWriter(new FileWriter(file))
bw.write(text)
bw.close()
</code></pre>

<p><a href="https://www.safaribooksonline.com/library/view/scala-cookbook/9781449340292/ch12s03.html">reference</a></p>

<p>Read in utilities can be found in package <code>scala.io.Source</code> (<a href="http://www.scala-lang.org/api/current/scala/io/Source.html">both the class and its companion object</a></p>

<h1 id="toc_8">string formatter</h1>

<p><a href="http://docs.scala-lang.org/overviews/core/string-interpolation.html">reference here</a></p>

<p>This is very useful feature, make the mixing of string and variables much easier, and also handling the escape characters well.</p>

<ul>
<li><code>s</code></li>
<li><code>f</code></li>
<li><code>raw</code></li>
</ul>

<h1 id="toc_9">regex option</h1>

<p><code>i</code> - case insensitive<br/>
<code>d</code> - only unix lines are recognized as end of line<br/>
<code>m</code> - enable multiline mode<br/>
<code>s</code> - <code>.</code> matches any characters including line end<br/>
<code>u</code> - Enables Unicode-aware case folding<br/>
<code>x</code> - Permits whitespace and comments in pattern</p>

<p>The way of specifying it like follow:<br/>
<code>&quot;&quot;&quot;(?i)the \w+?(?=\W)&quot;&quot;&quot;.r</code><br/>
The <code>&quot;&quot;&quot;</code> means raw string. No need to double the <code>\</code><br/>
and the <code>(?i)</code> at the beginning gives the option of case insensitive</p>

<p><a href="http://daily-scala.blogspot.com/2010/01/regular-expression-2-rest-regex-class.html">reference here</a><br/>
<a href="http://www.scala-lang.org/api/rc2/scala/util/matching/Regex.html">official document</a></p>

<hr/>

<ul>
<li>
<a href="#toc_0">Convert between regular collection and parallel collection</a>
</li>
<li>
<a href="#toc_1">function / method handler</a>
</li>
<li>
<a href="#toc_2">Get the type of variable in REPL</a>
</li>
<li>
<a href="#toc_3">Span method</a>
</li>
<li>
<a href="#toc_4">flatMap and Option</a>
</li>
<li>
<a href="#toc_5">Patter Matching with default</a>
</li>
<li>
<a href="#toc_6">Pattern matching in array assignment, variable name must start with lower case letter</a>
</li>
<li>
<a href="#toc_7">read and write to file</a>
</li>
<li>
<a href="#toc_8">string formatter</a>
</li>
<li>
<a href="#toc_9">regex option</a>
</li>
</ul>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[setting]]></title>
    <link href="www.echo-ohce.com/14742423250740.html"/>
    <updated>2016-09-18T16:45:25-07:00</updated>
    <id>www.echo-ohce.com/14742423250740.html</id>
    <content type="html"><![CDATA[
<p>This documents various settings that I normally use. Keep updating.  </p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">python notebook imports and setting</a>
</li>
<li>
<a href="#toc_1">Mac multiple desktop screen spaces setting.</a>
</li>
<li>
<a href="#toc_2">python conda virtual environment and jupyter notebook kernel</a>
</li>
<li>
<a href="#toc_3">Mac OS gets very busy on background process</a>
</li>
<li>
<a href="#toc_4">DNS and VPN connection issue</a>
</li>
</ul>


<hr/>

<h1 id="toc_0">python notebook imports and setting</h1>

<pre><code class="language-python">import os
import pandas as pd
import numpy as np
import math
import plotly.plotly as py
import plotly.offline as po
import plotly.graph_objs as go
import plotly.tools as tls
import colorlover as cl
import cufflinks as cf
import subprocess
from ipywidgets import interact
import glob # for shell expansion
from IPython.display import display, HTML, Image
import matplotlib.pyplot as plt

%matplotlib inline

po.init_notebook_mode()
cf.set_config_file(theme=&#39;white&#39;)
pd.set_option(&#39;precision&#39;, 3) # printing precision
np.set_printoptions(precision=3) # printing precision
cf.go_offline() # using cufflinks at offline mode
</code></pre>

<h1 id="toc_1">Mac multiple desktop screen spaces setting.</h1>

<p><a href="http://www.tweaking4all.com/os-tips-and-tricks/macosx-tips-and-tricks/mac-multiple-desktops-spaces/">Very good post</a><br/><br/>
Using <code>SteerMouse</code> to set the mouse shortcut.</p>

<h1 id="toc_2">python conda virtual environment and jupyter notebook kernel</h1>

<p>Need install a plugin <code>nb_conda_kernels</code></p>

<ol>
<li>switch to the desired virtual environment</li>
<li>install plugin</li>
<li>run jupyter notebook then pick the desired kernel from notebook</li>
</ol>

<pre><code class="language-bash">source activate environment_name
conda install -c conda-forge nb_conda_kernels
jupyter notebook

source deactive
</code></pre>

<p><a href="https://github.com/Anaconda-Platform/nb_conda_kernels">plugin&#39;s main page</a><br/>
<a href="http://stackoverflow.com/a/38880722">stackoverflow post</a></p>

<h1 id="toc_3">Mac OS gets very busy on background process</h1>

<ol>
<li>It is because of <code>optimal Layout</code></li>
<li>The app to check the system performance is <code>Activity Monitor</code></li>
<li>There is another process named <code>distnoted</code> taking a lot of CPU resources. This is Apple&#39;s kernel process, but it is safely to be removed. Using the script (saved as <code>install_checkdistnoted.sh</code> add a cron job to kill this process when it takes too much resources.<br/>
See the <a href="http://apple.stackexchange.com/a/234478">original post for this scirpt</a><br/></li>
</ol>

<h1 id="toc_4">DNS and VPN connection issue</h1>

<pre><code class="language-bash">alias reload=&quot;sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist;sudo launchctl load -w /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist&quot;
</code></pre>

<p>From obuli: this refreshes DNS entries somehow i have to do this every time I connect to VPN from my home</p>

<hr/>

<ul>
<li>
<a href="#toc_0">python notebook imports and setting</a>
</li>
<li>
<a href="#toc_1">Mac multiple desktop screen spaces setting.</a>
</li>
<li>
<a href="#toc_2">python conda virtual environment and jupyter notebook kernel</a>
</li>
<li>
<a href="#toc_3">Mac OS gets very busy on background process</a>
</li>
<li>
<a href="#toc_4">DNS and VPN connection issue</a>
</li>
</ul>


]]></content>
  </entry>
  
</feed>
